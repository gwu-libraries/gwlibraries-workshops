{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python for Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**GW Libraries and Academic Innovation**\n",
    "\n",
    "Friday, February 12, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Workshop goals\n",
    "\n",
    "This workshop introduces some fundamental concepts of natural-language processing through a hands-on approach with Python.\n",
    "\n",
    "By the conclusion of this workshop, you will have worked through the following:\n",
    "- Processing a collection of texts with the `spacy` Python library\n",
    "- Exploring core features of `spacy` for NLP, including tokenization, part-of-speech tagging, and named-entity recognition\n",
    "- Computing basic metrics about the dateset using some of these features\n",
    "- Exploring vector-based word-representation as a measure of document similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tips for using this Google Colab notebook\n",
    "\n",
    "When working in a Google Colaboratory notebook, `Shift-Return` (`Shift-Enter`) runs the cell you're on. You can also run the cell using the `Play` button at the left edge of the cell.\n",
    "\n",
    "There are many other keyboard shortcuts. You can access the list via the menu bar, at `Tools`-->`Command palette`. In fact, you can even customize your keyboard shortcuts using `Tools`-->`Keyboard shortcuts`.\n",
    "\n",
    "(If you're working in an Anaconda/Jupyter notebook: \n",
    " - `Control-Enter` (`Command-Return`) runs the cell you're on. You can also run the cell using the `Run` button in the toolbar. `Esc`, then `A` inserts a cell above where you are.\n",
    " - `Esc`, then `B` inserts a cell below where you are.\n",
    " - More shortcuts under `Help` --> `Keyboard Shortcuts`)\n",
    "\n",
    "You will probably get some errors in working through this notebook. That's okay, you can just go back and change the cell and re-run it.\n",
    "\n",
    "The notebook auto-saves as you work, just like gmail and most Google apps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "\n",
    "#### Defining NLP\n",
    "\n",
    "For the purposes of this workshop, natural-language processing (NLP for short) refers to the use of computational methods to analyze samples of written or spoken language produced by human beings. The term _natural_ refers to the fact that the languages in question are those that have emerged from the crucible of human speech and social interaction. An example of a language **not** considered natural would be a programming language, such as Python. Computers, of course, regularly process these kinds of languages; in a sense, that is what they are designed to do. \n",
    "\n",
    "Natural language, for many reasons, some of which we will explore below, prove far less amenable to computational processing. \n",
    "\n",
    "#### Why NLP?\n",
    "\n",
    "- To make computers better at talking to us / thinking like us (semantic search, artificial intelligence)\n",
    "- To improve digital interfaces (voice recognition)\n",
    "- To automate human communicative tasks (chatbots, autocomplete, text generation)\n",
    "- To understand human discourse computationally (textual analysis)\n",
    "- To enhance surveillance, monitoring, and control (content filtering, predictive analysis)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "[spaCy](https://spacy.io/) is a Python library for NLP. Other libraries exists, most notably [NLTK](https://www.nltk.org/) that provide similar functionality. But spaCy combines a user-friendly interface with high performance (the _Cy_ in _spaCy_ alludes to the fact that under the hood, a lot of the library is written in Cython, which is a hybrid of Python and the programming language C). \n",
    "\n",
    "If you haven't used it before, you'll probably need to install `spaCy`. (The latest version is 2.3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spacy` processes text with the aid of **models**, which are files containing numerical weights derived from neural networks. These networks are trained on linguistic and textual features like parts-of-speech and named entities (which we'll discuss below). \n",
    "\n",
    "A number of models are available for languages other than English. See the [spaCy documentation](https://spacy.io/usage/models) for more information. You can also train your own models, if you are working with special kinds of texts. \n",
    "\n",
    "But in this workshop, we'll use one of the pre-trained models for parsing English. Because the models are rather larger, they require downloading separately. \n",
    "\n",
    "We'll use the \"medium\" model for English, because the \"small\" model lacks some features it will be useful to explore. For your own projects, you might want to try the \"large\" model, which is the most fully featured and most accurate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import `spacy` and load our model. This may take a few moments.\n",
    "\n",
    "If you get an error on loading the module, try this instead:\n",
    "\n",
    "```\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also import some other Python libraries that will be helpful for our analysis. \n",
    "\n",
    "`pandas`, `numpy`, `matplotlib`, and `requests` may already be installed, if you're using a Colab notebook or an Anaconda distribution of Python. Otherwise, you can install them first as follows:\n",
    "\n",
    "```\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install matplotlib\n",
    "!pip install requests\n",
    "```\n",
    "\n",
    "The `collections` library should be part of the standard Python installation (no need to install separately)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data\n",
    "\n",
    "Now we need some textual data to work with. NLP generally works best on clean texts without typos, special characters, etc. But those often aren't the kinds of text we want to process. To simulate something more interesting, we're going to use a Twitter dataset in this workshop.\n",
    "\n",
    "The tweets in this dataset are from the official accounts of members of the United States Senate, collected between January 1, 2020 and May 7, 2020 by GW LAI's Social Feed Manager project and available on [TweetSets](https://tweetsets.library.gwu.edu/). \n",
    "\n",
    "I have downsampled this dataset to a size more manageable for live coding. The initial dataset contained 40,000 tweets. I have also removed most of the metadata from the original tweets, keeping just a few fields (in addition to the full text). \n",
    "\n",
    "If you wish to replicate my extraction and sampling process, see [this notebook](). **ADD LINK**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the `requests` library to fetch the tweet data as a JSON file and convert it to Python objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = requests.get('https://raw.githubusercontent.com/gwu-libraries/gwlibraries-workshops/master/text-analysis-python/senate-tweetset-sample-2020.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets = resp.json()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the data is a list of dictionaries, and that the text of each tweet is accessible via the `full_text` key. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the text\n",
    "\n",
    "Processing a text with Spacy will an object called a `Doc`. This process is computationally expensive, though spaCy is highly optimized, and it includes the special `.pipe` method for handling multiple texts in parallel.\n",
    "\n",
    "**Note** When I say \"text,\" I mean a Python **string**. spaCy cannot process other Python data types. \n",
    "\n",
    "We can pass a list of texts to `.pipe` instead of iterating over them with a `for` loop, and we will get back a collection of `Doc` objects.\n",
    "\n",
    "Here I'm using a list comprehension to create a list of just the strings in the `full_text` field of each tweet. Then I pass that list to `nlp.pipe`. I wrap the latter in `list` because technically, `.pipe` returns an iterable, which we can loop over, but converting it to a list will allow us to get specific elements by index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [tweet['full_text'] for tweet in tweets]\n",
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That should have taken more than a few seconds, unless you have a very fast machine. But now our texts are processed, and we can start to explore their structure.\n",
    "\n",
    "**Important** I did _not_ overwrite my `tweets` variable with the processed versions of the tweets, because the original dataset contains important metadata (like each tweet's author, number of times it was retweeted, etc.). When processing texts, you'll generally need to keep the metadata separate. So now we have two lists, `tweets` and `docs`. As long as we keep the two lists in tact and in the same order, we can easily reference back from the second to the first (e.g., to associate a given tweet's contents with its creator)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP fundamentals\n",
    "\n",
    "There are a few key components of a `Doc` that we will review. \n",
    "\n",
    "#### Tokens\n",
    "\n",
    "A token is a unique set of characters obtained from a larger string by discarding some elements of the string. That definition is vague on purpose, because **tokenization** can be achieved in different ways. Let's see if we can determine how spaCy tokenizes. \n",
    "\n",
    "Note that if I just inspect a spaCy `doc`, it doesn't look much different from a string. But to inspect the tokens in it, I can simply wrap it in Python's `list` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare that with what we get by simply splitting the **string** version of this text on whitespace with Python's builtin `.split` method. What do you notice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0]['full_text'].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python's built-in `split` command separates a string based on a single character or character-combination at a time. By default, it splits on whitespace. For English text, this leaves punctuation marks attached to the words they preceded or follow. \n",
    "\n",
    "Using regular expressions, it's possible to create more complex string separations with the `re` library in Python. But that's still not an easy task, given the occurence of tokens that actually contain punctuation: for example, _U.S._ or _U.K._ spaCy includes recipes to account for such tokens in its tokenizing routine.\n",
    "\n",
    "**Note** spaCy's tokenization performs quite well as a general rule, at least on what we might call standard English text. But if you're working with text that is non-standard or just messy, it may not be as accurate. As with most of its functionality, it's possible to customize spaCy's tokenizer to handle special cases, but that's beyond the scope of this workshop. See the [dcumentation](https://spacy.io/usage/linguistic-features#tokenization) for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Stopwords, punctuation, and URL's\n",
    "\n",
    "If you compare the two lists above, you'll see that in the list derived from our spaCy `doc`, the elements are not surrounded by single quotes. This is a signal that a spaCy `Token` is not a Python `str`. But like Python strings, which have special methods like `split` built-in, tokens have their own methods and properties. \n",
    "\n",
    "For example, spaCy attempts to mark URL's and email addresses as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The last token in the second doc is a URL.\n",
    "docs[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1][-1].like_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use some of the token's properties to identify what we might call \"content\" words in our text, preparatory to performing some form of semantic analysis. We'll most likely want to filter out punctuation. We'll also probably want to filter out **stopwords**, which in English include articles (_a_, _an_, and _the_), conjunctions (_and_, _or_), prepositions (_of_, _in_), and the like. \n",
    "\n",
    "Let's write a function that accepts a spaCy `Document` object as its argument and returns only those tokens that are neither punctuation, whitespace, stopwords, nor URL's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(doc):\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        # We include is_space because even though the default tokenization ignores the space between words, extra spaces\n",
    "        # like line breaks can register as distinct tokens\n",
    "        if not token.is_stop and not token.is_space and not token.is_punct and not token.like_url:\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stops(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, it's not perfect. In the original text, \"Monday, May 4\" is given as a date. Our function kept the \"4\" but discarded \"May\": it doesn't have a way to distinguish _May_ the month from _may_ the auxillary verb, which is a stopword. You can augment or even replace spaCy's built-in list of stopwords with your own. For instance, if your text has a lot of dates in it, you may want to remove _may_ from the list.\n",
    "\n",
    "We could also weed out tokens like \"4\" by checking the `Token.is_digit` and/or `Token.like_num` flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmas and parts-of-speech\n",
    "\n",
    "We'll use our `remove_stops` function a bit later. Now let's look at some properties of tokens that we might want to analyze across our collection of documents.\n",
    "\n",
    "spaCy is more than just a tokenizer. When we pass a string to the `nlp` function, it analyzes the text using a series of models. One of these models tags every token with its grammatical part of speech (POS). The models are probabilistic, so depending on the nature of the text, the results may be more or less accurate.\n",
    "\n",
    "We can view the POS tags using the `.pos_` attribute of any given token. (Note the underscore at the end of the attribute!) Definitions of the tags are available on the website of the [Universal Dependencies project](https://universaldependencies.org/docs/u/pos/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a dictionary comprehension to view the .pos_ attribute of the tokens in a spaCy doc, along with the token's string representation\n",
    "{token.text: token.pos_ for token in docs[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Along with the part of speech, another useful piece of token metadata is the _lemma_ of each word, which is a sort of normalized form of it intended to make comparison between different grammatical inflections (plurals, verb tense, etc.) easy to compare. We can access it via the `Token.lemma_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{token.text: token.lemma_ for token in docs[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that lemmatization in spaCy leaves proper nouns (like the _Groups_ in _Toyota Working Groups_) alone. But _practiced_ becomes _practice_, _authorities_ becomes _authority_, and _best_ becomes _good_. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Named entities\n",
    "\n",
    "So far we've looked at **syntactic** features of our dataset. spaCy also includes some tools that allow us to look at the **semantic** information in an quantitative way. It's worth pointing out (again) that semantic computational analysis is quite challenging and remains a very active area of research. spaCy isn't necessarily intended to be used on its own for this work, but rather as a pre-processing tool to feed into other kinds of tools and models capable of more sophisticated analysis.\n",
    "\n",
    "One kind of semantic analysis identifies the named entities in a collection of documents. These are words or phrases that refer to people, places, organizations, etc. Because such names can be either single words or phrases, it's not sufficient to identify the proper nouns in a text. spaCy uses a special probabilistic model to extract and classify named entities. Let's see how accurate it is.\n",
    "\n",
    "A`Document` has a property call `ents` that returns only the named entities recognized for that document. Each entity has a `label_` attribute that identifies [the classification](https://spacy.io/api/annotation#named-entities) assigned to it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the built in visualizer, we can see each document with the named entities highlighted, along with the category (the `label_`) it's been assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(docs[10], style='ent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Use `displacy` to examine a few different tweets and see how the named entity recognition has performed. What do you notice? Is it particularly bad at recognizing or classifying certain kinds of entities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's look at these entities a bit more programmatically. \n",
    "\n",
    "To start with, we can use a dictionary comprehension to create a dictionary of all the entities in the documents in our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {ent.text: ent.label_ for doc in docs for ent in doc.ents}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, the accuracy isn't bad, but it's far from perfect. A `GPE` is a geopolitical designation, like a city or a state, and those seem largley to be classified correctly. The results for `ORG` and `PERSON` are spottier. It flags certain hashtags as `PERSON`s and \"PPP\" as an `ORG`, which in thise case, it's probably not. Also, the entity models picks out some phrases in Spanish which do not appear to refer to named entities at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spacy's tagging, can begin to look at our dataset as **a collection** of texts, rather than text by text. \n",
    "\n",
    "Let's write a function a function that calculates the number of times each unique entity appears in our dataset, classified by entity label. Our function should accept a list of spaCy `Document`s and return a dictionary such that `ent_dict['PERSON']` (for instance) shows all the unique `PERSON` entities and their frequency in the collection.\n",
    "\n",
    "To do this, we'll use a couple of special Python types from the `collections` module. (We imported them earlier.)\n",
    "- `defaultdict` creates a Python dictionary whose **values** are initialized as another Python collection type, such as a list or a dictionary. `defaultdict` provides a convenient way to make a nested data structure.\n",
    "- `Counter` is a Python dictionary that initializes every value to 0. It's useful for counting a collection of objects (in this case, our lemmas.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ents(docs):\n",
    "    ent_dict = defaultdict(Counter) # Initialize our nest dictionary with a Counter\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents: # Loop over doc.ents to get the entities, not over doc, which returns the tokens\n",
    "            label = ent.label_ # The descriptive label/category\n",
    "            text = ent.text # The string representation of the entity; note that entities aren't lemmatized\n",
    "            ent_dict[label][text] += 1 # Increment the Counter associated with that label\n",
    "    return ent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict = count_ents(docs)\n",
    "ent_dict['PERSON'].most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the above to find the top 20 persons, we can observe a few things:\n",
    "- \"#COVID19\" is mistakenly labeled as a PERSON.\n",
    "- Named entities aren't lemmatized, so names like \"Bolton\" and \"John Bolton\" show up as separate entities.\n",
    "\n",
    "You can update individual entities to correct their classification, but this has to be done at the document level, so it would require writing code to update each document where the entity appears. You can also train your own entity model, but that requires having a pre-tagged dataset to train the model on. See the [spaCy docs](https://spacy.io/usage/linguistic-features#named-entities) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word and document similarity\n",
    "\n",
    "A more ambitious form of semantic analysis seeks quantitatively to represent the _meanings_ of words based on their relative similarity to other words. As you might imagine, this task is fraught with difficulty, since meaning in natural languages is so highly contextual. The words we speak or write represent just the tip of the iceberg of what we mean -- a fact attested to by how frequently a community of human speakers can disagree about the meaning of the utterances their members produce. Certain words stand out as flashpoints of controversy: think about the range of meanings people attribute to a word like _racism_ or _safety_ or _science_. But in general, this indeterminancy affects all human language. Beneath the written text or spoken utterance lies a huge weight of personal experience and collective history.\n",
    "\n",
    "NLP algorithms generally represent \"meaning\" in terms of a much narrower version of context: namely, the collocation of linguistic features across a corpous of texts. These algorithms tend either to apply sophisticated statistical techniques or -- more recently -- to make use of neural networks. But the basic premise is that \"similar\" or \"related\" words tend to occur more frequently together than dismilar or unrelated words. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy's models produce both `Token` and `Document` **vectors**, which are derived from a technique called [word2vec](https://en.wikipedia.org/wiki/Word2vec). These vectors are represented in spaCy as objects of type `numpy.array`. The latter is basically a more performant version of a Python list, optimized for numeric operations.\n",
    "\n",
    "The vectors by themselves are not terribly informative:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0][0].vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But the vectors allow us to compare two tokens, using each token's built-in `.similarity` method. Tokens with a higher score are supposedly more similar. Let's take a few tokens in isolation to illustrate.\n",
    "\n",
    "Note that we are procssing the words first with the `nlp` function to create of each a spaCy `Document` consisting of a single token. We can't compare the similarity of unprocessed Python strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the model on individual words\n",
    "banana = nlp('banana')\n",
    "orange = nlp('orange')\n",
    "apple = nlp('apple')\n",
    "dog = nlp('dog')\n",
    "cat = nlp('cat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "banana.similarity(orange)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orange.similarity(cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat.similarity(dog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems fairly reasonable to say that an orange is more similar to a banana than to a cat. I'm not sure why _cat_ and _dog_ appear so much more similar than _orange_ and _banana_, however.\n",
    "\n",
    "Also, the dubious logic of collocation appears in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dog.similarity(nlp('wolf'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It stands to reason that the accuracy of the similarity model depends on the size and nature of the corpus used to train it. You can train your own word2vec models using other Python libraries (like [Gensim](https://radimrehurek.com/gensim/)) and import the results into spaCy; this approach might be particularly useful if you're working with a corpus of fairly specialized texts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we use word-vector similarity to analyze our dataset? \n",
    "\n",
    "One approach might be to look for the documents that are most similar to a given document. spaCy automatically assigns each `Document` a vector that represents the average of its word vectors, and this is used as the basis for the `Document.similarity` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[0].similarity(docs[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could use this code to create a similarity score for every document in our collection with every other document, but that's a fairly intensive computation, since our dataset contains about 8,000 documents.\n",
    "\n",
    "For illustration, let's pick one tweet and find other tweets similar to it. We can use our original dataset to include tweet metadata in our analysis.\n",
    "\n",
    "By converting our dataset of tweets into a pandas `DataFrame`, we can leverage its fast indexing and sorting methods to isolate particular documents based on their metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame.from_records(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find Elizabeth Warren's most popular tweet.\n",
    "\n",
    "First we filter out all tweets where the name on the acount does not contain \"Warren.\"\n",
    "\n",
    "Then we sort on the `retweet_count` column to find her most popular tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warren_df = tweet_df.loc[tweet_df['name'].str.contains('Warren')]\n",
    "warren_df.sort_values(by='retweet_count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because slicing and sorting a `DataFrame` don't change its index, we can use that to find the corresponding document in our collection for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1358]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warren_tweet_index = 1358"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Can you write a function that accepts a list of documents and an index to a particular document, and then returns a dictionary mapping each document to its similarity score (measured against the document specified by index)? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compute_scores(docs, warren_tweet_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scores` object by itself doesn't tell us much, since it references each document only by its index. But it's a `Counter` object, so we can easily find the top N scores. \n",
    "\n",
    "And then we can use those entries to slice our `DataFrame` of tweets to see the tweet text and associated metadata.\n",
    "\n",
    "We have to write `score[0]` in the `DataFrame.loc[]` expression because `Counter.most_common` returns a Python tuple, the first element of which is the key -- in this case, the document index."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scores` object by itself doesn't tell us much, since it references each document only by its index. But it's a `Counter` object, so we can easily find the top N scores. \n",
    "\n",
    "And then we can use those entries to slice our `DataFrame` of tweets to see the tweet text and associated metadata.\n",
    "\n",
    "We have to write `score[0]` in the `DataFrame.loc[]` expression because `Counter.most_common` returns a Python tuple, the first element of which is the key -- in this case, the document index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.loc[[score[0] for score in scores.most_common(10)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be easier to see the full text if we just inspect the `.values` attribute of that column.\n",
    "\n",
    "We can see that our similarity scoring did a pretty good job of identifying as most similar tweets about big corporations, billionaires, and unemployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.loc[[score[0] for score in scores.most_common(10)]]['full_text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've taken our first steps toward building a document classifier! The initial results look pretty good, though as with any use of NLP, it's worth doing more exploration in order to evaluate the accuracy and robustness of the method and possibly to fine tune it. \n",
    "\n",
    "If you're interested in document classification, there are other approaches that don't use word vectors/embeddings. **Topic modeling** is among the most widely used. \n",
    "\n",
    "In recent years, **deep neural networks** have garnered a lot of attention in NLP, including for text classification. spaCy includes a `TextCategorizer` component that can be used to train a network for text classification. Unlike topic modeling and other statistical approaches, such networks usually require a labeled training dataset. \n",
    "\n",
    "See the [spaCy docs](https://spacy.io/usage/training#textcat) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because natural-language processing is such a diverse and active field, there is a wealth of resources available for further study. One place to begin is with the [O'Reilly Online Library](https://www.safaribooksonline.com/library/view/temporary-access), which has many books devoted to Python and NLP. Access to O'Reilly Online is free for GW faculty, students, and staff. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus Material: Document Similarity without Stopwords\n",
    "\n",
    "Since every token in a document has a vector, including punctuation, stopwords, and URL's, this score might be weighted by a lot of information we don't really care about. Can we make it cleaner by comparing only the vectors of \"content\" words?\n",
    "\n",
    "We can, but it requires a little bit of reverse-engineering. In what follows, we'll implement our own version of spaCy's `.similarity` method to measure the similarity between only the \"content\" words in our documents.\n",
    "\n",
    "In what follows, we're using special `numpy` methods -- the functions prefixed by `np`, which is the alias we used when import `numpy` -- to work with the word vectors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The vector of a document = average of the token vectors\n",
    "# We can use this to get the vector of the tokens minus stopwords, etc.\n",
    "def vectorize_without_stops(doc):\n",
    "    # Remember that our function remove_stops returns the list of content words in a given spaCy Document\n",
    "    # We're using numpy.array to create an array of the word vectors in that list\n",
    "    # Each word vector is already a numpy array, so we're creating a 2-dimensional array\n",
    "    vectors = np.array([token.vector for token in remove_stops(doc)])\n",
    "    # Some document vectors might have only null values, which will cause numpy to raise an error\n",
    "    # We just return a placeholder case, so that we can filter it out later\n",
    "    if not vectors.any():\n",
    "        return vectors\n",
    "    # Then we use the numpy mean method to average all those word vectors into a single vector \n",
    "    # (which is how Document.vector is created in spaCy)\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the vectorized version of our dataset\n",
    "# Each element in this list will be a single vector, representing the mean of the word vectors in a single document\n",
    "# but ONLY for the \"content\" words in that document\n",
    "doc_vecs = [vectorize_without_stops(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function does some math to find the cosine similarity \n",
    "# Based on the second answer provided here: https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
    "def cosine_sim(vec1, vec2):\n",
    "    # Each argument should be a document vector (mean of token vectors)\n",
    "    # This just returns 0 for the similarity score if one or the other of the vectors is null\n",
    "    if not vec1.any() or not vec2.any():\n",
    "        return 0\n",
    "    # This is the cosine similarity formula: the inner product of two vectors divided by the product of their norms\n",
    "    return np.inner(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_2(vectors, idx_of_target): \n",
    "    sim_scores = Counter()          # We'll again use a Counter to keep track of the scores, so that we can easily find the top scores\n",
    "    target_vec = vectors[idx_of_target]    # Assumes that our target is in the collection\n",
    "    for i, vec in enumerate(vectors): # Enumerate lets us keep track of the index of each vector in the collection\n",
    "        score = cosine_sim(target_vec, vec)   # Compute the cosine similarity\n",
    "        sim_scores[i] = score\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = compute_scores_2(doc_vecs, warren_tweet_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.loc[[score[0] for score in scores2.most_common(10)]]['full_text'].values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
