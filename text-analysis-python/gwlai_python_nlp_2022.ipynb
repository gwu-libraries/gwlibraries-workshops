{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python for Natural Language Processing (NLP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A hands-on tutorial from [GW Libraries & Academic Innovation](https://library.gwu.edu)\n",
    "\n",
    "Created & presented by Dolsy Smith, dsmith@gwu.edu.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use this tutorial\n",
    "\n",
    "This tutorial is designed for those with some prior exposure to the Python programming language and the Jupyter environment. If you're looking for an introduction to Python, check out GW LAI's [workshops on programming](https://library.gwu.edu/events?f%5B0%5D=series%3A252), which include curricula for beginners, such as [Python Camp](https://library.gwu.edu/events/python-camp-1).\n",
    "\n",
    "1. Download this notebook and open it with a Jupyter notebook running Python 3.\n",
    "   - To download from GitHub, click the `Raw` button on the menu bar directly above the title of this notebook.\n",
    "   - You should see a screen with a lot of indented text and curly braces. Right click on the screen and select `Save As...` or `Save Page As...` from your browser's pop-up menu.\n",
    "   - Save the `.ipynb` file in a folder where you can access it from your local Jupyter environment.\n",
    "2. **OR** open this Google Colab [notebook]() and copy it to your Google Drive.\n",
    "3. Execute the code cells, following the instructions above each cell where provided.\n",
    "4. If a cell is marked as a **video**, run the cell to load the embedded video content, then watch the video for an exposition of key concepts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction & Setup\n",
    "#### Video 1: Welcome\n",
    "\n",
    "Run the cell below and watch the embedded video for an introduction to this workshop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://player.vimeo.com/video/672086906?h=cdae634450\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x1352e3b20>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame('https://player.vimeo.com/video/672086906?h=cdae634450', width=\"640\", height=\"360\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 2: What is a _natural_ language?\n",
    "\n",
    "Run the cell below and watch the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://player.vimeo.com/video/672086622?h=34079fb3f7\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x135246370>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame('https://player.vimeo.com/video/672086622?h=34079fb3f7', width=\"640\", height=\"360\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installing spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[spaCy](https://spacy.io/) is a Python library for NLP, combining a user-friendly interface with high performance. \n",
    "\n",
    "Below we `pip install` the library in our local Python environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy processes text with the aid of **models**, which are files containing numerical weights derived from neural networks. These networks are trained on linguistic and textual features like parts-of-speech and named entities (which we'll discuss below). \n",
    "\n",
    "In this tutorial, we'll use one of the pre-trained models for parsing English. Because the models involve large files, they require downloading separately. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A number of models are available for languages other than English. See the [spaCy documentation](https://spacy.io/usage/models) for more information. You can also train your own models, if you are working with special kinds of texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can import `spacy` and load our model. This may take a few moments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get an error on loading the module, try running this code instead:\n",
    "\n",
    "```\n",
    "import spacy\n",
    "import en_core_web_md\n",
    "nlp = en_core_web_md.load()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also import some other Python libraries that will be helpful for our analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **pandas** library may already be installed, if you're using a Colab notebook or an Anaconda distribution of Python. Otherwise, you can install it first as follows before running the `import` command.\n",
    "\n",
    "```\n",
    "!pip install pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These modules are part of the standard library and should require no installation\n",
    "from collections import defaultdict, Counter\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 3: NLP for Twitter data\n",
    "\n",
    "Run the cell below and watch the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://player.vimeo.com/video/672082587?h=a3815c9039\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f1bdbb0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://player.vimeo.com/video/672082587?h=a3815c9039\" , width=\"640\", height=\"360\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see the code I used to prepare the dataset [here](https://github.com/gwu-libraries/gwlibraries-workshops/blob/master/text-analysis-python/workshop_data_prep.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset for this tutorial is shared with the GW community via Box.com. Here's how to access and use it:\n",
    "1. Follow [this link](https://gwu.box.com/shared/static/ngzd15ylkzrfi7i9ihs86jyprc72wshi.json) and log into Box with your GW NetID and password.\n",
    "2. Complete the two-factor authentications step (if necessary) to verify your identity.\n",
    "3. The download should begin automatically, or else a pop up will ask you what you want to do with the file. If prompted, choose the option to save it to your computer.\n",
    "4. The file is called `senate-tweetset-sample-2020.json`. Locate this file in your Downloads folder (or wherever you have saved it on your computer).\n",
    "5. If you are running this notebook in your own Jupyter environment (not in the cloud), move the file to the same folder where this notebook is saved.\n",
    "6. If you are running this notebook in Google Colab, do the following:\n",
    "   \n",
    "   a. From the menu on the left-hand side of your notebook, click the folder icon to open the Files panel.\n",
    "   \n",
    "   b. Click the file upload button and use the files browser to select the JSON file from step 4 from the folder where you have saved it on your computer. \n",
    "   \n",
    "   c. After uploading, `senate-tweetset-sample-2020.json` should appear in the Files panel of your Colab notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we're ready to do some NLP!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing text with spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the JSON dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step is to load the JSON file you made available to your Jupyter/Colab environment. We'll use the `json` library to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('senate-tweetset-sample-2020.json', 'r') as f:\n",
    "    tweets = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you get a `FileNotFoundError` when running the cell above, make sure that the file you downloaded is in the same folder as your Jupyter notebook (or that you have uploaded it to your Colab notebook environment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our `tweets` variable should now hold a list of Python dictionaries. Let's inspect the first one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the text of each tweet is accessible via the `full_text` key. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets[0]['full_text']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processing the text\n",
    "\n",
    "Processing a Python string with spaCy yields an object called a `Doc`. Since our dataset consists of several thousand documents, we can use the `.pipe` method for processing a collection of strings in parallel.\n",
    "\n",
    "First we create a list of just the strings in the `full_text` field of each Tweet. Then we pass that list to `nlp.pipe`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [tweet['full_text'] for tweet in tweets]\n",
    "docs = list(nlp.pipe(texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the result of the `nlp.pipe` method call in a call to the Python `list` function, which will allow us to access each parsed document by its position in the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note** \n",
    "\n",
    "We did not overwrite our `tweets` variable because the original dataset contains useful metadata such as the account that authored each Tweet. Now we have two lists, `tweets` and `docs`. As long as we keep the two lists intact and in the same order, we can reference back from the second to the first when we need any of those metadata elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NLP fundamentals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 4: Tokenization\n",
    "\n",
    "Watch the video below for an explanation of _tokens_ in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://player.vimeo.com/video/672083751?h=3a5f963601\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f1bdd00>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://player.vimeo.com/video/672083751?h=3a5f963601\" , width=\"640\", height=\"360\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 5: Stopwords\n",
    "\n",
    "Watch the video below for an explanation of _stopwords_ in NLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://player.vimeo.com/video/672085745?h=31a7425a86\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f1bdd60>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://player.vimeo.com/video/672085745?h=31a7425a86\" , width=\"640\", height=\"360\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filtering stopwords and other kinds of tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With spaCy, we can use a token's properties to filter out those that may not be relevent to our analysis. In the cell below, we create a function that accepts an instance of spaCy `Document` class as its argument and returns only those tokens in the document that are **not** one of the following:\n",
    " - punctuation\n",
    " - white space\n",
    " - stopwords \n",
    " - URL's\n",
    " \n",
    "**Note**\n",
    " \n",
    "We filter on white space because even though spaCy removes the white space between words when tokenizing, it treats any extra white spaces, like tabs and line breaks, as separate tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_stops(doc):\n",
    "    '''\n",
    "    Returns a list of spaCy tokens, excluding stopwords and other less semantically relevant content\n",
    "    :param doc: a spaCy Document object\n",
    "    '''\n",
    "    tokens = []\n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_space and not token.is_punct and not token.like_url:\n",
    "            tokens.append(token)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's test our function on a Tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_stops(docs[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the original text, `Monday, May 4` is given as a date. Our function kept the `4` but discarded `May`: it doesn't have a way to distinguish `May` the month from `may` the auxillary verb, which is a stopword. \n",
    "\n",
    "You can [augment or even replace](https://spacy.io/usage/linguistic-features#language-subclass) spaCy's built-in list of stopwords with your own. For instance, if your text has a lot of dates in it, you may want to remove `may` from the list.\n",
    "\n",
    "We could also weed out tokens like `4` by checking the `Token.is_digit` and/or `Token.like_num` flags."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmas and parts-of-speech"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy is more than just a tokenizer. When we pass a string to the `nlp` function, it analyzes the text using a series of models. One of these models tags every token with its grammatical part of speech (POS). The models are probabilistic, so depending on the nature of the text, the results may be more or less accurate.\n",
    "\n",
    "We can view the POS tags using the `.pos_` attribute of any given token. (Note the underscore at the end of the attribute!) Definitions of the tags are available on the website of the [Universal Dependencies project](https://universaldependencies.org/docs/u/pos/) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{token.text: token.pos_ for token in docs[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another useful piece of token metadata is the _lemma_ of each word, which is a normalized form intended to facilitate comparisons between different grammatical inflections (plurals, verb tense, etc.). We can access it via the `Token.lemma_` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{token.text: token.lemma_ for token in docs[0]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that lemmatization in spaCy leaves proper nouns (like the `Groups` in `Toyota Working Groups`) alone. But `practiced` becomes `practice`, `authorities` becomes `authority`, and `best` becomes `good`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 6: Named entities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://player.vimeo.com/video/672077809?h=4008747699\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f1bd5b0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://player.vimeo.com/video/672077809?h=4008747699\" , width=\"640\", height=\"360\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A spaCy `Document` has a property call `ents` that returns only the named entities recognized for that document. Each entity has a `label_` attribute that identifies [the classification](https://spacy.io/usage/linguistic-features#accessing-ner) assigned to it.\n",
    "\n",
    "Below we use a dictionary comprehension to create a dictionary of all the entities in the documents in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = {ent.text: ent.label_ for doc in docs for ent in doc.ents}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the built-in visualizer, we can see each document with the named entities highlighted, along with the category (the `label_`) it's been assigned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "displacy.render(docs[0], style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercise\n",
    "\n",
    "Use `displacy` to examine a few different tweets and see how the named-entity recognition has performed. What do you notice? Is it particularly bad at recognizing or classifying certain kinds of entities?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing named entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using spaCy's tagging, we can begin to look at our dataset as **a collection** of texts, rather than text by text. \n",
    "\n",
    "Below we write a function that calculates the number of times each unique entity appears in our dataset, classified by entity label. Our function should accept a list of spaCy `Document` objects and return a dictionary such that `ent_dict['PERSON']` (for instance) shows all the unique `PERSON` entities and their frequency in the collection.\n",
    "\n",
    "To do this, we'll use a couple of special Python types from the `collections` module. (We imported them earlier.)\n",
    "- `defaultdict` creates a Python dictionary whose **values** are initialized as another Python collection type, such as a list or a dictionary. `defaultdict` provides a convenient way to make a nested data structure.\n",
    "- `Counter` is a Python dictionary that initializes every value to 0. It's useful for counting a collection of objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ents(docs):\n",
    "    '''\n",
    "    Creates a nested dictionary: the top-level contains types of named entities. \n",
    "    Each named-entity entry contains a count of how many times each particular entity appears in the dataset.\n",
    "    :param docs: A list of spaCy document objects\n",
    "    '''\n",
    "    ent_dict = defaultdict(Counter) # Initialize our nested dictionary with a Counter\n",
    "    for doc in docs:\n",
    "        for ent in doc.ents: # Loop over doc.ents to get the entities, not over doc, which returns the tokens\n",
    "            label = ent.label_ # The descriptive label/category\n",
    "            text = ent.text # The string representation of the entity; note that entities aren't lemmatized\n",
    "            ent_dict[label][text] += 1 # Increment the Counter associated with that label\n",
    "    return ent_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ent_dict = count_ents(docs)\n",
    "ent_dict['PERSON'].most_common(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run the above to find the top 20 persons, we can observe a few things:\n",
    "- `#COVID19` is mistakenly labeled as a `PERSON`.\n",
    "- Named entities aren't lemmatized, so names like `Bolton` and `John Bolton` show up as separate entities.\n",
    "\n",
    "You can update individual entities to correct their classification, but this has to be done at the document level, so it would require writing code to update each document where the entity appears. You can also train your own entity model, but that requires a pre-tagged dataset. See the [spaCy docs](https://spacy.io/usage/linguistic-features#named-entities) for more info."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word and document similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 7: Word vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://player.vimeo.com/video/672086104?h=7586ab4062\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f1bda90>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://player.vimeo.com/video/672086104?h=7586ab4062\"  , width=\"640\", height=\"360\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Analyzing document similarity with word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "spaCy's token- and document-level vectors  are derived from a technique called [word2vec](https://en.wikipedia.org/wiki/Word2vec). These vectors are represented in spaCy as objects of type `numpy.array`. \n",
    "\n",
    "We could use these vectors to create a similarity score between every document in our collection and every other document, but that's a fairly intensive computation, since our dataset contains about 8,000 documents.\n",
    "\n",
    "For illustration, let's pick one Tweet and find other Tweets similar to it. We can use our original dataset to include Tweet metadata in our analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By converting our dataset into a pandas `DataFrame` object, we can leverage the latter's fast indexing and sorting methods to isolate particular documents based on their metadata."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df = pd.DataFrame.from_records(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's find Elizabeth Warren's most popular Tweet.\n",
    "1. Filter out all Tweets where the name on the acount does not contain the string `Warren`.\n",
    "2. Sort on the `retweet_count` column to find her most popular Tweet.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warren_df = tweet_df.loc[tweet_df['name'].str.contains('Warren')]\n",
    "warren_df.sort_values(by='retweet_count', ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because slicing and sorting a `DataFrame` don't change its index, we can use that to find the corresponding document in our collection for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs[1358]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warren_tweet_index = 1358"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we write a function that will take 1) a collection of documents, and 2) an index to a document in that collection that serves as the comparator (the _target_). We'll calculate the similarity score between that comparator document and every other document, returning the scores in a `Counter` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores(docs, idx_of_target): \n",
    "    '''\n",
    "    Creates a similarity score between each document in the collection and the document with the provided index.\n",
    "    :param docs: a list of spaCy documents\n",
    "    :param idx_of_target: an integer corresponding the index in the list of the comparator document\n",
    "    '''\n",
    "    sim_scores = Counter()\n",
    "    # Assumes that our target is in the collection\n",
    "    target = docs[idx_of_target]  \n",
    "    # Enumerate lets us keep track of the index of each document in the collection\n",
    "    for i, doc in enumerate(docs): \n",
    "         # Compute the cosine similarity\n",
    "        score = doc.similarity(target)  \n",
    "        sim_scores[i] = score\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = compute_scores(docs, warren_tweet_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scores` object by itself doesn't tell us much, since it references each document only by its index. But it's a `Counter` object, so we can easily find the top N scores. \n",
    "\n",
    "And then we can use those entries to slice our `DataFrame` of Tweets to see the Tweet text and associated metadata.\n",
    "\n",
    "We have to write `score[0]` in the `DataFrame.loc[]` expression because `Counter.most_common` returns a Python tuple, the first element of which is the key -- in this case, the document index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.loc[[score[0] for score in scores.most_common(10)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be easier to see the full text if we just inspect the `.values` attribute of that column.\n",
    "\n",
    "We can see that our similarity scoring did a pretty good job of identifying as most similar Tweets about big corporations, billionaires, and unemployment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.loc[[score[0] for score in scores.most_common(10)]]['full_text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we've taken our first steps toward building a document classifier! The initial results look pretty good, though as with any use of NLP, it's worth doing more exploration in order to evaluate the accuracy and robustness of the method and possibly to fine-tune it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Video 8: Word vectors without stopwords\n",
    "\n",
    "The following video explains the code below for calculating a document similarity score without stopwords, punctuation, white space, or URL's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"640\"\n",
       "            height=\"360\"\n",
       "            src=\"https://player.vimeo.com/video/672087275?h=81d1d33bf2\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x10f1bdf10>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IFrame(\"https://player.vimeo.com/video/672087275?h=81d1d33bf2\"  , width=\"640\", height=\"360\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_without_stops(doc):\n",
    "    '''\n",
    "    Creates a document vector based on the vectors of all tokens filtered by the remove_stops function\n",
    "    :param doc: a spaCy Document object\n",
    "    '''\n",
    "    vectors = np.array([token.vector for token in remove_stops(doc)])\n",
    "    if not vectors.any():\n",
    "        return vectors\n",
    "    return np.mean(vectors, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_vecs = [vectorize_without_stops(doc) for doc in docs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_sim(vec1, vec2):\n",
    "    '''\n",
    "    Computes the cosine similarity between two numpy vectors.\n",
    "    :param vec1, vec2: numpy arrays\n",
    "    '''\n",
    "    if not vec1.any() or not vec2.any():\n",
    "        return 0\n",
    "    return np.inner(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_scores_2(vectors, idx_of_target): \n",
    "    '''\n",
    "    Computes the scores between a collection of document vectors and the vector of one document in that collection.\n",
    "    :param vectors: a list of document vectors (numpy arrays)\n",
    "    :param idx_of_target: an integer representing the index of the document in the collection to be compared against\n",
    "    '''\n",
    "    sim_scores = Counter()         \n",
    "    target_vec = vectors[idx_of_target]   \n",
    "    for i, vec in enumerate(vectors): \n",
    "        score = cosine_sim(target_vec, vec)   # Compute the cosine similarity\n",
    "        sim_scores[i] = score\n",
    "    return sim_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores2 = compute_scores_2(doc_vecs, warren_tweet_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet_df.loc[[score[0] for score in scores2.most_common(10)]]['full_text'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resources for further exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're interested in document classification, there are other approaches that might be of interest. \n",
    "- The [gensim](https://radimrehurek.com/gensim_3.8.3/) Python library includes support for topic modeling, as well as tools for creating custom word embeddings.\n",
    "- spaCy includes a [TextCategorizer](https://spacy.io/api/textcategorizer) component that can be used to train a neural network for text classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A wealth of resources exist on natural language processing and NLP with Python, many of them freely available on the web.\n",
    "\n",
    "The [spaCy documentation](https://spacy.io/) is a great place to start.\n",
    "\n",
    "For members of the GW community, the [O'Reilly Online Library](https://www.safaribooksonline.com/library/view/temporary-access) provides access to a wide variety of ebooks on this topic, including titles like [Natural Language Processing with Python and spaCy](https://learning.oreilly.com/library/view/natural-language-processing/9781098122652/) (No Starch Press, 2020) and [Practical Natural Language Processing with Python](https://learning.oreilly.com/library/view/practical-natural-language/9781484262467/?ar=) (Apress, 2020)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding assistance for GW affiliates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Current GW faculty, students, and staff can [get assistance](https://library.gwu.edu/program-and-code) with research projects involving Python by scheduling a [coding consultation](https://calendly.com/gwul-coding)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
