{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook-instructor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuIowtekqdKO"
      },
      "source": [
        "Remember to periodically save!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3y8TlR2lFM-"
      },
      "source": [
        "# Web Scraping with Requests-HTML\n",
        "\n",
        "Documentation: https://requests.readthedocs.io/projects/requests-html/en/latest/\n",
        "\n",
        "If you've used Python for getting web pages, connecting to an API, or getting files off the web, odds are you've encountered the requests library. The library we're using today, Requests-HTML (note the different name) was developed by the author of the requests, Kenneth Reitz.  It's intended to be an easy-to-learn web scraping library, it's basically a layer on top of requests and BeautifulSoup. BeautifulSoup is another good basic Python library for parsing HTML. Lots of tutorials and documentation on it. \n",
        "\n",
        "\n",
        "## Part 1: Scraping headlines from the GW Hatchet\n",
        "\n",
        "Before we start scraping, let's look at the robots.txt file for the site. This is a list of instructions to crawlers--usually search engines--about how to crawl the site and which parts to stay away from. \n",
        "\n",
        "https://www.gwhatchet.com/robots.txt\n",
        "\n",
        "Note that this site is very open about crawling. That doesn't mean there aren't still legal and ethical considerations, but we're respecting their instructions about the site. \n",
        "\n",
        "\n",
        "**Getting started**\n",
        "\n",
        "First we need to install the `requests_html` Python library using the pip command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqrCkPF_h6Lq"
      },
      "source": [
        "!pip install requests_html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ULpyL6Gh7Jb"
      },
      "source": [
        "from requests_html import HTMLSession"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwCI4gvvoq5C"
      },
      "source": [
        "We  need to make an HTTP GET request to get the website HTML. To do this we create an HTMLSession. That session object has methods to make GET requests. \n",
        "\n",
        "The get() method takes the URL as an argument. Sends an HTTP GET request. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi2LtNMAiInM"
      },
      "source": [
        "session = HTMLSession()\n",
        "\n",
        "r = session.get('https://www.gwhatchet.com/section/news/')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "ACd8zgLk5kru",
        "outputId": "12e4c25b-3a81-4be1-e925-1a8f2d9ac18b"
      },
      "source": [
        "r.html"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<HTML url='https://www.gwhatchet.com/section/news/'>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FjCtI0k3BI"
      },
      "source": [
        "`r` is a Response object and it has an html attribute which contains all of the HTML from the page in Unicode. We can look at the web page's text using the text attribute.  This is like looking at the text content of all of the HTML tags. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI7rZ_7ciNVo"
      },
      "source": [
        "r.html.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6RLw4p6kyOi"
      },
      "source": [
        "This html attribute also has many other attributes which are shortcuts to parts of the content. There is a links attribute which extracts all of the links in the page. Note that these are returned in alphabetical order, not the order on the page. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll7C6qZoiYsU"
      },
      "source": [
        "r.html.links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "KVBoUxmOftp7",
        "outputId": "94808a07-5926-4c1b-b93d-924b6356e88d"
      },
      "source": [
        "r.html.find('footer')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Element 'footer' >]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdNug2oCwCVP"
      },
      "source": [
        "Now let's work at extracting certain parts of the page. Suppose we want all of the headlines. First we need to examine the page and determine if there is any markup that helps us identify headlines.  \n",
        "\n",
        "*Bring up the HTML of the page.*\n",
        "\n",
        "We use CSS selectors to specify tags, classes, ids, or other attributes. \n",
        "https://www.w3schools.com/cssref/css_selectors.asp\n",
        "\n",
        "We can see that all of the headlines are in h2 tags with a class of post-title. If there is a class that identifies the elements we want, that's ideal. \n",
        "\n",
        "In this case, we can use the h2 tag OR we could also use the class. *Show both parameters.*\n",
        "\n",
        "We can now use the html object's find() method to provide a CSS selector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2N1H7ouwNOI"
      },
      "source": [
        "titles = r.html.find('h2')\n",
        "#titles = r.html.find('post-title')"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "TSbc5hC6xjv2",
        "outputId": "de711b0d-608c-4441-f129-f3edc8109ba5"
      },
      "source": [
        "type(titles)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RCGecahk1TG"
      },
      "source": [
        "titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRWbUOVYx_ik"
      },
      "source": [
        "`titles` is a list of Elements (capital E). We can access just the text content of those Elements using the `full_text` attribute. Let's look at just the first one to start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "bs3H_K3jyfZP",
        "outputId": "fdda4d43-312a-4191-e57b-e0e99ff6a375"
      },
      "source": [
        "titles[0].text"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'DDOT releases first-ever Foggy Bottom traffic crash data'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlBMFvwxy4Q3"
      },
      "source": [
        "Now let's grab the text of all of the headlines. We'll iterate through the Elements in the titles list and add to our headlines list the text attribute from each one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "G-8jwZjWx9zg",
        "outputId": "d2b6aaf5-b8fb-478e-abe9-a61f4c0a7aa7"
      },
      "source": [
        "headlines = [t.text for t in titles]\n",
        "print(headlines)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DDOT releases first-ever Foggy Bottom traffic crash data', \"Faculty call on officials to enact a 'cluster hire' of minority faculty\", 'GW marks 200 years, begins monthslong celebration', \"Board delays decision on next year's cost of attendance\", 'Students rely on hobbies to maintain mental health during pandemic', \"Officials debut task force on the future of GW's academics\", \"GW Hillel launches program to aid senior citizens' vaccine registrations\", 'COVID-19 a factor in higher volume of research output this year', 'School Without Walls reopens with quarter of student body', 'RHA expands programming as more students return to campus', 'Sections', 'Reader Services', 'Learn More', 'Get in Touch']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3S0uHnSzu75"
      },
      "source": [
        "That's fine for one page, but if we want to get a lot more headlines, we'll need to check multiple pages. Let's look at the Hatchet website and see which pages have new articles. \n",
        "\n",
        "*Return to Hatchet website and page through news section.*\n",
        "\n",
        "Note: there are only 10 headlines per page, and then we need to click Next to go to another page. Clicking on that, we can see that there is number in the URL. So seeing that, we can use Python to get the HTML of each page and then get the headlines from each one. \n",
        "\n",
        "Before we jump in and do that, we're at the point where we should consider what is the most polite way to get this content from the Hatchet website. When you're web scraping, you don't want to interfere with a website's regular traffic by requesting lots of content all at one. We've got a lot of us online in this workshop, so let's give our requests a little space. We'll pause a second between requesting each page. \n",
        "\n",
        "To do that pausing, we need to import another library in the Python standard library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGK-CL3H44jj"
      },
      "source": [
        "from time import sleep"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "FZeKIPvkyqi7",
        "outputId": "c392796d-951c-44c7-c7ba-55a624cb5a20"
      },
      "source": [
        "news = []\n",
        "\n",
        "# get the first 4 pages\n",
        "for i in range(1,5):\n",
        "    resp = session.get('https://www.gwhatchet.com/section/news/page/{}'.format(i))\n",
        "    #titles = resp.html.find('h2')\n",
        "    titles = resp.html.find('.post-title')\n",
        "    headlines = [t.text for t in titles]\n",
        "    news.extend(headlines)\n",
        "    sleep(1)\n",
        "\n",
        "print(news)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['DDOT releases first-ever Foggy Bottom traffic crash data', \"Faculty call on officials to enact a 'cluster hire' of minority faculty\", 'GW marks 200 years, begins monthslong celebration', \"Board delays decision on next year's cost of attendance\", 'Students rely on hobbies to maintain mental health during pandemic', \"Officials debut task force on the future of GW's academics\", \"GW Hillel launches program to aid senior citizens' vaccine registrations\", 'COVID-19 a factor in higher volume of research output this year', 'School Without Walls reopens with quarter of student body', 'LGBTQ business student group seeks to build community', 'RHA expands programming as more students return to campus', 'Crosswalk among renovations installed at intersection', 'Crime log: Nine Title IX cases filed with GWPD last week', 'Human rights experts analyze ethnic minority experiences in China', 'Panel addresses vaccine misinformation, hesitancy', 'Officials exploring options for in-person Commencement in May', 'Students ramp up D.C. statehood calls amid growing national attention', 'SA executive vice president applications open', 'Faculty-wide survey of LeBlanc closes', 'Law professor declines to defend Trump for impeachment', 'Foggy Bottom, VSTC delay opening Tuesday', 'Crime log: GWPD receives tip on sexual assault case', 'Black student groups celebrate identities with themed events', 'Dining representatives boost social media use during COVID-19', '1,500 students return to campus housing for spring', 'Hiring freeze limited course offerings this year: faculty', 'Biden rolls out major higher education policies in first days', 'LeBlanc discusses plans for 2021, takeaways from virtual learning', \"D.C.'s vaccine rollout plagued by racial, geographic disparities\", 'Roosevelt at GW launches frontline workers advocacy campaign', 'Faculty concerned about library funding after budget cuts', 'Regulatory Studies Center research professor dies at 58', 'BHC kicks off with virtual celebration of Black identities', 'GW Hospital fined $100,000 for alleged hazardous waste violations', 'Best-selling author discusses caste systems in latest book', 'Brookins steps down as SA president', 'Campus shuts down ahead of snowstorm', 'Panel breaks down takeaways from Capitol riot', 'Officials to assess diversity on campus with external audit', 'ANC approves dedication of Khashoggi, RBG roadways']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "p0jJgzTK5sz4",
        "outputId": "76500d77-cd7d-4ebe-e71e-0963dacd9d43"
      },
      "source": [
        "len(news)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcVHkZvRi6a7"
      },
      "source": [
        "Now that you have this data, you could go on to do other analysis of it. Maybe you want to look for mentions of certain entitites or do sentiment analysis or topic modelling of the headlines. There's a workshop on Natural Language Processing, or NLP tomorrow at 1PM if you want to learn more about working with text data: https://library.gwu.edu/news-events/events/python-natural-language-processing-0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IrUJRuT7lny"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "Collect the bylines from the news-articles on the page. Start with one page of news articles. \n",
        "\n",
        "Hints:\n",
        "1. determine what tag or class identifies the text you want. \n",
        "2. use the find() method to identify the element and collect them into a list. Assign that to a variable called bylines. \n",
        "3. Extract the text from each element and assign the results to a list called authors. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1R-7v5W8j23"
      },
      "source": [
        "# Note that if you use just byline-author as below you will also pick up the text \"By\". \n",
        "# bylines = r.html.find('.byline-author')\n",
        "# bylines[0].text"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_m9UPlx7qaF"
      },
      "source": [
        "bylines = r.html.find('.byline-author > a')"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "D9LE4BqG76if",
        "outputId": "6d9d6b7b-242b-4eac-d297-256f4b0cf062"
      },
      "source": [
        "bylines[0].text"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Chow Paueksakon'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6KnrU_2p77O1",
        "outputId": "7ea2385c-dd49-4077-8d8a-df844c4f3368"
      },
      "source": [
        "authors = [b.text for b in bylines]\n",
        "authors"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Chow Paueksakon',\n",
              " 'Ishani Chettri',\n",
              " 'Nicholas Pasion',\n",
              " 'Zach Schonfeld',\n",
              " 'Zach Schonfeld',\n",
              " 'Abby Kennedy',\n",
              " 'Yankun Zhao',\n",
              " 'Brennan Fiske',\n",
              " 'Lia DeGroot',\n",
              " 'Michelle Vassilev',\n",
              " 'Clara Duhon',\n",
              " 'Jarrod Wardwell',\n",
              " 'Yutong Jiang']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue35f0Ho-Vld"
      },
      "source": [
        "If you want to use this data to get a count of how many times each author has been published, you could count them. There is a count() method in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "uksVpVLX8mLx",
        "outputId": "91b26109-eb3e-4773-b907-fd766c1b834c"
      },
      "source": [
        "authors.count(\"Jarrod Wardwell\")"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QobayzUUb8DP"
      },
      "source": [
        "The .count() method is a bit inefficient for our purposes because you have to use the count method with each possible author's name. Better to use Python's Counter from the collections module. We can give Counter a list and it will return a count of each value that appears. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "zwRBSDBn8GXt",
        "outputId": "9f36f56b-3f19-453d-a534-5f7eb34cb878"
      },
      "source": [
        "from collections import Counter\n",
        "a = Counter(authors)\n",
        "a"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Abby Kennedy': 1,\n",
              "         'Brennan Fiske': 1,\n",
              "         'Chow Paueksakon': 1,\n",
              "         'Clara Duhon': 1,\n",
              "         'Ishani Chettri': 1,\n",
              "         'Jarrod Wardwell': 1,\n",
              "         'Lia DeGroot': 1,\n",
              "         'Michelle Vassilev': 1,\n",
              "         'Nicholas Pasion': 1,\n",
              "         'Yankun Zhao': 1,\n",
              "         'Yutong Jiang': 1,\n",
              "         'Zach Schonfeld': 2})"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjXqHE6kzIkO"
      },
      "source": [
        "## Part 2: More scraping! Scraping course listings from the Schedule of Classes\n",
        "\n",
        "**Have you saved lately? Be sure to save your notebook file!**\n",
        "\n",
        "*Bring up the GW Schedule of Classes* \n",
        "https://my.gwu.edu/mod/pws/subjects.cfm?campId=1&termId=202101\n",
        "\n",
        "Recommend starting with one page, and then building up as you determine how to access the data you need. We'll start with just the American Studies courses for Spring. *Drill down to the Main Campus Spring semester and then select American Studies*.\n",
        "\n",
        "Let's suppose we want to have a spreadsheet listing all of the courses in our department for our major. \n",
        "\n",
        "We will need to parse the HTML to get the information in each of these rows. (We'll ignore the comments and Find Books links.)\n",
        "\n",
        "###robots.txt\n",
        "Before doing anything, let's look at the robots.txt\n",
        "\n",
        "https://my.gwu.edu/robots.txt\n",
        "Our page is in /mod/pws and there are no prohibitions about crawling that content. \n",
        "\n",
        "**Now we can move on and look at the HTML for the page we want:**\n",
        "*View Page Source. Discuss HTML tags used for tables: table, tr, td, etc.*\n",
        "\n",
        "Relevant tags and courses include the `tr` tag in the tables. It looks like we have a class on each row which we could use, `.crseRow1`. We could look for all of the `td`s within `.crseRow1`, but then we would lose the contxt of which row they were in. \n",
        "\n",
        "Let's see what we get with the class that's on each row in each table. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGGI44Op95Ru"
      },
      "source": [
        "session_new = HTMLSession()\n",
        "\n",
        "amst_url = 'https://my.gwu.edu/mod/pws/courses.cfm?campId=1&termId=202101&subjId=AMST'\n",
        "response = session_new.get(amst_url)"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "C-OjmlkW4-cL",
        "outputId": "b58764ab-9279-482e-f44c-3c4415f81690"
      },
      "source": [
        "# Might be thinking we could look for all of the td's within .crseRow1\n",
        "rows = response.html.find('.crseRow1 > td')\n",
        "rows[0].text"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'OPEN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "SJGRCuWz5Kkb",
        "outputId": "45225eef-3557-49f6-bad9-8efe09912c32"
      },
      "source": [
        "len(rows)\n",
        "#We have a long list of cells, and we would lose the contxt of which row they were in. "
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "495"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEZkavki5ZTj"
      },
      "source": [
        "Instead, let's try to work with each row, so we have a list of rows, and drill down to extract the td content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "NaEGkVj0zWDY",
        "outputId": "aee593b9-8b00-4198-af10-a45aff3802f7"
      },
      "source": [
        "trs = response.html.find('.crseRow1')\n",
        "trs[0].text"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'OPEN\\n15114\\nAMST 1000\\n10\\nBodies of Work\\n3.00\\nIvy, N\\nREMOTE INSTR\\nM\\n12:45PM - 03:15PM\\n01/11/21 - 04/26/21'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXmzzQLm4tUV"
      },
      "source": [
        "We get back a string, with each cell's text concatenated. It looks like the text for each cell is separated by a `\\n` newline character. We still have access to the elements within the table rows object. For example, we can look at the cells (`td` tags) in our first row as follows: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ9RJRQX4brK"
      },
      "source": [
        "cells = trs[0].find('td')"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "5FNGRMPB7wtD",
        "outputId": "7d118d28-4fea-49c9-91b9-32cc65a810b1"
      },
      "source": [
        "cells[0].text"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'OPEN'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y2NVTVw8aqJ"
      },
      "source": [
        "It appeared that there is a newline between each cell, which could be useful as a delimiter, to separate the td's in each row. However, looking at the site, we can see that the time for the class is on a different line from the day of the week. Let's take a look at each cell: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "CVHzV5q-8SKg",
        "outputId": "83b957e1-bfd6-44fc-b9f3-2b2c5702366a"
      },
      "source": [
        "for c in cells:\n",
        "    print(\"td: \",c.text)"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "td:  OPEN\n",
            "td:  15114\n",
            "td:  AMST 1000\n",
            "td:  10\n",
            "td:  Bodies of Work\n",
            "td:  3.00\n",
            "td:  Ivy, N\n",
            "td:  REMOTE INSTR\n",
            "td:  M\n",
            "12:45PM - 03:15PM\n",
            "td:  01/11/21 - 04/26/21\n",
            "td:  \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPAxaznY_I5T"
      },
      "source": [
        "Now that we know how to access the text of each `td` and that there are some extra newlines in there, let's create a function that will strip out newlines in the text of the td tags. Our function will return a list that has the text of all of the cells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXTOGm368WDt"
      },
      "source": [
        "def get_tds(tr):\n",
        "    row = [td.text.replace('\\n', ' ') for td in tr.find('td')]\n",
        "    return row"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPcM1Kf6_xk_"
      },
      "source": [
        "Now we can use this to get all the relevant text in all of the course tables on the page, and collect them into a courses list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx86mOG-_d6f"
      },
      "source": [
        "courses = []\n",
        "for tr in trs:\n",
        "    courses.append(get_tds(tr))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmNPksDl_wOu"
      },
      "source": [
        "# Look at courses. Note that there are some here that don't appear on the web page! \n",
        "# That's because there is display:none as a style on some of the tables. \n",
        "\n",
        "courses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzDaqrR3Al6F"
      },
      "source": [
        "Now that we've extracted this data, we want to hold onto it. We'll create a CSV file to hold this data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVWHER9zABQL"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('amst.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    for course in courses:\n",
        "        writer.writerow(course)"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "RhGRgC96AHSb",
        "outputId": "44467f18-e2d3-41d6-d7c3-5e48b5751ae0"
      },
      "source": [
        "!head amst.csv"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "OPEN,15114,AMST 1000,10,Bodies of Work,3.00,\"Ivy, N\",REMOTE INSTR,M 12:45PM - 03:15PM,01/11/21 - 04/26/21,\r\n",
            "WAITLIST,17707,AMST 1000,11,Media Culture & COVID,3.00,\"McAlister, M\",REMOTE INSTR,MW 02:20PM - 03:35PM,01/11/21 - 04/26/21,\r\n",
            "OPEN,18875,AMST 1000,12,Nature & Culture of Children,3.00,\"Cohen-Cole, J\",REMOTE INSTR,W 03:30PM - 06:00PM,01/11/21 - 04/26/21,\r\n",
            "WAITLIST,17948,AMST 1050,10,Race and Racism in US History,3.00,\"Guglielmo, T\",REMOTE INSTR,MW 09:35AM - 10:50AM,01/11/21 - 04/26/21,\r\n",
            "CLOSED,17949,AMST 1200,10,The Sixties in America,3.00,\"Osman, S\",REMOTE INSTR,TR 11:10AM - 12:00PM,01/11/21 - 04/26/21,Linked\r\n",
            "WAITLIST,17950,AMST 1200,30,Discussion,0.00,\"Osman, S\",REMOTE INSTR,R 09:35AM - 10:25AM,01/11/21 - 04/26/21,Find Books\r\n",
            "WAITLIST,17951,AMST 1200,31,Discussion,0.00,\"Osman, S\",REMOTE INSTR,R 12:45PM - 01:35PM,01/11/21 - 04/26/21,Find Books\r\n",
            "WAITLIST,17952,AMST 1200,32,Discussion,0.00,\"Osman, S\",REMOTE INSTR,R 02:20PM - 03:10PM,01/11/21 - 04/26/21,Find Books\r\n",
            "WAITLIST,17953,AMST 1200,33,Discussion,0.00,\"Osman, S\",REMOTE INSTR,R 03:45PM - 04:35PM,01/11/21 - 04/26/21,Find Books\r\n",
            "CLOSED,12750,AMST 2020W,80,\"Washington, DC: History, Culture, and Politics\",3.00,\"Klemek, C\",REMOTE INSTR,R 11:10AM - 01:00PM,01/11/21 - 04/26/21,XList\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdkgWVTYAvC7"
      },
      "source": [
        "We could also load this CSV into pandas, if we wanted to use some of its filtering and analysis methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emcYgBGCAImk"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpNe9d1kATRU"
      },
      "source": [
        "df = pd.DataFrame(courses, columns = ['STATUS', 'CRN', 'SUBJECT', 'SECT', 'COURSE', 'CREDIT', 'INSTR.', 'BLDG/RM', 'DAY/TIME', 'FROM / TO', 'Books'])"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "retCJ-C_ATl-"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKu0c8nRAUfE"
      },
      "source": [
        "df[df['STATUS'] == \"OPEN\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz26ssh3A2aw"
      },
      "source": [
        "Now that we've done this on one page, what might be next steps?\n",
        "\n",
        "* Create a function to modularize the work we did. THen apply this to other pages (Note that there were 2 pages for AMST) and other departments. \n",
        "* Consider how you can be respectful of the website. Where could you insert some sleep statements? \n",
        "* Maybe there are better times of day to run this kind of scraping?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e_vdT5zAfwF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}