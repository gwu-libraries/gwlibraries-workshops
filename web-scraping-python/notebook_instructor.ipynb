{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook-instructor.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuIowtekqdKO"
      },
      "source": [
        "Remember to periodically save!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3y8TlR2lFM-"
      },
      "source": [
        "# Web Scraping with Requests-HTML\n",
        "\n",
        "Documentation: https://requests.readthedocs.io/projects/requests-html/en/latest/\n",
        "\n",
        "If you've used Python for getting web pages, connecting to an API, or getting files off the web, odds are you've encountered the requests library. The library we're using today, Requests-HTML (note the different name) was developed by the author of the requests, Kenneth Reitz.  It's intended to be an easy-to-learn web scraping library, it's basically a layer on top of requests and BeautifulSoup. BeautifulSoup is another good basic Python library for parsing HTML. Lots of tutorials and documentation on it. \n",
        "\n",
        "\n",
        "## Part 1: Scraping headlines from the GW Hatchet\n",
        "\n",
        "Before we start scraping, let's look at the robots.txt file for the site. This is a list of instructions to crawlers--usually search engines--about how to crawl the site and which parts to stay away from. \n",
        "\n",
        "https://www.gwhatchet.com/robots.txt\n",
        "\n",
        "Note that this site is very open about crawling. That doesn't mean there aren't still legal and ethical considerations, but we're respecting their instructions about the site. \n",
        "\n",
        "\n",
        "**Getting started**\n",
        "\n",
        "First we need to install the `requests_html` Python library using the pip command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqrCkPF_h6Lq"
      },
      "source": [
        "!pip install requests_html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ULpyL6Gh7Jb"
      },
      "source": [
        "from requests_html import HTMLSession"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwCI4gvvoq5C"
      },
      "source": [
        "We  need to make an HTTP GET request to get the website HTML. To do this we create an HTMLSession. That session object has methods to make GET requests. \n",
        "\n",
        "The get() method takes the URL as an argument. Sends an HTTP GET request. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi2LtNMAiInM"
      },
      "source": [
        "session = HTMLSession()\n",
        "\n",
        "r = session.get('https://www.gwhatchet.com/section/news/')"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACd8zgLk5kru",
        "outputId": "3f277aea-4bfb-451a-ecb4-561c81da32a9"
      },
      "source": [
        "r.html"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<HTML url='https://www.gwhatchet.com/section/news/'>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FjCtI0k3BI"
      },
      "source": [
        "`r` is a Response object and it has an html attribute which contains all of the HTML from the page in Unicode. We can look at the web page's text using the text attribute.  This is like looking at the text content of all of the HTML tags. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI7rZ_7ciNVo"
      },
      "source": [
        "r.html.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6RLw4p6kyOi"
      },
      "source": [
        "This html attribute also has many other attributes which are shortcuts to parts of the content. There is a links attribute which extracts all of the links in the page. Note that these are returned in alphabetical order, not the order on the page. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll7C6qZoiYsU"
      },
      "source": [
        "r.html.links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KVBoUxmOftp7",
        "outputId": "6593370a-dfc0-44a4-fd7d-267fd0824d2b"
      },
      "source": [
        "r.html.find('footer')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Element 'footer' >]"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdNug2oCwCVP"
      },
      "source": [
        "Now let's work at extracting certain parts of the page. Suppose we want all of the headlines. First we need to examine the page and determine if there is any markup that helps us identify headlines.  \n",
        "\n",
        "*Bring up the HTML of the page.*\n",
        "\n",
        "We use CSS selectors to specify tags, classes, ids, or other attributes. \n",
        "https://www.w3schools.com/cssref/css_selectors.asp\n",
        "\n",
        "We can see that all of the headlines are in h2 tags with a class of post-title. If there is a class that identifies the elements we want, that's ideal. \n",
        "\n",
        "In this case, we can use the h2 tag OR we could also use the class. *Show both parameters.*\n",
        "\n",
        "We can now use the html object's find() method to provide a CSS selector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2N1H7ouwNOI"
      },
      "source": [
        "titles = r.html.find('h2')\n",
        "#titles = r.html.find('post-title')"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TSbc5hC6xjv2",
        "outputId": "b4c779f8-e829-4528-eb0f-de450cb64e37"
      },
      "source": [
        "type(titles)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "list"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RCGecahk1TG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d17d0db-ff9c-4dcb-b1f2-8cb4e930b163"
      },
      "source": [
        "titles"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('post-title',)>,\n",
              " <Element 'h2' class=('widgettitle',)>,\n",
              " <Element 'h2' class=('widgettitle',)>,\n",
              " <Element 'h2' class=('widgettitle',)>,\n",
              " <Element 'h2' class=('widgettitle',)>]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRWbUOVYx_ik"
      },
      "source": [
        "`titles` is a list of Elements (capital E). We can access just the text content of those Elements using the `full_text` attribute. Let's look at just the first one to start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bs3H_K3jyfZP",
        "outputId": "24d89469-751c-4999-d3f8-c36f6d3b606f"
      },
      "source": [
        "titles[0].text"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Alumnus sues University over motorized tricycle crash'"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlBMFvwxy4Q3"
      },
      "source": [
        "Now let's grab the text of all of the headlines. We'll iterate through the Elements in the titles list and add to our headlines list the text attribute from each one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G-8jwZjWx9zg",
        "outputId": "1c68d321-3ac7-45a6-a2d1-1f00bc094fd2"
      },
      "source": [
        "headlines = [t.text for t in titles]\n",
        "print(headlines)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Alumnus sues University over motorized tricycle crash', 'Officials respond to alleged mishandling of Title IX cases', 'SBA Senate funds thousands for International Law Society', 'Officials to reclassify postdoc researchers as trainees', 'GW rises three spots on U.S. News and World Report rankings', 'Milken professors sign Supreme Court brief on abortion rights', 'Officials watching potential shutdown ahead of Commencement', 'Black female officers sue MPD for sexual harassment', 'Officials search for new sustainability director', 'Panelists talk teaching racial justice in public administration', 'Sections', 'Reader Services', 'Learn More', 'Get in Touch']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3S0uHnSzu75"
      },
      "source": [
        "That's fine for one page, but if we want to get a lot more headlines, we'll need to check multiple pages. Let's look at the Hatchet website and see which pages have new articles. \n",
        "\n",
        "*Return to Hatchet website and page through news section.*\n",
        "\n",
        "Note: there are only 10 headlines per page, and then we need to click Next to go to another page. Clicking on that, we can see that there is number in the URL. So seeing that, we can use Python to get the HTML of each page and then get the headlines from each one. \n",
        "\n",
        "Before we jump in and do that, we're at the point where we should consider what is the most polite way to get this content from the Hatchet website. When you're web scraping, you don't want to interfere with a website's regular traffic by requesting lots of content all at one. We've got a lot of us online in this workshop, so let's give our requests a little space. We'll pause a second between requesting each page. \n",
        "\n",
        "To do that pausing, we need to import another library in the Python standard library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGK-CL3H44jj"
      },
      "source": [
        "from time import sleep"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FZeKIPvkyqi7",
        "outputId": "04dc5650-b27b-4707-b95d-e984357d8023"
      },
      "source": [
        "news = []\n",
        "\n",
        "# get the first 4 pages\n",
        "for i in range(1,5):\n",
        "    resp = session.get('https://www.gwhatchet.com/section/news/page/{}'.format(i))\n",
        "    #titles = resp.html.find('h2')\n",
        "    titles = resp.html.find('.post-title')\n",
        "    headlines = [t.text for t in titles]\n",
        "    news.extend(headlines)\n",
        "    sleep(1)\n",
        "\n",
        "print(news)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Alumnus sues University over motorized tricycle crash', 'Officials respond to alleged mishandling of Title IX cases', 'SBA Senate funds thousands for International Law Society', 'Officials to reclassify postdoc researchers as trainees', 'GW rises three spots on U.S. News and World Report rankings', 'Milken professors sign Supreme Court brief on abortion rights', 'Officials watching potential shutdown ahead of Commencement', 'Black female officers sue MPD for sexual harassment', 'Officials search for new sustainability director', 'Panelists talk teaching racial justice in public administration', 'Student Court sets hearing for first-year senator case', 'Crime log: Spouse of staff member carjacked by male suspect', 'GW Law launches strategic planning process with Deloitte', 'SA textbook exchange program to alleviate costs: students', 'Officials to hold internal diversity review instead of external audit', 'Communication needed to convey FAFSA changes: experts', 'Absence of Vern COVID-19 testing site stirs health concerns', 'Pilot program to shut down E Street homeless encampment', \"GW Hospital records hundreds of flights since helipad's opening\", 'Officials expect thousands at bicentennial celebration this weekend', 'Community coordinators enter first year in residence halls', 'Puerto Rican governor recaps political career highlights', 'Organizers plan in-person events to promote monthlong Latinx celebration', 'GWorld vendors unaware of single-use plastics policy', 'Townhouse Row residents set to return next week', 'Officials expanding COVID testing availability after student concerns', 'SA president vetoes bill amending senate elections', 'Male driver carjacked near Shenkman Hall', 'Community members, experts say CCST contact tracing notifications lack details', 'Police arrest Sunrise GW members at Capitol protest', 'Student Court halts fall referendum on first-year senate seats', 'Elliott, CCAS launch joint group to analyze religion, public policy', 'Student Court justice resigns ahead of potential hearings', 'GWSB launches three cross-school engineering certificates', 'Officials expand SafeRide program after student advocacy', 'SA Senate calls on GW to offer hybrid learning option', 'Crime log: Amazon packages stolen from West Hall loading dock', 'Limited COVID-19 testing appointments led to delays: students', 'Lerner service expands with upswing in daily visitors', 'Professor receives grant to develop breast cancer therapy']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0jJgzTK5sz4",
        "outputId": "3d73202c-c1b8-4097-8d77-8e4c8ffb8e45"
      },
      "source": [
        "len(news)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcVHkZvRi6a7"
      },
      "source": [
        "Now that you have this data, you could go on to do other analysis of it. Maybe you want to look for mentions of certain entitites or do sentiment analysis or topic modelling of the headlines. There's a workshop on Natural Language Processing, or NLP tomorrow at 1PM if you want to learn more about working with text data: https://library.gwu.edu/news-events/events/python-natural-language-processing-0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IrUJRuT7lny"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "Collect the bylines from the news-articles on the page. Start with one page of news articles. \n",
        "\n",
        "Hints:\n",
        "1. determine what tag or class identifies the text you want. \n",
        "2. use the find() method to identify the element and collect them into a list. Assign that to a variable called bylines. \n",
        "3. Extract the text from each element and assign the results to a list called authors. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x1R-7v5W8j23"
      },
      "source": [
        "# Note that if you use just byline-author as below you will also pick up the text \"By\". \n",
        "# bylines = r.html.find('.byline-author')\n",
        "# bylines[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_m9UPlx7qaF"
      },
      "source": [
        "bylines = r.html.find('.byline-author a')"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "D9LE4BqG76if",
        "outputId": "1dccb4ca-ab7e-4ba9-8376-077f36de22e4"
      },
      "source": [
        "bylines[0].text"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Zachary Blackburn'"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6KnrU_2p77O1",
        "outputId": "71981762-3acf-4c57-f811-244ae310ffea"
      },
      "source": [
        "authors = [b.text for b in bylines]\n",
        "authors"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Zachary Blackburn',\n",
              " 'Abby Kennedy',\n",
              " 'Alexis Doe',\n",
              " 'Lauren Sforza',\n",
              " 'Katelyn Aluise',\n",
              " 'Cleo Hudson',\n",
              " 'Alexandra Kicior',\n",
              " 'Isha Trivedi',\n",
              " 'Zachary Blackburn',\n",
              " 'Michelle Vassilev',\n",
              " 'Baxter Wareham']"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue35f0Ho-Vld"
      },
      "source": [
        "If you want to use this data to get a count of how many times each author has been published, you could count them. There is a count() method in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uksVpVLX8mLx",
        "outputId": "2e93dd4e-bf98-4ffe-9199-a0b8a683c6c1"
      },
      "source": [
        "authors.count(\"Zachary Blackburn\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QobayzUUb8DP"
      },
      "source": [
        "The .count() method is a bit inefficient for our purposes because you have to use the count method with each possible author's name. Better to use Python's Counter from the collections module. We can give Counter a list and it will return a count of each value that appears. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zwRBSDBn8GXt",
        "outputId": "ae8a4305-be64-46b2-bd3a-dfcd50f2e82f"
      },
      "source": [
        "from collections import Counter\n",
        "a = Counter(authors)\n",
        "a"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Counter({'Abby Kennedy': 1,\n",
              "         'Alexandra Kicior': 1,\n",
              "         'Alexis Doe': 1,\n",
              "         'Baxter Wareham': 1,\n",
              "         'Cleo Hudson': 1,\n",
              "         'Isha Trivedi': 1,\n",
              "         'Katelyn Aluise': 1,\n",
              "         'Lauren Sforza': 1,\n",
              "         'Michelle Vassilev': 1,\n",
              "         'Zachary Blackburn': 2})"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uap3MNvhtmS",
        "outputId": "a163e900-8c85-409b-9191-efcc91d004f9"
      },
      "source": [
        "b = Counter(authors)\n",
        "print(b.most_common(3))\n",
        "print(b[\"Clara Duhon\"])\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Zachary Blackburn', 2), ('Abby Kennedy', 1), ('Alexis Doe', 1)]\n",
            "0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjXqHE6kzIkO"
      },
      "source": [
        "## Part 2: More scraping! Scraping course listings from the Schedule of Classes\n",
        "\n",
        "**Have you saved lately? Be sure to save your notebook file!**\n",
        "\n",
        "*Bring up the GW Schedule of Classes* \n",
        "https://my.gwu.edu/mod/pws/subjects.cfm?campId=1&termId=202101\n",
        "\n",
        "Recommend starting with one page, and then building up as you determine how to access the data you need. We'll start with just the American Studies courses for Spring. *Drill down to the Main Campus Spring semester and then select American Studies*.\n",
        "\n",
        "Let's suppose we want to have a spreadsheet listing all of the courses in our department for our major. \n",
        "\n",
        "We will need to parse the HTML to get the information in each of these rows. (We'll ignore the comments and Find Books links.)\n",
        "\n",
        "###robots.txt\n",
        "Before doing anything, let's look at the robots.txt\n",
        "\n",
        "https://my.gwu.edu/robots.txt\n",
        "Our page is in /mod/pws and there are no prohibitions about crawling that content. \n",
        "\n",
        "**Now we can move on and look at the HTML for the page we want:**\n",
        "*View Page Source. Discuss HTML tags used for tables: table, tr, td, etc.*\n",
        "\n",
        "Relevant tags and courses include the `tr` tag in the tables. It looks like we have a class on each row which we could use, `.crseRow1`. We could look for all of the `td`s within `.crseRow1`, but then we would lose the contxt of which row they were in. \n",
        "\n",
        "Let's see what we get with the class that's on each row in each table. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGGI44Op95Ru"
      },
      "source": [
        "session_new = HTMLSession()\n",
        "\n",
        "amst_url = 'https://my.gwu.edu/mod/pws/courses.cfm?campId=1&termId=202103&subjId=AMST'\n",
        "response = session_new.get(amst_url)"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "C-OjmlkW4-cL",
        "outputId": "ebb27085-0832-4f19-8840-a2838286c45f"
      },
      "source": [
        "# Might be thinking we could look for all of the td's within .crseRow1\n",
        "rows = response.html.find('.crseRow1 td')\n",
        "rows[0].text"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'WAITLIST'"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYGV6yQNSQ9m",
        "outputId": "97380e2d-31af-452a-967d-06d5b23b146a"
      },
      "source": [
        "for row in rows[:10]:\n",
        "  print(row.text)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WAITLIST\n",
            "68081\n",
            "AMST 1050\n",
            "11\n",
            "What is Democracy?\n",
            "3.00\n",
            "Anker, E\n",
            "MON B32\n",
            "T\n",
            "12:45PM - 03:15PM\n",
            "08/30/21 - 12/11/21\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SJGRCuWz5Kkb",
        "outputId": "a7b1c9aa-a7a7-431c-dd75-094cc1ea0283"
      },
      "source": [
        "len(rows)\n",
        "#We have a long list of cells, and we would lose the contxt of which row they were in. "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "638"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEZkavki5ZTj"
      },
      "source": [
        "Instead, let's try to work with each row, so we have a list of rows, and drill down to extract the td content. We'll start a level above. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "NaEGkVj0zWDY",
        "outputId": "d2a81b21-9df4-49d3-8805-181ecc6b7f12"
      },
      "source": [
        "trs = response.html.find('.crseRow1')\n",
        "trs[0].text"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'WAITLIST\\n68081\\nAMST 1050\\n11\\nWhat is Democracy?\\n3.00\\nAnker, E\\nMON B32\\nT\\n12:45PM - 03:15PM\\n08/30/21 - 12/11/21'"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXmzzQLm4tUV"
      },
      "source": [
        "We get back a string, with each cell's text concatenated. It looks like the text for each cell is separated by a `\\n` newline character. We still have access to the elements within the table rows object. For example, we can look at the cells (`td` tags) in our first row as follows: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ9RJRQX4brK"
      },
      "source": [
        "cells = trs[0].find('td')"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "5FNGRMPB7wtD",
        "outputId": "0dd862a2-40ca-430f-d050-0045bd040906"
      },
      "source": [
        "cells[0].text"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'WAITLIST'"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y2NVTVw8aqJ"
      },
      "source": [
        "It appeared that there is a newline between each cell, which could be useful as a delimiter, to separate the td's in each row. However, looking at the site, we can see that the time for the class is on a different line from the day of the week. Let's take a look at each cell: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVHzV5q-8SKg",
        "outputId": "1e0ff091-2eeb-4321-d567-f3c41167154d"
      },
      "source": [
        "for c in cells:\n",
        "    print(\"td: \", c.text)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "td:  WAITLIST\n",
            "td:  68081\n",
            "td:  AMST 1050\n",
            "td:  11\n",
            "td:  What is Democracy?\n",
            "td:  3.00\n",
            "td:  Anker, E\n",
            "td:  MON B32\n",
            "td:  T\n",
            "12:45PM - 03:15PM\n",
            "td:  08/30/21 - 12/11/21\n",
            "td:  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPAxaznY_I5T"
      },
      "source": [
        "Now that we know how to access the text of each `td` and that there are some extra newlines in there, let's create a function that will strip out newlines in the text of the td tags. Our function will return a list that has the text of all of the cells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXTOGm368WDt"
      },
      "source": [
        "def get_tds(tr):\n",
        "    row = [td.text.replace('\\n', ' ') for td in tr.find('td')]\n",
        "    return row"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPcM1Kf6_xk_"
      },
      "source": [
        "Now we can use this to get all the relevant text in all of the course tables on the page, and collect them into a courses list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx86mOG-_d6f"
      },
      "source": [
        "courses = []\n",
        "for tr in trs:\n",
        "    courses.append(get_tds(tr))"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EmNPksDl_wOu"
      },
      "source": [
        "# Look at courses. Note that there are some here that don't appear on the web page! \n",
        "# That's because there is display:none as a style on some of the tables. \n",
        "\n",
        "courses"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzDaqrR3Al6F"
      },
      "source": [
        "Now that we've extracted this data, we want to hold onto it. We'll create a CSV file to hold this data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVWHER9zABQL"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('amst.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    for course in courses:\n",
        "        writer.writerow(course)"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhGRgC96AHSb",
        "outputId": "ee081093-6ee0-4c93-d69d-ad31c3cf465e"
      },
      "source": [
        "!head amst.csv"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WAITLIST,68081,AMST 1050,11,What is Democracy?,3.00,\"Anker, E\",MON B32,T 12:45PM - 03:15PM,08/30/21 - 12/11/21,\r\n",
            "OPEN,62945,AMST 1100,10,Politics and Film,3.00,\"Anker, E\",FNGR 108 AND FNGR 108,M 12:45PM - 02:00PM AND M 07:10PM - 09:40PM,08/30/21 - 12/11/21,Linked\r\n",
            "CLOSED,62983,AMST 1100,30,Discussion,0.00,\"Anker, E\",GELM B04,W 09:35AM - 10:25AM,08/30/21 - 12/11/21,Find Books\r\n",
            "CLOSED,62984,AMST 1100,31,Discussion,0.00,\"Anker, E\",1957 E B16,W 11:10AM - 12:00PM,08/30/21 - 12/11/21,Find Books\r\n",
            "OPEN,62985,AMST 1100,32,Discussion,0.00,\"Anker, E\",1957 E B17,W 09:35AM - 10:25AM,08/30/21 - 12/11/21,Find Books\r\n",
            "CLOSED,62986,AMST 1100,33,Discussion,0.00,\"Anker, E\",FNGR 108,W 12:45PM - 01:35PM,08/30/21 - 12/11/21,Find Books\r\n",
            "CLOSED,64060,AMST 1100,34,Discussion,0.00,\"Anker, E\",1957 E B17,W 12:45PM - 01:35PM,08/30/21 - 12/11/21,Find Books\r\n",
            "CLOSED,64061,AMST 1100,35,Discussion,0.00,\"Anker, E\",1957 E 214,W 02:20PM - 03:10PM,08/30/21 - 12/11/21,Find Books\r\n",
            "WAITLIST,68545,AMST 1100,36,Discussion,0.00,\"Anker, E\",MON 351,W 11:10AM - 12:00PM,08/30/21 - 12/11/21,Find Books\r\n",
            "OPEN,68546,AMST 1100,37,Discussion,0.00,\"Anker, E\",MON 251,W 02:20PM - 03:10PM,08/30/21 - 12/11/21,Find Books\r\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdkgWVTYAvC7"
      },
      "source": [
        "We could also load this CSV into pandas, if we wanted to use some of its filtering and analysis methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emcYgBGCAImk"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpNe9d1kATRU"
      },
      "source": [
        "df = pd.DataFrame(courses, columns = ['STATUS', 'CRN', 'SUBJECT', 'SECT', 'COURSE', 'CREDIT', 'INSTR.', 'BLDG/RM', 'DAY/TIME', 'FROM / TO', 'Books'])"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "retCJ-C_ATl-"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKu0c8nRAUfE"
      },
      "source": [
        "df[df['STATUS'] == \"OPEN\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSpY4DhhTk_Y"
      },
      "source": [
        "Instructions for mounting your Google Drive: \n",
        "https://www.marktechpost.com/2019/06/07/how-to-connect-google-colab-with-google-drive/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d8lq1eU7gZI3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea05e426-1185-4b5b-f679-8e22ebf189a2"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ng91cOZcT0iW"
      },
      "source": [
        "!cp amst.csv /content/drive/MyDrive\n"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz26ssh3A2aw"
      },
      "source": [
        "Now that we've done this on one page, what might be next steps?\n",
        "\n",
        "* Create a function to modularize the work we did. Then apply this to other pages (Note that there were 2 pages for AMST) and other departments. \n",
        "* Consider how you can be respectful of the website. Where could you insert some sleep statements? \n",
        "* Maybe there are better times of day to run this kind of scraping?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2e_vdT5zAfwF"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}