{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "notebook-public.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m3y8TlR2lFM-"
      },
      "source": [
        "# Web Scraping with Requests-HTML\n",
        "\n",
        "Documentation: https://requests.readthedocs.io/projects/requests-html/en/latest/\n",
        "\n",
        "If you've used Python for getting web pages, connecting to an API, or getting files off the web, odds are you've encountered the requests library. The library we're using today, Requests-HTML (note the different name) was developed by the author of the requests, Kenneth Reitz.  It's intended to be an easy-to-learn web scraping library and is basically a layer on top of requests and BeautifulSoup. BeautifulSoup is another good basic Python library for parsing HTML. There are plenty of tutorials and documentation on it. \n",
        "\n",
        "\n",
        "## Part 1: Scraping headlines from the GW Hatchet\n",
        "\n",
        "Before we start scraping, let's look at the robots.txt file for the site. This is a list of instructions to crawlers--usually search engines--about how to crawl the site and which parts to stay away from. \n",
        "\n",
        "https://www.gwhatchet.com/robots.txt\n",
        "\n",
        "Note that this site is very open about crawling. That doesn't mean there aren't still legal and ethical considerations, but we're respecting their instructions about the site. \n",
        "\n",
        "\n",
        "**Getting started**\n",
        "\n",
        "First we need to install the `requests_html` Python library using the pip command."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqrCkPF_h6Lq"
      },
      "source": [
        "!pip install requests_html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_ULpyL6Gh7Jb"
      },
      "source": [
        "from requests_html import HTMLSession"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pwCI4gvvoq5C"
      },
      "source": [
        "We  need to make an HTTP GET request to get the website HTML. To do this we create an HTMLSession. That session object has methods to make GET requests. \n",
        "\n",
        "The get() method takes the URL as an argument. Sends an HTTP GET request. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qi2LtNMAiInM"
      },
      "source": [
        "session = HTMLSession()\n",
        "\n",
        "r = session.get('https://www.gwhatchet.com/section/news/')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ACd8zgLk5kru"
      },
      "source": [
        "r.html"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E2FjCtI0k3BI"
      },
      "source": [
        "`r` is a Response object and it has an html attribute which contains all of the HTML from the page in Unicode. We can look at the web page's text using the text attribute.  This is like looking at the text content of all of the HTML tags. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI7rZ_7ciNVo"
      },
      "source": [
        "r.html.text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6RLw4p6kyOi"
      },
      "source": [
        "This html attribute also has many other attributes which are shortcuts to parts of the content. There is a links attribute which extracts all of the links in the page. Note that these are returned in alphabetical order, not the order on the page. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ll7C6qZoiYsU"
      },
      "source": [
        "r.html.links"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdNug2oCwCVP"
      },
      "source": [
        "Now let's work at extracting certain parts of the page. Suppose we want all of the headlines. First we need to examine the page and determine if there is any markup that helps us identify headlines.  \n",
        "\n",
        "View the page source of the website to examine the HTML. \n",
        "\n",
        "We use CSS selectors to specify tags, classes, ids, or other attributes. \n",
        "https://www.w3schools.com/cssref/css_selectors.asp\n",
        "\n",
        "We can see that all of the headlines are in h2 tags with a class of post-title. If there is a class that identifies the elements we want, that's ideal. \n",
        "\n",
        "In this case, we can use the h2 tag OR we could use the class. \n",
        "\n",
        "We can now use the html object's find() method to provide a CSS selector. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2N1H7ouwNOI"
      },
      "source": [
        "titles = r.html.find('h2')\n",
        "#titles = r.html.find('post-title')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TSbc5hC6xjv2"
      },
      "source": [
        "type(titles)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8RCGecahk1TG"
      },
      "source": [
        "titles"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRWbUOVYx_ik"
      },
      "source": [
        "`titles` is a list of Elements (capital E). We can access just the text content of those Elements using the `full_text` attribute. Let's look at just the first one to start."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bs3H_K3jyfZP"
      },
      "source": [
        "titles[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YlBMFvwxy4Q3"
      },
      "source": [
        "Now let's grab the text of all of the headlines. We'll iterate through the Elements in the titles list and add to our headlines list the text attribute from each one:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G-8jwZjWx9zg"
      },
      "source": [
        "headlines = [t.text for t in titles]\n",
        "print(headlines)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3S0uHnSzu75"
      },
      "source": [
        "That's fine for one page, but if we want to get a lot more headlines, we'll need to check multiple pages. Let's look at the Hatchet website and see which pages have new articles. \n",
        "\n",
        "*Return to Hatchet website and page through news section.*\n",
        "\n",
        "Note: there are only 10 headlines per page, and then we need to click Next to go to another page. Clicking on that, we can see that there is number in the URL. So seeing that, we can use Python to get the HTML of each page and then get the headlines from each one. \n",
        "\n",
        "Before we jump in and do that, we're at the point where we should consider what is the most polite way to get this content from the Hatchet website. When you're web scraping, you don't want to interfere with a website's regular traffic by requesting lots of content all at one. We've got a lot of us online in this workshop, so let's give our requests a little space. We'll pause a second between requesting each page. \n",
        "\n",
        "To do that pausing, we need to import another library in the Python standard library. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aGK-CL3H44jj"
      },
      "source": [
        "from time import sleep"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZeKIPvkyqi7"
      },
      "source": [
        "news = []\n",
        "\n",
        "# get the first 4 pages\n",
        "for i in range(1,5):\n",
        "    resp = session.get('https://www.gwhatchet.com/section/news/page/{}'.format(i))\n",
        "    #titles = resp.html.find('h2')\n",
        "    titles = resp.html.find('.post-title')\n",
        "    headlines = [t.text for t in titles]\n",
        "    news.extend(headlines)\n",
        "    sleep(1)\n",
        "\n",
        "print(news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0jJgzTK5sz4"
      },
      "source": [
        "len(news)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LcVHkZvRi6a7"
      },
      "source": [
        "Now that you have this data, you could go on to do other analysis of it. Maybe you want to look for mentions of certain entitites or do sentiment analysis or topic modelling of the headlines. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0IrUJRuT7lny"
      },
      "source": [
        "**Exercise:**\n",
        "\n",
        "Collect the bylines from the news-articles on the page. Start with one page of news articles. \n",
        "\n",
        "Hints:\n",
        "1. determine what tag or class identifies the text you want. \n",
        "2. use the find() method to identify the element and collect them into a list. Assign that to a variable called bylines. \n",
        "3. Extract the text from each element and assign the results to a list called authors. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l_m9UPlx7qaF"
      },
      "source": [
        "bylines = r.html.find('.byline-author > a')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9LE4BqG76if"
      },
      "source": [
        "bylines[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6KnrU_2p77O1"
      },
      "source": [
        "authors = [b.text for b in bylines]\n",
        "authors"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ue35f0Ho-Vld"
      },
      "source": [
        "If you want to use this data to get a count of how many times each author has been published, you could count them. There is a count() method in Python."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uksVpVLX8mLx"
      },
      "source": [
        "authors.count(\"Jarrod Wardwell\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QobayzUUb8DP"
      },
      "source": [
        "The .count() method is a bit inefficient for our purposes because you have to use the count method with each possible author's name. Better to use Python's Counter from the collections module. We can give Counter a list and it will return a count of each value that appears. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zwRBSDBn8GXt"
      },
      "source": [
        "from collections import Counter\n",
        "a = Counter(authors)\n",
        "a"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjXqHE6kzIkO"
      },
      "source": [
        "## Part 2: More scraping! Scraping course listings from the Schedule of Classes\n",
        "\n",
        "**Have you saved lately? Be sure to save your notebook file!**\n",
        "\n",
        "*Bring up the GW Schedule of Classes* \n",
        "https://my.gwu.edu/mod/pws/subjects.cfm?campId=1&termId=202101\n",
        "\n",
        "One approach to take is to examine and scrape from one page, and then build up to collect more pages and sections as you determine how to access the data you need. We'll start with just the American Studies courses for Spring. \n",
        "\n",
        "Let's suppose we want to have a spreadsheet listing all of the courses in our department for our major. We will need to parse the HTML to get the information in each of these rows. (We'll ignore the comments and Find Books links.)\n",
        "\n",
        "###robots.txt\n",
        "Before doing anything, let's look at the robots.txt\n",
        "\n",
        "https://my.gwu.edu/robots.txt\n",
        "Our page is in /mod/pws and there are no prohibitions about crawling that content. \n",
        "\n",
        "**Now we can move on and look at the HTML for the page we want:**\n",
        "\n",
        "Relevant tags and courses include the `tr` tag in the tables. It looks like we have a class on each row which we could use, `.crseRow1`. We could look for all of the `td`s within `.crseRow1`, but then we would lose the contxt of which row they were in. \n",
        "\n",
        "Let's see what we get with the class that's on each row in each table. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pGGI44Op95Ru"
      },
      "source": [
        "session_new = HTMLSession()\n",
        "\n",
        "amst_url = 'https://my.gwu.edu/mod/pws/courses.cfm?campId=1&termId=202101&subjId=AMST'\n",
        "response = session_new.get(amst_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C-OjmlkW4-cL"
      },
      "source": [
        "# You might be thinking we could look for all of the td's within .crseRow1, however this may not help us. \n",
        "rows = response.html.find('.crseRow1 > td')\n",
        "rows[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SJGRCuWz5Kkb"
      },
      "source": [
        "len(rows)\n",
        "#We have a long list of cells, and we would lose the contxt of which row they were in. "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEZkavki5ZTj"
      },
      "source": [
        "Instead, let's try to work with each row, so we have a list of rows, and drill down to extract the td content."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NaEGkVj0zWDY"
      },
      "source": [
        "trs = response.html.find('.crseRow1')\n",
        "trs[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXmzzQLm4tUV"
      },
      "source": [
        "We get back a string, with each cell's text concatenated. It looks like the text for each cell is separated by a `\\n` newline character. We still have access to the elements within the table rows object. For example, we can look at the cells (`td` tags) in our first row as follows: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QJ9RJRQX4brK"
      },
      "source": [
        "cells = trs[0].find('td')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5FNGRMPB7wtD"
      },
      "source": [
        "cells[0].text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0y2NVTVw8aqJ"
      },
      "source": [
        "It appeared that there is a newline between each cell, which could be useful as a delimiter, to separate the td's in each row. However, looking at the site, we can see that the time for the class is on a different line from the day of the week. Let's take a look at each cell: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVHzV5q-8SKg"
      },
      "source": [
        "for c in cells:\n",
        "    print(\"td: \",c.text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PPAxaznY_I5T"
      },
      "source": [
        "Now that we know how to access the text of each `td` and that there are some extra newlines in there, let's create a function that will strip out newlines in the text of the td tags. Our function will return a list that has the cleaned text of all of the cells. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SXTOGm368WDt"
      },
      "source": [
        "def get_tds(tr):\n",
        "    row = [td.text.replace('\\n', ' ') for td in tr.find('td')]\n",
        "    return row"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BPcM1Kf6_xk_"
      },
      "source": [
        "Now we can use this to get all the relevant text in all of the course tables on the page, and collect them into a courses list."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zx86mOG-_d6f"
      },
      "source": [
        "courses = []\n",
        "for tr in trs:\n",
        "    courses.append(get_tds(tr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PzDaqrR3Al6F"
      },
      "source": [
        "Now that we've extracted this data, we want to hold onto it. We'll create a CSV file to hold this data. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nVWHER9zABQL"
      },
      "source": [
        "import csv\n",
        "\n",
        "with open('amst.csv', 'w', newline='') as csvfile:\n",
        "    writer = csv.writer(csvfile, delimiter=',')\n",
        "    for course in courses:\n",
        "        writer.writerow(course)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhGRgC96AHSb"
      },
      "source": [
        "!head amst.csv"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qdkgWVTYAvC7"
      },
      "source": [
        "We could also load this CSV into pandas, if we wanted to use some of its filtering and analysis methods. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "emcYgBGCAImk"
      },
      "source": [
        "import pandas as pd\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EpNe9d1kATRU"
      },
      "source": [
        "df = pd.DataFrame(courses, columns = ['STATUS', 'CRN', 'SUBJECT', 'SECT', 'COURSE', 'CREDIT', 'INSTR.', 'BLDG/RM', 'DAY/TIME', 'FROM / TO', 'Notes'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "retCJ-C_ATl-"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uKu0c8nRAUfE"
      },
      "source": [
        "df[df['STATUS'] == \"OPEN\"]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz26ssh3A2aw"
      },
      "source": [
        "Now that we've done this on one page, what might be next steps? How can you be respectful of the site when scraping?"
      ]
    }
  ]
}