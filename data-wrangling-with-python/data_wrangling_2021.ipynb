{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_wrangling_2021.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8krBWEWkoxvc"
      },
      "source": [
        "# Data Wrangling with Python\n",
        "\n",
        "In this workshop, we'll dive deep into some techniques for cleaning and re-shaping data in Python using the [pandas](https://pandas.pydata.org/docs/) library. \n",
        "\n",
        "Here's what you can expect to practice:\n",
        " - working with CSV data from various sources\n",
        " - joining datasets on common elements\n",
        " - handling text and time series data\n",
        " - grouping and reshaping datasets to create plots\n",
        " - working with numeric data at different magnitudes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKgf3CWX0CES"
      },
      "source": [
        "## Research question\n",
        "\n",
        "**Do tweets by United States Senators that reference COVID-19 correlate with the incidence of COVID-19 cases in their states?**\n",
        "\n",
        "## Data sources\n",
        "\n",
        " - tweets by Senators in the 116th Congress, collected by GW's [SFM project](https://library.gwu.edu/scholarly-technology-group/social-feed-manager)\n",
        " - names, states, and social media handles of current and historical US legislators, compiled by [@unitedstates](https://theunitedstates.io/)\n",
        " - daily COVID-19 incidence by U.S. state and territory, compiled by _The New York Times_\n",
        " - Federal Information Processing Standards (FIPS) codes for the United States"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOEyziFbBQY7"
      },
      "source": [
        "## Setting up\n",
        "\n",
        "Let's import any libraries we'll need. \n",
        "\n",
        "We'll do most of our work in `pandas`, which should be available automatically in a Google Colab environment. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mjj0DUvKBzYi"
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DXUJ6fmiB3hG"
      },
      "source": [
        "## Loading our main dataset\n",
        "\n",
        "Our dataset of tweets by US Senators in the 116th Congress was collected from the public Twitter API, but Twitter's use agreement prohibits sharing the data publicly. So you'll need to load the file that I shared with you from your Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arRhl72rlwE-"
      },
      "source": [
        "# This code will make your Google Drive accessible from your Colab Notebook\n",
        "# You'll need to click the link provided, copy the token, and paste it into the form.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JrJ5fYvXC2qz"
      },
      "source": [
        "### Loading a file from Google Drive\n",
        "\n",
        "1. Run the cell, follow the link, paste the token in the form provided, and press `Enter`.\n",
        "2. You should see `Mounted at /content/drive` as the output of that cell. \n",
        "3. Now click the Folder icon on the right-hand taskbar of your Colab Notebook. Your mounted Google Drive should be in the folder called `drive`. \n",
        "4. If you added the file to your Drive, you can find it under `MyDrive`, which contains all the files and folders in your root Drive folder.\n",
        "5. Find the file called `data-wrangling-workshop_twitter-data.csv`. \n",
        "5. Click to the right of the filename and select `Copy path`.\n",
        "6. Paste this into a new code cell between quotation marks (as a string) and assign it to a new variable."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6CY8gpT3mGeX"
      },
      "source": [
        "path_to_twitter_data = '/content/drive/MyDrive/workshops/Data-Wrangling-101/data-wrangling-workshop_twitter-data.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "85VsWD0SmIpw"
      },
      "source": [
        "# With the drive mounted and the path assigned to a variable, we should be able\n",
        "# to open the file with pandas \n",
        "# The read_csv method loads a CSV file into a pandas DataFrame\n",
        "tweets = pd.read_csv(path_to_twitter_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ds4Wry9WG2Gr"
      },
      "source": [
        "### Exploring & cleaning the data\n",
        "\n",
        "The Twitter API provides a rich set of metadata as well as the text of each tweet. \n",
        "\n",
        "Our `DataFrame` has some handy methods we can use to inspect our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heVfqEOEK_a5"
      },
      "source": [
        "# By default, a DataFrame displays only the first 5 and last 5 rows\n",
        "tweets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xHHrNR42LJpr"
      },
      "source": [
        "# We can get a list of columns from the .columns attribute.\n",
        "# But it's sometimes more helpful to see how many rows are null for each column.\n",
        "# DataFrame.isna() returns a DataFrame where every null value has been replaced\n",
        "# by the Boolean value True. \n",
        "# All other values have been set to False.\n",
        "# By calling the .sum() method on **that** DataFrame, we can actually get a COUNT\n",
        "# of the nulls in each column. \n",
        "# Columns with 0 have no nulls.\n",
        "tweets.isna().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZR5P5VfQMd8t"
      },
      "source": [
        "# We can also see a list of the unique values in a column with the .unique() method.\n",
        "# Taking the len() of that list gives a count of unique values.\n",
        "len(tweets.user_screen_name.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diOOcP8SMz_3"
      },
      "source": [
        "There are only 100 Senators, but more screen names occur in our dataset because many Senators use both a personal Twitter account and an official account."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w_820Ig26Hl_"
      },
      "source": [
        "#### Dropping duplicates\n",
        "\n",
        "Since we're counting tweets, we don't want any duplicates. A DataFrame has a method for checking for those, too.\n",
        "\n",
        "`duplicated()` returns `True` if an element has a duplicate anywhere in the DataFrame (or the column, if the method is called on a column), and `False` if it is unique.\n",
        "\n",
        "For Twitter data, the `id` field should be unique to each tweet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7kxql955c0C"
      },
      "source": [
        "# The keep=False argument specifies that we return all rows with duplicates.\n",
        "dupes = tweets.loc[tweets.id.duplicated(keep=False)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f0WcIDVJ51at"
      },
      "source": [
        "# The duplicates aren't necessarily contiguous, so we sort by the id column to inspect the duplicates.\n",
        "dupes.sort_values(by='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KMBKm74u6d58"
      },
      "source": [
        "# We can use drop_duplicates to get rid of these duplicates, keeping only the first occurence.\n",
        "# The subset keyword argument indicates the column in which to identify duplicate values\n",
        "tweets = tweets.drop_duplicates(subset='id')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJBM0NV4O9U9"
      },
      "source": [
        "### Working with time series \n",
        "\n",
        "Our `tweets` dataset has a few timestamp columns. The `parsed_created_at` contains a timestamp designed to be machine-readable. But `pandas` doesn't automatically convert timestamps to Python `datetime` objects, which is the type in Python designed for working with time series."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r5Hh54ZoPl7-"
      },
      "source": [
        "# The column's dtype tells us the data type.\n",
        "# The 'O' represents either a string or a column of mixed typed.\n",
        "tweets.parsed_created_at.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFVcPKsHQBA5"
      },
      "source": [
        "# We can convert these parsed strings to datetime objects easily with pandas.\n",
        "# Let's assign the converted data to a new column\n",
        "tweets['tweet_date'] = pd.to_datetime(tweets.parsed_created_at)\n",
        "tweets.tweet_date.dtype"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eenC2yAeQ6As"
      },
      "source": [
        "# Now we can see the first and last date in our dataset using the .max() and .min() functions.\n",
        "(tweets.tweet_date.min(), tweets.tweet_date.max())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bNXrLZADRWCc"
      },
      "source": [
        "## Enhancing our data\n",
        "\n",
        "Our `tweets` dataset has a lot of information about each tweet but not very much information about the authors of the tweets. We don't know which account corresponds to the Senator from a given state. We'll need this additional information in order to compare tweets with COVID incidence. \n",
        "\n",
        "### Adding US postal codes to Senators' tweets\n",
        "\n",
        "Below we'll exploit the fact that we can `merge` DataFrames on common elements. We want to add a column to our dataset indicating the Senator's home state.\n",
        "\n",
        "The @unitedstates project provides extensive metadata on current and past members of Congress in a variety of machine-readable formats. We'll use their `CSV` files for convenience with `pandas`.\n",
        "\n",
        "We need two different files:\n",
        " - The [legislators-historical](https://theunitedstates.io/congress-legislators/legislators-historical.csv) file contains information on members of the 116th Congress who are **no longer serving**.\n",
        " - The [legislators-current](https://theunitedstates.io/congress-legislators/legislators-current.csv) files contains informations on those members who are serving in the 117th Congress."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J0zJU862SJ5O"
      },
      "source": [
        "# Below are the urls for each\n",
        "leg_hist_url = 'https://theunitedstates.io/congress-legislators/legislators-historical.csv'\n",
        "leg_curr_url = 'https://theunitedstates.io/congress-legislators/legislators-current.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIaFIva6RVWl"
      },
      "source": [
        "# We load each into a new DataFrame\n",
        "# read_csv() can read data from a URL as well as a file\n",
        "leg_hist = pd.read_csv(leg_hist_url)\n",
        "leg_curr = pd.read_csv(leg_curr_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cf_JSvVhQlJG"
      },
      "source": [
        "# Since we need data from both, we can concatenate them (stack one on top of the other)\n",
        "# using pd.concat().\n",
        "# This works best when the datasets have the same columns\n",
        "# We can test for that like so, converting the list of columns to a Python set.\n",
        "set(leg_hist.columns) == set(leg_curr.columns)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCPwAwSKUMDP"
      },
      "source": [
        "# Note that pd.concat() expects its argument to be a Python list.\n",
        "leg_all = pd.concat([leg_hist, leg_curr])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhuzNnTwT_df"
      },
      "source": [
        "# We need just a few columns, so let's take a slice of our combined DataFrame.\n",
        "# The twitter column contains the legislator's Twitter handle.\n",
        "# The type column indicates whether they are a Senator or Representative.\n",
        "columns = ['full_name', 'type', 'state', 'twitter']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CbfMFGyLU7Lw"
      },
      "source": [
        "# We use the .copy() method to avoid warnings from pandas.\n",
        "# Without .copy(), the slice returns a reference to the original DataFrame.\n",
        "# Appending .copy() makes a duplicate (a copy in a new location in memory.)\n",
        "leg_all = leg_all[columns].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuwZN0TAU9zP"
      },
      "source": [
        "# Now let's filter our dataset to keep only Senators\n",
        "sen_all = leg_all.loc[leg_all.type == 'sen'].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpcfycNdVnqr"
      },
      "source": [
        " - We need to enhance the `tweets` dataset with the information from the `state` column of `sen_all`, so that every tweet will be associated with its author's state. \n",
        " - In order to do this computationally, we need a common data element.\n",
        " - The best candidate is the Twitter handle or screen name, since it's included in both datasets. Twitter screen names, moreover, are by definition uniquely identifying, fixed strings, like email addresses. \n",
        " - Matching on proper names is much more difficult, since a great deal of variation can occur among representations of a person's name.\n",
        "\n",
        "We can use pandas indexing, along with the `.isin()` method, to check the Twitter handles in our `tweets` dataset against the @unitedstates dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBQ0d27BVKj6"
      },
      "source": [
        "# Find rows (tweets) where the Twitter screen name does not appear in sen_all's list\n",
        "filtered = tweets.loc[~tweets.user_screen_name.isin(sen_all.twitter)]\n",
        "# Count the number of unique screen names in that group\n",
        "len(filtered.user_screen_name.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtkWxwVitqKu"
      },
      "source": [
        "We're missing the personal accounts of the Senators but capturing, for the most part, their official Senate accounts. In a \"real\" research project, we'd want either to have a good methodological justification for excluding the personal accounts, or to add the missing data from other sources.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJ6Khqr-t4-D"
      },
      "source": [
        "#### Data joins \n",
        "\n",
        "We have two datasets:\n",
        " - Tweets by Senators in the 116th Congress. \n",
        " - Metadata on each Senator for all Congresses (we are interested in the **states** they represent).\n",
        "\n",
        "We want to combine these datasets using the Twitter handle/screen name as the common element. The resulting dataset should include:\n",
        " - The data from the `tweets` dataset **for every Senator with a matching screen name** in the @unitedstates dataset.\n",
        " - The states for those Senators (from the @unitedstates dataset).\n",
        "\n",
        "It should exclude:\n",
        " - Tweets from accounts not listed in the @unitedstates dataset.\n",
        " - States for Senators not in the `tweets` dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Famo1rf-yJCt"
      },
      "source": [
        "To create the new dataset, we will use a DataFrame's `merge()` method. The type of merge we need in this case is called `inner`, which is the default.\n",
        "\n",
        "Above we used `pd.concat()` to combine two datasets by gluing one to the bottom of the other.\n",
        "\n",
        "`DataFrame.merge()` does something different. It produces a new dataset by splicing together rows from each dataset where the rows share a column element. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnWSYWSutzFd"
      },
      "source": [
        "# Here's a small example to help reason about how merge works.\n",
        "df1 = pd.DataFrame({'keys': ['Raoul', 'Abdul', 'Emily', 'Brett'],\n",
        "                    'values_from_1': ['Python', 'Python', 'R', 'Java']})\n",
        "df2 = pd.DataFrame({'keys': ['Raoul', 'Raoul', 'Emily', 'Emily', 'Emily', 'Abdul'],\n",
        "                    'values_from_2': [.95, .92, .99, .98, 1, .95]})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g-_Z25rs03AP"
      },
      "source": [
        "df1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YKgwU0qa038Z"
      },
      "source": [
        "df2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u_1lK61M1Dw-"
      },
      "source": [
        "Let's say `df1` contains some data about students: their names and their programming languages of choice. \n",
        "\n",
        "`df2` contains more data about some of those students: their most recent test scores.\n",
        "\n",
        " - Our shared elements reside in the `keys` column, since (some) of the same names appear in both datasets. (We assume that each name refers to the same student in both sets, and that each name is unique. Otherwise, our merge will create ambiguity.) The `on` argument to `merge` indicates the name of the column that contains the shared elements.\n",
        " - The `values` columns contain the data we want to merge (to associate via the shared elements in `keys`).\n",
        " - By performing an `inner` merge, we retain only those rows where there is a match in both sets. As a result, `Brett` drops out of our merged dataset, since no scores are recorded for that name.\n",
        " - Where common elements repeat in one of the datasets (as some of the names do in `keys`), the merged elements from the other dataset will repeat, too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7hQBUv8n05R7"
      },
      "source": [
        "df1.merge(df2, on='keys')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzt_d7gh3aXM"
      },
      "source": [
        "# We'll use the same approach to merge our tweets dataset to our sen_all dataset.\n",
        "# Merges can be (computationally) expensive, so it will be faster to remove\n",
        "# rows and columns we don't need. \n",
        "sen_states = sen_all[['twitter', 'state']].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XC4BA3t63v9h"
      },
      "source": [
        "# We'll drop rows for Senators without Twitter accounts (that's most of them, since this is a historical dataset).\n",
        "sen_states = sen_states.loc[~sen_states.twitter.isnull()].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2JCOyt0C39Pd"
      },
      "source": [
        "# Because our shared element -- the Twitter handle -- belongs to two columns with different names,\n",
        "# we specify each as a parameter to the merge() method.\n",
        "# left_on refers to the DataFrame whose method we're calling.\n",
        "# right_on refers to the DataFrame we're passing as an argument to the former.\n",
        "tweets_states = tweets.merge(sen_states, \n",
        "                             left_on='user_screen_name', \n",
        "                             right_on='twitter')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zLpS9Eeh4Zol"
      },
      "source": [
        "# Let's see what percentage of our original data we were able to keep\n",
        "len(tweets_states) / len(tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2MuwP-E4iXq"
      },
      "source": [
        "# It's crucial that we don't end up with any duplicate rows, since that could throw off\n",
        "# our counts.\n",
        "# The elements in the id column (from the Twitter dataset) should be unique. \n",
        "# We can compare these to the length of our new dataset as a whole.\n",
        "len(tweets_states) == len(tweets_states.id.unique())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Em6u2aB64Zm"
      },
      "source": [
        "#### Applying a test\n",
        "\n",
        "A `DataFrame` comes with many built-in methods that produce `True/False` values from conditions applied to the elements of a dataset. We've used `isna` and `duplicated` thus far. \n",
        "\n",
        "We can also define custom functions. Let's write one to test whether a given tweet mentions the pandemic and then apply it to the text of our tweets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWV6gfzd63-2"
      },
      "source": [
        "def is_pandemic(tweet):\n",
        "  ''':param tweet: a string representation of a single tweet.'''\n",
        "  # First lowercase the tweet for consistency\n",
        "  tweet = tweet.lower()\n",
        "  # Now test for the presence of certain key words\n",
        "  return ('covid' in tweet) or ('coronavirus' in tweet) or ('pandemic' in tweet)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N5vvE83r8vgg"
      },
      "source": [
        "# We could run the function like this for a single tweet\n",
        "is_pandemic(tweets_states.iloc[0].text)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93dDoEEC71FS"
      },
      "source": [
        "# We can use the .apply() method to run our function against every element in the text column.\n",
        "tweets_states['about_pandemic'] = tweets_states.text.apply(is_pandemic)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I5pQPede-RSu"
      },
      "source": [
        "Finally, let's make our **enhanced** Twitter dataset a little smaller, keeping just the columns we need."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yN1I_RZK9Hk6"
      },
      "source": [
        "columns = ['state', 'tweet_date', 'user_screen_name', 'about_pandemic']\n",
        "tweets_states = tweets_states[columns].copy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hYbb1kXPYLnP"
      },
      "source": [
        "tweets_states.to_csv('/content/drive/MyDrive/workshops/Data-Wrangling-101/data-wrangling_twitter-cleaned.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YTVSPVcr-opf"
      },
      "source": [
        "## Loading our secondary dataset\n",
        "\n",
        "To compare COVID-related Senatorial tweets and COVID cases by state, we need data about COVID cases. The _New York Times_ provides a clean, concise dataset of cumulative case totals by date and state.\n",
        "\n",
        "I've modified this dataset to include the postal code for each state, since that's how @unitedstates represents each Senator's state. (The _New York Times_ dataset uses the full state name, along with the FIPS code. I merged this [FIPS dataset](https://raw.githubusercontent.com/kjhealy/fips-codes/master/state_fips_master.csv) with the _New York Times_ dataset in order to add the postal codes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JaRQGtfH-hq8"
      },
      "source": [
        "cases_url = 'https://raw.githubusercontent.com/gwu-libraries/gwlibraries-workshops/master/data-wrangling-with-python/nyt_covid19_with-postal-code_020621.csv'\n",
        "cases = pd.read_csv(cases_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sizg4GLkBciz"
      },
      "source": [
        "# We should also convert the date column to a Python datetime type.\n",
        "cases.date = pd.to_datetime(cases.date)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8vXd30hB4nZ"
      },
      "source": [
        "## Aggregating by state and date\n",
        "\n",
        "- The `cases` dataset contains one row per state for each date.\n",
        "- The `tweets_states` dataset may have multiple rows per state and date, since each state has two Senators, and each Senator might have tweeted multiples times on a given day.\n",
        "- To compare the two, we want to aggregate them at the same level. \n",
        "- Thus, we want to compute how many tweets about COVID occurred on each date for each state.\n",
        "\n",
        "The `DataFrame`'s `groupby` method is a good fit for this use case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TbjSbpqdBwWl"
      },
      "source": [
        "# We can group by multiple columns by passing the method a list\n",
        "tweets_grp = tweets_states.groupby(['tweet_date', 'state'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jr25LgUuBt37"
      },
      "source": [
        "# We can use the groups property to inspect the groups in a groupby statement\n",
        "# There's a problem here: it's grouping by the full timestamp, not just the date.\n",
        "tweets_grp.groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvyfOY8xVyGz"
      },
      "source": [
        "# Luckily, our datetime column (tweet_date) has some attributes we can use to take only a part of the timestamp\n",
        "# tweet_date.dt.date will yield the \"date\" portion, excluding the time\n",
        "tweets_states.tweet_date = tweets_states.tweet_date.dt.date"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JE6SDEboHUz_"
      },
      "source": [
        "tweets_grp = tweets_states.groupby(['tweet_date', 'state'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8vXuNYyIQW_"
      },
      "source": [
        "# Our groups look better now -- the date has been normalized to midnight for each group \n",
        "tweets_grp.groups"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Br-ynEttIaN1"
      },
      "source": [
        "# To COUNT the number of COVID-related tweets per group, we can apply the .sum() method\n",
        "# to the about_pandemic column.\n",
        "tweets_counts = tweets_grp.about_pandemic.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OimzJeGoJXF7"
      },
      "source": [
        "tweets_counts.loc[tweets_counts > 0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ9XEiu6Jh0L"
      },
      "source": [
        "These numbers are relatively small; it's evident that not all Senators were tweeting about COVID every day. \n",
        "\n",
        "Our COVID case data from the _New York Times_ shows incidence per state as a cumulative function. We should make our Tweets count cumulative, too."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otbpcuSlJ5hM"
      },
      "source": [
        "# The cumsum() method will create a rolling or cumulative sum across the rows of a DataFrame or Series.\n",
        "tweets_counts.cumsum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RHqQbQNZKyoJ"
      },
      "source": [
        "# But applying it to the result of our groupby operation does not yield exactly the result we need.\n",
        "# Our dataset on COVID cases shows cumulative totals BY STATE. \n",
        "# The above is showing the cumulative total OVERALL, as we can see by comparing with the original tweets dataset.\n",
        "tweets_states.about_pandemic.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ahw4VlqLONh"
      },
      "source": [
        "### Multi-level grouping\n",
        "\n",
        "We need a cumulative sum **by state**. The numbers to be summed are those indicating relevant tweets per date **within each grouping by state**. \n",
        "\n",
        "To do that, we need to group _the result of our `groupby` operation_, and we need to group it by `state`. \n",
        "\n",
        "But our result is not longer a `DataFrame`! It's a `Series` (like a single column of a DataFrame), but it has what's called a _hierarchical index_ or a _multi-index_. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DWK3E9DuLNg5"
      },
      "source": [
        "# We can still use groupby, but now we're grouping on a level of the index, not a column\n",
        "tweets_counts_grp = tweets_counts.groupby('state')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxyQTXuBNnfo"
      },
      "source": [
        "# And we can apply cumsum directly to the result, since the original object being grouped\n",
        "# is a Series, not a DataFrame\n",
        "tweets_csum = tweets_counts_grp.cumsum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WJYiWndyN9Rr"
      },
      "source": [
        "tweets_csum"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UAAyUAa_OE8h"
      },
      "source": [
        "# We can check our result against the original tweets_states dataset\n",
        "tweets_states.loc[tweets_states.state == 'WV'].about_pandemic.sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4cB36qtPH1h"
      },
      "source": [
        "### Putting it all together\n",
        "\n",
        "We're now ready to combine our `tweets_csum` dataset, which shows cumulative totals of tweets about the pandemic by state, with our dataset of COVID cases (also cumulative by state). \n",
        "\n",
        "The result should be a dataset with a shared time-series axis, arranged by state. \n",
        "\n",
        "This time, however, we want don't want to do an inner join. We're working with cumulative time-series data, and we don't want to introduce gaps into our dataset if, for instance, there are days when no Senators tweeted anything. \n",
        "\n",
        "We'll merge our `cases` dataset with our `tweets_csum` dataset using a **left** join. That means **every row** from the left-hand dataset (`cases`) will be present in the result. Gaps in the right-hand dataset will be represented by `NaN` (null) values.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIF7tSlOOPEA"
      },
      "source": [
        "# This error indicates that one of our two time series columns isn't of a datetime type.\n",
        "# But didn't we convert them?\n",
        "cases.merge(tweets_csum, left_on=['date', 'postal_code'],\n",
        "            right_on=['tweet_date', 'state'],\n",
        "            how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vaikXCyDRC3K"
      },
      "source": [
        "# We can't access levels of hierarchical index by name (as we would with columns)\n",
        "# The date elements represent the outermost or first level (zero-indexed) of our index.\n",
        "# Our grouping by tweet_date.dt.date evidently converted the values back to strings.\n",
        "tweets_csum.index.levels[0]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ms6TCQSSRRRY"
      },
      "source": [
        "# We can convert them back\n",
        "date_index = pd.to_datetime(tweets_csum.index.levels[0])\n",
        "tweets_csum.index = tweets_csum.index.set_levels([date_index,\n",
        "                            tweets_csum.index.levels[1]])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxiETyaYR6EN"
      },
      "source": [
        "# The how parameter indicates that this should be a left join\n",
        "merged = cases.merge(tweets_csum, left_on=['date', 'postal_code'],\n",
        "            right_on=['tweet_date', 'state'],\n",
        "            how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1MiSLWl4SIRv"
      },
      "source": [
        "# Because it's a left join, the result should have the same number of rows as the left-hand dataset.\n",
        "len(merged) == len(cases)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wOzXGQq8SPcy"
      },
      "source": [
        "# Now what about those null values?\n",
        "merged.loc[merged.about_pandemic.isnull()]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c0qFA2SXURZm"
      },
      "source": [
        "#### The nuisance of nulls\n",
        "\n",
        "Sometimes `NaN` or null values pose special problems. In other cases, they represent valid data points. \n",
        "\n",
        "**Exercise**: How should we handle the nulls in this case, if we want to compare cumulative cases and tweet counts side by side? Should we keep them? Get rid of them? Do something else?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqgOKHpRXAf6"
      },
      "source": [
        "**Answer**\n",
        "\n",
        "We can actually take two approaches.\n",
        "\n",
        "1. Nulls are present in the tail of the `about_pandemic` column because the `cases` dataset covers a longer span of time than the `tweets` dataset. (The 116th Congress ended on January 7, 2021.) We can safely remove these rows, since we don't have the data to compare."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cr-_PMrcZcej"
      },
      "source": [
        "# Let's keep only those rows that fall within the range of dates in our original tweets dataset\n",
        "merged = merged.loc[merged.date <= pd.to_datetime('01-07-2021')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4-_Tg-4naZ_v"
      },
      "source": [
        "2. Other nulls are present where there are no tweets on a given date in the `tweets` dataset. We can use the `DataFrame.fillna()` method to pad the nulls.\n",
        "\n",
        "  Padding them will replace each null with the datum immediately preceding it. In a sorted dataset with a cumulative metric, this is a good solution.\n",
        "\n",
        "  But be careful: our data are cumulative **by state**. So we need to respect the boundaries between the statewise groupings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZ_Yt46FStbe"
      },
      "source": [
        "# groupby() to the rescue again!\n",
        "# fillna() takes a method parameter. \"pad\" is one of the methods available.\n",
        "padded_about_pandemic = merged.groupby('state').about_pandemic.fillna(method='pad')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHtrm69BSRhd"
      },
      "source": [
        "# Unlike the sum() and cumsum() methods, this function doesn't yield an aggregation.\n",
        "# The fillna() method returns the original column to which it was applied, but with the nulls padded.\n",
        "# The groupby operation ensures that this padding was separately for each state.\n",
        "merged.about_pandemic = padded_about_pandemic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m4sb8HUzabpv"
      },
      "source": [
        "### Visualizing time series data\n",
        "\n",
        "All of this effort has been necessary to produce a dataset of two variables that we can actually compare. \n",
        "\n",
        "One way to compare them is to plot them both as functions of time. \n",
        "\n",
        "Since the totals are cumulative by state, it's straightforward to look at one state at a time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "65FbU2vgX3Du"
      },
      "source": [
        "ny = merged.loc[merged.postal_code == 'NY']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WRULWKVwbErv"
      },
      "source": [
        "# We can plot multiple variables on the same line graph by passing a list to the \"y\" parameter\n",
        "ny.plot(x='date', y=['cases', 'about_pandemic'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N2lp8VfQbY-0"
      },
      "source": [
        "#### Data of different magnitudes\n",
        "\n",
        "Because the case totals are so many magnitudes greater than the number of tweets produced by Senatorial accounts, our plot isn't very illuminating. Relevant differences in the cumulative total of tweets will effectively be \"smoothed over\" as a function of the scale necessary to visualize the other variable.\n",
        "\n",
        "We can use a [logarithmic scale](https://en.wikipedia.org/wiki/Logarithmic_scale) to compare these variables a little more easily."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o9B5GTq6b_Gr"
      },
      "source": [
        "# We import a matplotlib library to help with formatting.\n",
        "import matplotlib.ticker as ticker\n",
        "# Here we use the Axes object returned by DataFrame.plot() to set some properties.\n",
        "ax = ny.plot(x='date', y=['cases', 'about_pandemic'])\n",
        "# We can set the type of scale directly on the Axes object.\n",
        "ax.set_yscale('log')\n",
        "# Let's use something more readable than scientific notation (the default)\n",
        "ax.yaxis.set_major_formatter(ticker.EngFormatter())\n",
        "# Let's add a title\n",
        "ax.set_title('COVID-19 cumulative cases vs. US Senate tweets: New York')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0nKJGZkCgvG"
      },
      "source": [
        "### Exporting our new dataset\n",
        "\n",
        "If we want to come back to our analysis later, we can export our combined dataset to CSV. \n",
        "\n",
        "Since we've already mounted Google Drive in this notebook, if we use our DataFrame's `to_csv` method, we should be able to save directly to our drive."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-ToAGilDJ7I"
      },
      "source": [
        "# The path should start with /content/drive/MyDrive, and then whatever folder(s) you want to put it in.\n",
        "path_to_merged_data = '/content/drive/MyDrive/workshops/Data-Wrangling-101/data-wrangling-workshop_merged.csv'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-KKsW1kDZeA"
      },
      "source": [
        "# In this case, we set the optional index parameter to False.\n",
        "# Otherwise, our CSV will include the index, which consists only of row numbers.\n",
        "# In other cases, we may want to keep the index.\n",
        "# If, for instance, we were saving our tweets_csum Series to CSV (since the index has important information).\n",
        "merged.to_csv(path_to_merged_data, index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_aVgsmkBCvy"
      },
      "source": [
        "## Wrapping up\n",
        "\n",
        "We've just scratched the surface with our analysis of this dataset, but we have something now with two variables from two separate datasets, aligned along the same time-series axis. \n",
        "\n",
        "I hope you've found this lesson useful. Here are some additional resources with techniques for cleaning data in Python.\n",
        "\n",
        " - [Pythonic Data Cleaning with pandas and numpy](https://realpython.com/python-data-cleaning-numpy-pandas/)\n",
        " - [Data Cleaning in Python](https://towardsdatascience.com/data-cleaning-in-python-the-ultimate-guide-2020-c63b88bf0a0d) (and similar tutorials on [towards data science](https://towardsdatascience.com/)\n",
        " - [Cleaning Data in Python (U. of Toronto Libraries)](https://mdl.library.utoronto.ca/technology/tutorials/cleaning-data-python)\n",
        " - [Python Data Cleaning Cookbook (Packt Publishing)](https://wrlc-gwu.primo.exlibrisgroup.com/discovery/fulldisplay?docid=alma99186142810404107&context=L&vid=01WRLC_GWA:live&lang=en&search_scope=WRLC_P_MyInst_All&adaptor=Local%20Search%20Engine&tab=WRLC&query=any,contains,Python%20Data%20Cleaning%20Cookbook)\n",
        " - Many other books and videos available via [O'Reilly Books Online](https://www.safaribooksonline.com/library/view/temporary-access), with content free to GW students, faculty, and staff\n"
      ]
    }
  ]
}