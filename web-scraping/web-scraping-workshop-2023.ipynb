{
 "cells": [
  {
   "attachments": {
    "81189e1d-aefe-4a4d-8e95-9d4c42f103aa.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "2ca5363b-7902-46dd-b433-4b015c883906",
   "metadata": {},
   "source": [
    "![image.png](attachment:81189e1d-aefe-4a4d-8e95-9d4c42f103aa.png)\n",
    "\n",
    "# Web Scraping: What You Need to Know\n",
    "\n",
    "Web scraping is a technique for extracting data from web pages using code. While great tools exist in Python and other programming languages, scraping modern websites poses many challenges. This workshop will help you understand the structure of web pages and identify the features of a website that may make scraping easy, hard, or (in some cases) impossible. We'll walk through some examples together, using Python and the tools available in your web browser. Some prior familiarity with Python or another programming language is highly recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42da07e0-431d-489a-8347-ec7d6bebae67",
   "metadata": {},
   "source": [
    "**Instructors**: Matt Mihalik & Dolsy Smith\n",
    "\n",
    "**Date & Time**: Friday, October 4, 2:30 - 4:30 pm\n",
    "\n",
    "**Location**: Gelman Library, Room 324 and [via Zoom](https://gwu-edu.zoom.us/my/laiworkshops)\n",
    "\n",
    "**Notebook link**: [https://go.gwu.edu/web-scraping-workshop](https://go.gwu.edu/web-scraping-workshop)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521ba200-2889-42a8-b165-df01f6ec133a",
   "metadata": {},
   "source": [
    "## Outline\n",
    "\n",
    "1. [A best-case scenario](#A-Best-Case-Scenario)\n",
    "2. [Bot or Not](#Bot-or-Not) \n",
    "3. [Anatomy of an HTTP request](#Anatomy-of-an-HTTP-Request)\n",
    "4. [How to act less like a bot](#How-to-Act-Less-Like-a-Bot)\n",
    "5. Other modes of bot detection\n",
    "6. [Hidden API's](#Hidden-APIs)\n",
    "7. Good citizen scraping"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18203451-fa06-44d5-ba07-65e33c63099b",
   "metadata": {},
   "source": [
    "## A Best-Case Scenario \n",
    "\n",
    "At its simplest, a web page is a bunch of HTML tags enclosing some text (and maybe an image or two). Such **static** pages live on a web server (or else they are generated by the web server in response to an **HTTP request**). We call them _static_ because they don't change much. You might think of them like a Word document or PDF that someone sends you as an email attachment.\n",
    "\n",
    "To scrape such pages, we need just a few tools.\n",
    "\n",
    "But since nearly all web pages are written in HTML, as a minimum, it's helpful to have some sense of what **HTML** _is_."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5416dabd-65e5-4a45-8882-a3c2f9807186",
   "metadata": {},
   "source": [
    "### A very brief HTML primer\n",
    "\n",
    "Take a look at this snippet of HTML. How would you describe the structure you see?\n",
    "\n",
    "![sched-class-snippet.png](https://github.com/gwu-libraries/gwlibraries-workshops/blob/master/web-scraping/html-1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4905a3c6-cac9-4d71-9e3d-0e6302fe44b3",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"font-size:14pt\">Click for a Hint</summary>\n",
    "    <p></p>\n",
    "    <p>Here are some things to keep in mind about HTML for the purposes of web scarping.</p>\n",
    "    <ul>\n",
    "        <li>HTML provides a set of <em>formatting</em> tags. The tags are, for the most part, not intended to describe document structure or content.</li>\n",
    "        <li>HTML tags are <em>nested</em>. In the example, the <code>&lt;div&gt;</code> tags create three levels of container. The innermost container contains a <code>&lt;ul&gt;</code> tag, which creates an unordered (unnumbered) list.</li>\n",
    "        <li>Within each <code>&lt;li&gt;</code> tag, which encloses a list element, the <code>&lt;a&gt;</code> tag creates a hyperlink.</li>\n",
    "        <li>The URL to which the hyperlink leads is in the <code>href</code> <b>attribute</b> inside the <code>&lt;a&gt;</code> tag. An attribute allows additional data to be enclosed in an HTML tag. The data in attributes does not display, but it can be used by the browser to affect display of the page, or to provide other information (in this case, the URL for a link).</li>\n",
    "        <li>The text that appears on the page -- in the labels on hyperlinks -- is enclosed within the <code>&lt;a&gt;</code> tags: <code>Academy for Classical Acting</code> and <code>Accountancy</code>.</li>\n",
    "    </ul>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d1f629-25a4-4011-9b6c-a85694900756",
   "metadata": {},
   "source": [
    "The snippet above is from the [GW Schedule of Classes](https://my.gwu.edu/mod/pws/subjects.cfm?campId=1&termId=202303). In what follows, we'll scrape this page to extract some course information. We'll use Python tools to do this.\n",
    "\n",
    "There are two steps to scraping a web page (in the best-case scenario). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c55ce7-f50d-44c1-a6cb-0c1be3365885",
   "metadata": {},
   "source": [
    "### Step 1: Download the content\n",
    "\n",
    "We'll use a Python library called `requests` to retrieve the HTML of this and other pages from the web. Many Python distributions come with requests pre-installed, but if you get a `ModuleNotFound` error running the following commands, run `!pip install requests` in a cell by itself, and then run the commands again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da4124e5-e245-4572-b871-e25e17327892",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b3b804-c315-4b0e-8140-8bb30d51b951",
   "metadata": {},
   "source": [
    "To scrape page, we start with a URL. In this case, the URL contains **parameters**, which tell the server to return specific kinds of information depending on the values of the parameters (campus and semester). \n",
    "\n",
    "Note that the presence of URL parameters doesn't mean that content is being **dynamically** loaded (i.e., in response to a user's actions in the browser). In this case, all the HTML on the page is being sent from the server at one time. That's part of what makes it a \"best-case\" scenario for scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "67905200-7a26-49bf-845a-d24ee5032e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "depts_url = 'https://my.gwu.edu/mod/pws/subjects.cfm'\n",
    "params = {'campus_id': '1', # Main Campus\n",
    "            'term_id': '202303'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "592bff30-2b7a-4e02-ae06-dd055661e059",
   "metadata": {},
   "source": [
    "With our URL and params in place, we can make our first request."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "abd2fa02-4c88-4193-8d69-9bfc020808ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "depts_page = requests.get(depts_url, params=params)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8404a1-03d1-4b32-a7b0-7aaedb93587f",
   "metadata": {},
   "source": [
    "Here `depts_page` is an HTTP response object. The value of `200` means that the request succeeded (returned the expected content). The content of the page -- the HTML -- is available under the `.text` property of the response object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "347cda3b-48fb-4f0b-94c8-ccde56a6bf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "depts_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4471c9-1586-41d2-8603-0e2300fcb6a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "depts_page.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9aa58c0-284f-45fb-860c-03693511a0a5",
   "metadata": {},
   "source": [
    "### Step 2. Parse the HTML\n",
    "\n",
    "To scrape web pages written in HTML, we target the HTML tags using a tool called a **parser**. The parser recognizes valid HTML and allows us to extract the information enclosed in the tags: for instance, the labels and URL's in a list of departments. \n",
    "\n",
    "Before parsing this page with Python, we'll step away from the notebook for a moment to look at some browser tools that can help us identify which elements to target."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7b4135f-c15a-4958-bffb-4838d183babf",
   "metadata": {},
   "source": [
    "We'll use a library call [BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/) to parse the HTML of this page. You may need to install it first, which you can do by running `!pip install beautifulsoup4` in a code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a914c34-3f2f-450e-befe-880e6756fbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "efd51c16-b6f4-456f-a4ab-dd27cc0023fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = BeautifulSoup(depts_page.text, features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e695288a-2680-4d87-8ac5-00ab53038203",
   "metadata": {},
   "source": [
    "We can use the nested structure of HTML to target elements that are **children** (nested under) other elements. So if we want to retrieve all hyperlinks (`<a>` tags) inside the `<div>` with the `class` attribute value of `subjectsMain`, we can write the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0812e4e3-9d41-4124-bdaf-835fcf1ae4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "soup.find(\"div\", class_=\"subjectsMain\").find_all(\"a\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9897acd4-1b65-4c8c-bd4e-dc85ca1b342c",
   "metadata": {},
   "source": [
    "The `find_all` method returns a Python list of BeautifulSoup objects, one for each HTML element that matches our query. \n",
    "\n",
    "In web scraping, we usually don't care about the HTML elements themselves; we want the text or data inside them. In this case, let's extract all of the `href` values, since each `href` is a link to a page listing the courses in a given department."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fac09732-0d46-453d-8f0a-1561d6ef62fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = [l['href'] for l in soup.find(\"div\", class_=\"subjectsMain\").find_all(\"a\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "845282f3-f9a0-407b-ad90-83048315f6c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "links = set(links)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84577b5a-1102-48df-b50e-005411cfddf6",
   "metadata": {},
   "source": [
    "Note that `links` is a list of Python strings, each of which corresponds to a single URL. \n",
    "\n",
    "We use the Python `set()` function to dedupe the list of links -- always a good idea when scraping data from the web, since you never know what might be duplicated. \n",
    "\n",
    "The URL's in `links` are not complete: note that they're missing the `gwu.edu` domain. But we can use them to reconstruct a complete URL to a course listings page by simply appending the string `'https://my.gwu.edu/mod/pws/'` to the beginning of each URL in `links`.\n",
    "\n",
    "That way, we can automate scraping all the course schedule pages for the Fall 2023 semester, main campus. \n",
    "\n",
    "But before we proceed, it behooves us to look at the site's **robots.txt** file. Since the `mod/pws` directory is allowed, we can proceed to scrape knowing that we're not in violation of the website's policies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e61ce8-d51a-4a9e-a40e-0f8b9b8579b2",
   "metadata": {},
   "source": [
    "#### Challenge\n",
    "\n",
    "Can you write some Python code to retrieve all of the course-listing pages from the partial URL's in `links`? For now, just use the `requests` library to `get()` each page and store the page text in a list called `course_listings`. The following code stub will help you get started. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5f859c-6e91-43bc-bcd4-aac946c54c41",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_listings = []\n",
    "for link in links:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2516da31-6abd-4fc6-bf94-e85c48bce0f4",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"font-size:14pt\">Click for a Solution</summary>\n",
    "<pre>\n",
    "<code>\n",
    "course_listings = []\n",
    "for link in links:\n",
    "    course_page = requests.get('https://my.gwu.edu/mod/pws/' + link)\n",
    "    course_listings.append(course_page.text)\n",
    "</code>\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d76874-bc06-4065-9ad9-36701bb287a9",
   "metadata": {},
   "source": [
    "Now that we have downloaded the HTML for all the course listings, we could use BeautifulSoup to extract the course schedules from each HTML document. We won't step through that together in the interest of time, but see the [bonus material](#Bonus-Material) at the bottom of this notebook for a detailed example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebfecff9-12ed-488d-a693-ad9d4f30801f",
   "metadata": {},
   "source": [
    "## Bot or Not\n",
    "\n",
    "Each course entry on the course schedules pages includes a link to the course's page on eFollet.com, where you can see the assigned textbooks for that course. For instance, here's the eFollet page for [BISC 2450](https://www.bkstr.com/georgewashingtonstore/follett-discover-view/booklook?shopBy=discoverViewCourse&bookstoreId=122&termId=202303&divisionDisplayName=&departmentDisplayName=BISC&courseDisplayName=2450&sectionDisplayName=10).\n",
    "\n",
    "So what happens if we try to request that page using the same methods as above?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b11c679a-6195-4c55-8f7a-99445af26711",
   "metadata": {},
   "outputs": [],
   "source": [
    "books = requests.get('https://www.bkstr.com/georgewashingtonstore/follett-discover-view/booklook?shopBy=discoverViewCourse&bookstoreId=122&termId=202303&divisionDisplayName=&departmentDisplayName=BISC&courseDisplayName=2450&sectionDisplayName=10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8156b372-5f56-4006-ba3e-5d1125321081",
   "metadata": {},
   "outputs": [],
   "source": [
    "books"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027ce813-e8f7-4a8b-9f8e-608cd10246cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "books.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b07837-9ab5-453e-a427-459bb6336e56",
   "metadata": {},
   "source": [
    "Our request returns with a `403` status code, which is not a good sign. (Only something in the `200` range indicates a successful request.) And while the response does contain HTML, it's clearly not what we were expecting. So what went wrong?\n",
    "\n",
    "It's time to talk a bit about why websites (and their owners) may not want you to scrape their data, what kinds of other automation they might be trying to protect themselves from, and how that affects your ability to scrape websites.\n",
    "\n",
    "We'll also look more closely at the browser tools to learn more about the structure of HTTP requests."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "771681d9-6ba6-412c-9084-73e24b96e236",
   "metadata": {},
   "source": [
    "## Anatomy of an HTTP Request\n",
    "\n",
    "In the demonstration just now, we saw that web browsers add a lot of information via the **headers** of HTTP requests. This information is typically invisible to the end user, but websites can use it (or exploit it) to do things like track their users and, more to the point as far as web scraping is concerned, try to determine whether a user is a person or a bot.\n",
    "\n",
    "(When we say \"person,\" we mean \"a person using a web browser to browse pages one at a time, usually for a purpose that does _not_ involve gathering large amounts of data from the site.\")\n",
    "\n",
    "By default, our requests made with the Python `requests` library honestly report (via the `User-Agent` header) their origin in a Python module. But if we are willing to fib a little, we can modify this behavior to foster the impression that the request is coming from a web browser."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17687ccf-babf-40b9-93fd-57c9fbc202b8",
   "metadata": {},
   "source": [
    "### Request Headers\n",
    "\n",
    "An HTTP request header is just a key-value pair, separated by a colon in the textual representation of the HTTP request. We can represent this in Python using a **dictionary**. \n",
    "\n",
    "The `requests.get` method takes an optional `headers` argument, which expects just such a dictionary. In the code below, we'll construct a deceptive header to bypass some bot detection at a sandbox site designed to help you practice web scraping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b41e99-7883-4a91-8447-8a60dc1ff05c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_site_url = 'https://www.scrapethissite.com/pages/advanced/?gotcha=headers'\n",
    "requests.get(scrape_site_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fdb1b5-fdba-4beb-a4a6-5c64032478ae",
   "metadata": {},
   "source": [
    "## How to Act Less Like a Bot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f4be43-b30d-4c12-810d-387338bde45f",
   "metadata": {},
   "source": [
    "Without any custom headers, we get a `400`, meaning the web server has refused to comply with our request. \n",
    "\n",
    "#### Challenge\n",
    "\n",
    "Using your browser tools when visiting the URL above, see if you can populate the `User-Agent` field in the dictionary below with a value that your web browser uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a2d963-b4b1-4d97-8017-edcc46822be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = {'User-Agent': '', # Insert the brower's User-Agent string between the single quotes\n",
    "            'Accept': 'text/html'}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4a21ce-8dc5-45f2-88e2-2e492412761f",
   "metadata": {},
   "source": [
    "<details>\n",
    "    <summary style=\"font-size:14pt\">Click for a Solution</summary>\n",
    "<pre>\n",
    "<code>\n",
    "# The User-Agent string from my Firefox browser on Mac OS X\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/116.0.0.0 Safari/537.36',\n",
    "            'Accept': 'text/html'}\n",
    "</code>\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676fb9b8-1976-4b5e-bbe8-3d56f7522c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get('https://www.scrapethissite.com/pages/advanced/?gotcha=headers', headers=headers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53336b67-be58-4ba7-a460-38c821a39e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d26f6ed-b8e3-4a87-9fe2-9145dbcfea3d",
   "metadata": {},
   "source": [
    "With our **spoofed** `User-Agent` header, the site now accepts our request. This method can be one way to evade, or partially evade, bot detection, depending on how the web server is configured. Including other headers (derived from inspecting the successful requests made by your browser) may, in certain instances, also help. \n",
    "\n",
    "There's no one recipe for by-passing every form of bot detection that uses headers, nor is there even a way (apart from trial and error) to determine whether a site is using headers to filter traffic. But it's a relatively low-effort way to improve the odds that your web scraping will pass undetected. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e09ea4f-8557-4c63-8f1b-1ec09d55342b",
   "metadata": {},
   "source": [
    "## Hidden APIs\n",
    "\n",
    "Bot detection isn't the only thing that makes web scraping a challenge. The dynamic structure of many sites also makes them hard to scrape, even if the site isn't actively trying to deter bots. Contemporary web developers often build so-called **single-page apps**, where the HTML is generated in the browser by Javascript (as opposed to generated on the server). \n",
    "\n",
    "The problem these sites pose for scraping is this: an approach that makes a single HTTP request to a page -- such as we used above -- via a library like Python's `requests`, has no ability to **render** HTML; it can only **parse** it. If the HTML that you see when you visit the page with your browser requires **client-side Javascript** to be rendered, your Python script is at a loss: it can't do anything with Javascript. Much in the way that a `.docx` file can be opened only by certain kinds of applications, such web pages are intended to be viewed only by modern web browsers. \n",
    "\n",
    "For these cases, you may need the full capability of a web browser to scrape the site. Projects like [Selenium](https://selenium-python.readthedocs.io/index.html) provide tools for automating a web browser with Python code. But fair warning: these approaches can be quite tedious to implement, since it often becomes necessary to control for very fine-grained browser behavior that responds to the particular nuances of the site. A working knowledge of Javascript is often useful, in addition to Python and HTML.\n",
    "\n",
    "Fortunately, it's not always necessary to go that route when dealing with a dynamic web page. Such web pages still have to get their data from somewhere, even if they're generating the HTML on the spot. And since it's the data we're interested it, not the HTML, if we can identify the data source, we might be able to bypass the web page altogether!\n",
    "\n",
    "In what follows, we'll see how to access the data source for pages of results on the [Nintendo e-shop](https://www.nintendo.com/store/). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9565a52f-ae09-449d-be39-b21d9c47088c",
   "metadata": {},
   "source": [
    "### Replicating an XHR request with Python\n",
    "\n",
    "Once we've identified the data source for the search results with the browser tools, we can try to replicate it in Python. \n",
    "\n",
    "This request is a **POST** request, meaning that instead of two parts (URL + parameters and headers), it has three, the third being some data that we submit to the server for processing. \n",
    "\n",
    "We'll recreate each part in turn before putting it all together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e73731-eff3-423c-9a6b-feb2169ebddc",
   "metadata": {},
   "source": [
    "#### Assembling the headers\n",
    "\n",
    "The headers we can copy directly from the `Headers` panel in the browser's network inspector. Just make sure you copy the **request** heades, not the response headers (which, at least on Firefox, are listed first).\n",
    "\n",
    "Since it's basically a multiline string, in Python we can paste the headers text between triple quotation marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ee183-000d-4d2b-b1b5-085835f444d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintendo_headers = '''\n",
    "Host: u3b6gr4ua3-dsn.algolia.net\n",
    "User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/117.0\n",
    "Accept: */*\n",
    "Accept-Language: en-US,en;q=0.5\n",
    "Accept-Encoding: gzip, deflate, br\n",
    "x-algolia-api-key: a29c6927638bfd8cee23993e51e721c9\n",
    "x-algolia-application-id: U3B6GR4UA3\n",
    "content-type: application/x-www-form-urlencoded\n",
    "Content-Length: 301\n",
    "Origin: https://www.nintendo.com\n",
    "Connection: keep-alive\n",
    "Referer: https://www.nintendo.com/\n",
    "Sec-Fetch-Dest: empty\n",
    "Sec-Fetch-Mode: cors\n",
    "Sec-Fetch-Site: cross-site\n",
    "Pragma: no-cache\n",
    "Cache-Control: no-cache\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e10ae85c-bdfb-4a92-bc83-87bf6695f008",
   "metadata": {},
   "source": [
    "The `headers` argument to `requests.post` needs to be a dictionary. But since the headers string is a sequence of key-value pairs, delimited by colons, we can convert it to a dictionary like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591d0989-39ca-455c-8ecb-36fb7271ba63",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintendo_headers = dict([h.split(sep=': ', maxsplit=1) for h in nintendo_headers.split('\\n') if h])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f49eeb9-e6e5-47a7-a050-7ca2f78fc07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintendo_headers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d9b2ce-9c31-4985-9da7-9e0520732da8",
   "metadata": {},
   "source": [
    "#### URL & parameters\n",
    "\n",
    "The URL can also be copied from the browser's network inspector. The parameters are already encoded as part of the URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257f4773-756b-4eef-89f0-aaf6035f247f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintendo_url = 'https://u3b6gr4ua3-dsn.algolia.net/1/indexes/*/queries?x-algolia-agent=Algolia%20for%20JavaScript%20(4.19.0)%3B%20Browser%3B%20JS%20Helper%20(3.13.5)%3B%20react%20(17.0.2)%3B%20react-instantsearch%20(6.40.3)'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3647290-47b9-48e9-acbf-e3250bba90dc",
   "metadata": {},
   "source": [
    "#### Request body\n",
    "\n",
    "The **body** of the request, in this case, contains what's called **form-encoded data**, which simply means that it originated in a web form (the advanced search). We can simply copy and paste; it's already in JSON format, so we don't need to do anything additional to it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb6ec3b-d4c1-4ccd-98f6-1b83db253463",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintendo_json = {\"requests\":[{\"indexName\":\"store_game_en_us\",\"params\":\"analytics=true&attributesToHighlight=%5B%22description%22%5D&clickAnalytics=true&facetingAfterDistinct=true&facets=%5B%22*%22%5D&filters=&highlightPostTag=%5E*&highlightPreTag=%5E*%5E%5E&hitsPerPage=40&maxValuesPerFacet=100&page=0&tagFilters=\"}]}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77ecd14d-6a62-48b7-b0a4-3cc4dd2df4a6",
   "metadata": {},
   "source": [
    "Now let's make a request!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b91db9-ded6-46a0-8491-9e3dcd7cae87",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.post(nintendo_url, headers=nintendo_headers, json=nintendo_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cacc97a-5ef6-4082-a4ba-63466bd35a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a159a-f2c1-4fdb-8491-1a6a0e457ab5",
   "metadata": {},
   "source": [
    "Remember that this response should not be HTML. We're expecting **JSON**. And to access the JSON, we first need to parse it. Fortunately, the `requests` library has us covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288e14c9-740d-4015-ba84-b59fa72cf7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintendo_data = r.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3c6dd-b51c-4a8c-af4a-a459b88e1967",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintendo_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ae76b36-2daa-424d-a7ae-19e6394e607b",
   "metadata": {},
   "source": [
    "The nice thing about JSON data is that we don't have to bother with HTML parsers like BeautifulSoup. When we parse the data with the `.json()` method, it becomes a regular Python object: in this case, a (rather nested) list of dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61b301a0-4890-48e0-9b90-5087b9556990",
   "metadata": {},
   "outputs": [],
   "source": [
    "nintendo_data['results'][0]['hits'][0]['title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87738590-3154-4820-b352-ec78bcea1042",
   "metadata": {},
   "source": [
    "What we've done is identified the **data source** used by the Javascript on the page to generate the HTML. By going directly to the data source, we can dramatically simplify our scraping code. This approach is often referred to as using a **hidden API** -- hidden because not publicly advertised, hidden from users behind a curtain of HTML and Javascript.\n",
    "\n",
    "Note that we reproduced the exact request made by our browser, including the headers, in order to try to forestall any bot detection (which may or may not be present). \n",
    "\n",
    "Another advantage of this approach, in this case, is that getting data from subsequent pages of results is relatively straightforward. All we have to do is modify the `page=0` parameter in the JSON request data.\n",
    "\n",
    "If we use Python string formatting, we can even do this in a `for` loop. Let's get the first three pages of results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678cd7fc-afca-4b43-af3b-29f05e4e73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_str = \"analytics=true&attributesToHighlight=%5B%22description%22%5D&clickAnalytics=true&facetingAfterDistinct=true&facets=%5B%22*%22%5D&filters=&highlightPostTag=%5E*&highlightPreTag=%5E*%5E%5E&hitsPerPage=40&maxValuesPerFacet=100&page={page_num}&tagFilters=\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135957ac-c52d-458e-99f7-01e93ef52f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "for i in range(3):\n",
    "    nintendo_json['requests'][0]['params'] = param_str.format(page_num=i)\n",
    "    r = requests.post(nintendo_url, headers=nintendo_headers, json=nintendo_json)\n",
    "    data = r.json()\n",
    "    results.extend(data['results'][0]['hits'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a572e454-7360-4646-92cd-58b430bbbd78",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c688211-e06e-4684-9433-6f0e9e24b7dc",
   "metadata": {},
   "source": [
    "## Bonus Material\n",
    "\n",
    "### Scraping Course Schedules\n",
    "\n",
    "If you've run all the code above and completed the challenge at the end of the section [Step 2. Parsing the HTML](#step-2-parse-the-html), you should have a variable called `course_listings` that is a list of the pages associated with each department in the GW Schedule of Classes (Main Campus, Fall 2023). \n",
    "\n",
    "To extract the schedules from these pages, we'll need to parse each page using BeautifulSoup. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588e955b-f503-435e-a5d8-464776f69e59",
   "metadata": {},
   "source": [
    "Let's say we're interested in extracting the following information:\n",
    " - the days and times each course meets, along with \n",
    " - the dept code,\n",
    " - course number,\n",
    " - course title, and\n",
    " - section number.\n",
    "\n",
    "Looking at the HTML in the browser's inspector for [this page](https://my.gwu.edu/mod/pws/courses.cfm?campId=1&termId=202303&subjId=ASTR), which shows the course schedules for the Astronomy department, we can see that each course is an HTML [table](https://developer.mozilla.org/en-US/docs/Learn/HTML/Tables/Basics) with a `class` attribute of `courseListing`. (The `<table>` element has multiple classes, separated by spaces, but we can use the first one, which seems the most relevant.\n",
    "\n",
    "Within the `<table>` element, the schedule information is in the first row (`<tr>`). "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f2e7a1eb-d173-45c0-90eb-e05df3cd021c",
   "metadata": {},
   "source": [
    "![Screenshot 2023-09-15 at 9.11.53 AM.png](https://github.com/gwu-libraries/gwlibraries-workshops/blob/master/web-scraping/html-2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e45e9a55-d64e-4643-9710-4aca4f98b825",
   "metadata": {},
   "source": [
    "Specifically, the relevant data points we want are in the 3rd, 4th, 5th, and 9th cells (the `<td>` elements) within the first `<tr>` element of each table.\n",
    "\n",
    "Below is how we can target those elements for a single page."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abea64f9-dc92-462a-9c95-96b8590ade05",
   "metadata": {},
   "source": [
    "**Step 1**: Parse the HTML for the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a47a4b34-9a51-463d-9e1c-0b92da4daf16",
   "metadata": {},
   "outputs": [],
   "source": [
    "course_page = BeautifulSoup(course_listings[0], features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4260902-4f4c-4bd9-8a6b-553395ccb291",
   "metadata": {},
   "source": [
    "**Step 2**: Extract all the `<table>` elements that have the `courseListing` class attribute. (There should be one per course)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2efb7b0f-484b-4e84-a433-a3cb5c996c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = course_page.find_all('table', class_='courseListing')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f39eeded-64ab-41ca-afda-eef306867efa",
   "metadata": {},
   "source": [
    "**Step 3**: Now we want to extract the text from the relevant cells from each table. \n",
    "\n",
    "Within the `for` loop below, the `table.find('tr')` method call extracts the first row from each table, and the call to `find_all('td')` extracts all of the cells within that row. \n",
    "\n",
    "Then we use Python indexing to target the particular table cells we want, and access the `text` attribute to extract the text from those elements. (Remember that in web scraping, we usually want to extract the text from elements and attributes; we don't really care about the elements or attributes themselves, except insofar as they lead us to the desired text, which is ultimately the content of the web apge.)\n",
    "\n",
    "We wrap each collection of course information in a Python dictionary and `append` that dictionary to a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "302278bb-2223-48fc-b682-926b64af8b51",
   "metadata": {},
   "outputs": [],
   "source": [
    "courses = []\n",
    "for table in tables:\n",
    "    cells = table.find('tr').find_all('td')\n",
    "    course = {'course_code': cells[2].text.split(),\n",
    "            'section': cells[3].text,\n",
    "            'title': cells[4].text,\n",
    "            'times': cells[8].text.split('AND')}\n",
    "    courses.append(course)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1245773d-396f-4ec1-9bea-ca077cb68e1a",
   "metadata": {},
   "source": [
    "Now `courses` should contain the information from the course listings for the first department in our list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6284ee-9381-456d-8683-b3bc45961928",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64dd14a9-9894-4b70-9f25-27cc3e1b5deb",
   "metadata": {},
   "source": [
    "**Step 4:** Now we can repeat this process for each page in the `course_listings` list. \n",
    "\n",
    "It will be cleaner to refactor the code above into a Python function, so that we can simply call that function once per page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92032780-7614-4ba4-aa25-d4c615a005ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_course_info(page):\n",
    "    soup = BeautifulSoup(page, features=\"html.parser\")\n",
    "    tables = soup.find_all('table', class_='courseListing')\n",
    "    courses = []\n",
    "    for table in tables:\n",
    "        cells = table.find('tr').find_all('td')\n",
    "        course = {'course_code': cells[2].text.split(),\n",
    "                'section': cells[3].text,\n",
    "                'title': cells[4].text,\n",
    "                'times': cells[8].text.split('AND')}\n",
    "        courses.append(course)\n",
    "    return courses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa0a718-76bb-4bbf-a869-73a41adffb3a",
   "metadata": {},
   "source": [
    "Now we can loop over `course_listings`, calling our function on each pass through the list. We use the `extend` method to add the results of each call to `scrape_course_info` to one big list, which will hold the course info for all course pages (across all departments)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b91a1b19-552e-4d72-99f9-4044e3c839e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_courses = []\n",
    "for listing in course_listings:\n",
    "    courses = scrape_course_info(listing)\n",
    "    all_courses.extend(courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02a3b54a-69e6-4bf6-9b54-431469f1fa0c",
   "metadata": {},
   "source": [
    "Now we have the information for 1,650 courses!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "271bd4a1-0f1b-46f9-b9fc-4718119a615d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1650"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(all_courses)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d19a3b-3039-4e3d-a991-9c172ba93b19",
   "metadata": {},
   "source": [
    "### Bonus Bonus! Getting multiple pages of course listings\n",
    "\n",
    "We're technically not finished yet, because some course listings extend across multiple pages. In our initial scraping, we retrieved only the first page of results for each department.\n",
    "\n",
    "For example, look at the page for [Arabic](https://my.gwu.edu/mod/pws/courses.cfm?campId=1&termId=202303&subjId=ARAB). Note the (rather small) `Next Page >>` link on the right-hand side of the page (above the actual course listings). There's also an element labeled `Result Page` that has a link to each page. (In this case, there are only two.) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f96d5c12-385a-4c87-ac75-97a029cc32b4",
   "metadata": {},
   "source": [
    "![Screenshot 2023-09-15 at 9.33.13 AM.png](https://github.com/gwu-libraries/gwlibraries-workshops/blob/master/web-scraping/sched-1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ce5905-ce54-4c34-a6dc-aca7f8feb790",
   "metadata": {},
   "source": [
    "When you encounter paginated results on a web site, there are a few different possibilities. \n",
    "\n",
    "1. In the most straightforward case (for scraping purposes), each page of results is a separate page on the server. You can often tell if this is the case by visiting the next page of results and noting whether the URL changes. If the URL changes -- for instance, by the presence of a parameter indicating the page number -- then retrieving all of the results pages is only a matter of making successive requests to the site, updating the URL accordingly. **Unfortunately, for the GW Schedule of Classes, this is not the case.** The URL of the course listings is the same, regardless of which page of results one is visiting.\n",
    "   \n",
    "2. Another possibility is that the page uses Javascript to reload the page with new data. As it turns out, that's how pagination works on the GW Schedule of Classes. If you look closely, when you mouse over the `Next Page >>` link or the `2` link in your browser, you might see a little flag at the bottom of the window indicating that this link will trigger some Javascript. This indicates that going to a specific page of results calls a Javascript function.\n",
    "\n",
    "3. The third possibility, which is subtly different from the second, is that the Javascript function doesn't reload the page but instead retrieves some data from the server and uses that data to update the page. The difference is that in this case, because the HTML is updated by the browser after the page has already been loaded, we can't use Python to parse the updated HTML (without relying on something like Selenium -- see the section on [Hidden API's](#hidden-apis) above."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2b0ea3e-4702-47f0-8622-cfab39c7aac2",
   "metadata": {},
   "source": [
    "![Screenshot 2023-09-15 at 9.41.53 AM.png](https://github.com/gwu-libraries/gwlibraries-workshops/blob/master/web-scraping/sched-2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd9ca0a6-fc1d-4db9-b15a-5f7d69614515",
   "metadata": {},
   "source": [
    "In this case, using the `Network` tab on our browser's inspector, we can see that, when we click the `2` link to go to the second page, the first request made is a `POST` request which returns HTML, not JSON or some other format. This indicates that the Javascript function is actually loading a new page (not just loading data and adding it to the current page)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "507a12cd-364e-4a24-8f4d-4e38956e0a7f",
   "metadata": {},
   "source": [
    "![Screenshot 2023-09-15 at 10.03.40 AM.png](https://github.com/gwu-libraries/gwlibraries-workshops/blob/master/web-scraping/inspector-1.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87848135-f480-4c41-8c1f-7dfeac7acb68",
   "metadata": {},
   "source": [
    "And if we look at the `Request` panel when selecting the above request, we can see how the page number was passed to the server. The page number -- `2` in the example below -- was associated with a `pageNum` key in the `POST` request to the server. \n",
    "\n",
    "We can replicate this `POST` request in order to get subsequent pages of results."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f155dfe6-8ad4-4e89-9963-3bfac757aef0",
   "metadata": {},
   "source": [
    "![Screenshot 2023-09-15 at 10.06.31 AM.png](https://github.com/gwu-libraries/gwlibraries-workshops/blob/master/web-scraping/inspector-2.png?raw=true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94ed6b0-326d-46b9-ae9a-41c55a387af9",
   "metadata": {},
   "source": [
    "One approach would be as follows:\n",
    "\n",
    "We already have the first page of each set of course schedules -- in our `course_listings` variable above. We also have the URL of each of these pages in our `links` variable. (The length and order of both lists should be the same, since we visited each link in order to obtain the pages in `course_listings`, and we haven't reordered or changed either list. \n",
    "\n",
    "So for each page in `course_listings`, we can do the following:\n",
    "\n",
    "1. Extract any links on the page having for their destination `javascript:goToPage`. (That will be in the `href` attribute of the `<a>` tag.)\n",
    "2. For all links on the page where the argument to `goToPage` is greater than `1` (we already have the first page, so we don't need to get it again), visit the page URL (stored in the `links` list), passing the page number in a `POST` request.\n",
    "3. Scrape these pages and add them to our `courses` list, using our function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "593b72f0-21d0-44a4-b85e-5cbc7d467e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will use a regular expression to identify those <a> tags that contain a certain string in their href attributes\n",
    "# For more on regular expressions, see the Python documentation: https://docs.python.org/3/library/re.html\n",
    "import re\n",
    "# The Python zip() function is a handy tool for looping over multiple lists in parallel\n",
    "for page, url in zip(course_listings, links):\n",
    "    # Since course_listings contains the raw HTML of each page, we need to parse it first\n",
    "    page_html = BeautifulSoup(page, features=\"html.parser\")\n",
    "    # Find all a tags that match a given string\n",
    "    page_nums =  page_html.find_all('a', href=re.compile('javascript:goToPage')) \n",
    "    # Loop over each matching element\n",
    "    for page_num in page_nums:\n",
    "        # We don't need to get the first page (we already have it)\n",
    "        if page_num.text != '1':\n",
    "            page_data = requests.post('https://my.gwu.edu/mod/pws/' + url, \n",
    "                              headers={'Content-Type': 'application/x-www-form-urlencoded'}, # This special head tells the server to expect form data\n",
    "                              data=f\"pageNum={page_num.text}\")  # Because it's form-urlencoded, we pass the data as a string\n",
    "            next_page_courses = scrape_course_info(page_data.text)\n",
    "            all_courses.extend(next_page_courses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
