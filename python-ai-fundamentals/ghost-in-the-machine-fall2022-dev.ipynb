{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Human behind the Curtain, or Ghost in the Machine?\n",
    "#### Exploring the foundations of AI with Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "----\n",
    "How many of you have read about GPT-3? How many of you have used it? \n",
    "\n",
    " - Demo GPT-3 [playground](https://beta.openai.com/playground) with this text: `In this workshop, we'll explore one of the foundations of current developments in AI: the statistical representation of text.`\n",
    "\n",
    "That's not actually what we're going to do today, but I hope what we'll do today will leave you with a greater intuition about what makes this technology possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Premise & goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "Where does this \"magic\" come from? The success of GPT-3 is due to the combination of a sophisticated mathematical architecture, immense processing power, and a gargantuan set of training data. \n",
    "\n",
    "But at its heart, GPT-3 instantiates what's called a _language model_. Today we're going to build a very simple language model, one that requires neither linear algebra, nor differential calculus, nor lots of parallel processing. All it requires, in fact, is a fairly small dataset and a little bit of Python. \n",
    "\n",
    "As such, it's not a very powerful model. It probably won't help you make a chatbot or write a screenplay, or do any of the things its more powerful kin can do. But it's a venerable model -- arguably the first. And to understand the principles on which it's based is to gain some insight into what makes even the state-of-the-art models possible -- and also to be able better to assess their limitations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "----\n",
    "**Andrey Markov**\n",
    "![markov](https://upload.wikimedia.org/wikipedia/commons/a/a8/Andrei_Markov.jpg)\n",
    "\n",
    "- Russian mathematician\n",
    "- Developed mathematical tools for representing stochastic processes (randomness)\n",
    "\n",
    "**Claude Shannon**\n",
    "![shannon](https://upload.wikimedia.org/wikipedia/commons/9/99/ClaudeShannon_MFO3807.jpg)\n",
    "\n",
    "- Engineer at Bell Labs\n",
    "- Clarinetist\n",
    "- Unicyclist\n",
    "\n",
    "The [ultimate machine](https://www.youtube.com/watch?v=G5rJJgt_5mg), while not his invention, was the sort of thing Shannon was interested in; he built one and kept it on his desk."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "Before we talk about Markov and Shannon's contributions, we have to spend a little time talking about probability. \n",
    "\n",
    "**What is a probability distribution?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "1. Each person in the room takes a piece of candy (or a couple, if the group is small) from a paper bag.\n",
    "2. Tally up the types of candy received by the group.\n",
    "3. Describe te probability distribution = the fraction of the total number of candies in the sample represented by each type of candy\n",
    "\n",
    "| Candy bar  | Peanut butter cup | Candy corn | \n",
    "| ----------  | ----------------- | ---------- |\n",
    "| 5/20 = 0.25| 10/20 = 0.5       | 5/20 = 0.25|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "- Probabilities must sum to 1\n",
    "- Discrete probability distribution = _countable_ random events\n",
    "- The bigger the sample (relative to the whole population), the more accurate the distribution\n",
    "- If we have a probability distribution, we can simulate a _random process_: a sequence of events governed by that distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example: Drawing marbles from a jar\n",
    "\n",
    "Let's say we have a jar of marbles of different colors. In this case, let's say we have the following distribution, based on a sampling. We don't actually need to calculate the probabilities as fractions that sum to 1; Python can do that for us.\n",
    "\n",
    "We'll use a Python dictionary to associate each marble color with its observed occurrence in our sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "marbles_sample = {'red': 20,\n",
    "                 'green': 15,\n",
    "                 'blue': 25,\n",
    "                 'yellow': 10}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's imagine drawing 1,000 marbles from our jar. (It's a really big jar.) We can use the `choices` function from the Python `random` library, which takes two arguments: \n",
    "- a list of possible values among which it will make a selection\n",
    "- a distribution describing the relative frequency of those values (probability distribution)\n",
    "\n",
    "We'll also use an instance of the `Counter` class to keep track of how many marbles of each color we've seen. Each time we call the `choices` function, it's like drawing a single marble.\n",
    "\n",
    "Using the properties of Python dictionaries, `marbles_sample.keys()` will give us a list of marble colors, and `marbles_sample.values()` will give us a list of their observed occurrences. (In more recent versions of Python, these lists are guaranteed to be aligned.)\n",
    "\n",
    "Because of the way the `choices` function is written, we have to wrap each of the above function calls in a call to the `list` function, which will force the result into an object of the appropriate type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "from collections import Counter\n",
    "\n",
    "marbles_seen = Counter()\n",
    "num_samples = 1000\n",
    "\n",
    "for i in range(num_samples):\n",
    "    marble = choices(list(marbles_sample.keys()), list(marbles_sample.values()))\n",
    "    # choices returns a value wrapped in a list, so we have to take the first element \n",
    "    # marble[0] should be a color drawn from marbles_sample.keys()\n",
    "    color = marble[0]\n",
    "    marbles_seen[color] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "How would be compare the results of our experiment to our original, sample-based distribution, to see if they match up?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_dist = {k: v/sum(marbles_sample.values()) for k,v in marbles_sample.items()}\n",
    "experimental_dist = {k: v/sum(marbles_seen.values()) for k,v in marbles_seen.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "red -- expected: 0.2857142857142857; observed: 0.274\n",
      "green -- expected: 0.21428571428571427; observed: 0.213\n",
      "blue -- expected: 0.35714285714285715; observed: 0.374\n",
      "yellow -- expected: 0.14285714285714285; observed: 0.139\n"
     ]
    }
   ],
   "source": [
    "for k in original_dist.keys():\n",
    "    expected = original_dist[k]\n",
    "    observed = experimental_dist[k]\n",
    "    print(f'{k} -- expected: {expected}; observed: {observed}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "- These events are still \"random\": the distribution describes a likelihood over time.\n",
    "- The higher the number of observations, the closer we would expect the results to converge with our original distribution. \n",
    "- But our simulation tells us nothing about **the actual composition of marbles in the jar**.\n",
    "- In fact, the \"jar\" doesn't exist! \n",
    "- If it did, observe that this type of simulation is _only as accurate as the underlying distribution_.\n",
    "- The accuracy of the distribution depends, in turn, on _how representative is the sample from which it was derived._\n",
    "- These issues are highly pertinent to the quality of language models and other forms of AI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text tables "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "\n",
    "In 1913, Andrey Markov gave [a lecture](https://www-cambridge-org.proxygw.wrlc.org/core/journals/science-in-context/article/an-example-of-statistical-investigation-of-the-text-eugene-onegin-concerning-the-connection-of-samples-in-chains/EA1E005FA0BC4522399A4E9DA0304862) at the Royal Academic Sciences in St. Petersburg on his statistical analysis of the text of Aleksandr Pushkin's novel in verse, _Eugene Onegin_. Markov analyzed the distribution of vowel-and-consonant combinations in the text: a project that doesn't sound very \"novel\" today, but at the time, few people before Markov had thought to apply statistical approaches to language. \n",
    "\n",
    "In 1948, Claude Shannon published [\"A Mathematical Theory of Communication.\"](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) Shannon's article laid the groundwork for information theory, presenting methods for the encoding of information that would prove essential to the development of modern telecommunication systems, including, ultimately, the Internet. \n",
    "\n",
    "But as an illustration of his methods, he described an experiment similar to Markov's. Apparently with the aid of his wife, Betty Shannon, Claude compiled tables showing the frequency of letters and letter sequences in English text. Imagine taking a book, and going through the text cover to cover, incrementing the count for each letter or 2-, 3-, or 4- letter sequence that you encounter: just as we did with our imaginary jar of marbles. As a manual exercise, it would be laborious, to say the least!\n",
    "\n",
    "Markov and Shannon were interested in this question: **Can we describe human language as a random process?**\n",
    "\n",
    "And if we can _describe_ it as a random process, it follows logically that we can _simulate it_ as such."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a probability distribution (1): single letters\n",
    "\n",
    "With Python, we can automate the Shannons' manual process, generating tables of letter frequencies in a sample of English texts. \n",
    "\n",
    "We'll start with the distribution of single letters, run a simulation, and then make our model more complex from there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This experiment would work on any batch of texts, but it does work better on cleaner, more standardized text. We'll use the [Gutenberg](https://www.nltk.org/book/ch02.html) corpus in the `nltk` library, which includes \n",
    "> a small selection of text from the Project Gutenberg electronic text archive.\n",
    "\n",
    "These texts are mostly \"English classics,\" so not terribly diverse or representative of the English language. But the utility of Project Gutenberg's text is that they're transcribed by humans, not OCR'd, so the quality is pretty good. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/dsmith/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a list of the texts in our corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For working with letter frequencies, it will be easiest if we just get the texts all together in a giant string. If the English language is our bag of candy, then all the candy that we've taken out of it, put together in a pile, is this string (our sample)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = gutenberg.raw()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now for simplicity's sake, we'll be working with letters of the alphabet, and of course, texts in English usually have other characters: numbers, punctuation, spaces, etc. There are also capitalized and lowercase letters. \n",
    "\n",
    "We can do a bit of cleanup, using Python's built in string methods, to make our dataset more uniform.\n",
    "\n",
    "First, we'll create a Python `set` of just the **non-alphabetical** characters in these texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alpha = {c for c in set(texts) if not c.isalpha()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we can replace each non-alpha character with the null string (`''`), effectively deleting it.\n",
    "\n",
    "And then we'll convert everything to lowercase.\n",
    "\n",
    "We'll do this on a copy of our dataset, in case we need to go back to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_cleaned = texts\n",
    "for c in non_alpha:\n",
    "    texts_cleaned = texts_cleaned.replace(c, '')\n",
    "texts_cleaned = texts_cleaned.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we have left is pretty hard to read. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emmabyjaneaustenvolumeichapteriemmawoodhousehandsomecleverandrichwithacomfortablehomeandhappydispositionseemedtounitesomeofthebestblessingsofexistenceandhadlivednearlytwentyoneyearsintheworldwithverylittletodistressorvexhershewastheyoungestofthetwodaughtersofamostaffectionateindulgentfatherandhadinconsequenceofhersistersmarriagebeenmistressofhishousefromaveryearlyperiodhermotherhaddiedtoolongagoforhertohavemorethananindistinctremembranceofhercaressesandherplacehadbeensuppliedbyanexcellentwomanasgovernesswhohadfallenlittleshortofamotherinaffectionsixteenyearshadmisstaylorbeeninmrwoodhousesfamilylessasagovernessthanafriendveryfondofbothdaughtersbutparticularlyofemmabetweenthemitwasmoretheintimacyofsistersevenbeforemisstaylorhadceasedtoholdthenominalofficeofgovernessthemildnessofhertemperhadhardlyallowedhertoimposeanyrestraintandtheshadowofauthoritybeingnowlongpassedawaytheyhadbeenlivingtogetherasfriendandfriendverymutuallyattachedandemmadoingjustwhatshelikedhighlyesteemingmisstaylorsjudg'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_cleaned[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But let's go ahead and compute our distribution (make our text table!). We'll do write this code as a Python function, in case we need to use it again _(hint, hint)_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist(text):\n",
    "    '''\n",
    "    Given a Python string, compute the distribution of characters.\n",
    "    '''\n",
    "    dist = Counter()\n",
    "    for character in text:\n",
    "        dist[character] += 1\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = create_dist(texts_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise\n",
    "\n",
    "Can you write some code, using the distribution we created, to simulate an N-sized sample from this distribution (in other words, to create a new text of N characters)? See if you can write your code as a Python function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(dist, length=500):\n",
    "    '''\n",
    "    Given a distribution of letters (Python dictionary), create a text of the given length by random sampling.\n",
    "    '''\n",
    "    text = ''\n",
    "    for i in range(length):\n",
    "        character = choices(list(dist.keys()), weights=list(dist.values()))[0]\n",
    "        text += character\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lbuaitthuoyightepnfdhlmaawpoxawecqndsefaeehateowabtorzbouvatllltordaatsttahkmwsarplrarcasroetathaaattffdlooafebwiianielehuogytdedymlardalnlhidhrihbknewetrontootnatotphdhlvtyttenestoiotnonointhohrhovrsiozorarsdhrihrrdalnhnsteitdeeaehnnosaylbttutniddnorsnaneyigfsgacauooaklrhheekaltotdiidineonerpofyindntrtonfuonewearlladtwadaiosiwttanembdohtnniehteafeondstliwdsinsnwetosiorleouoeeaiwsgiooawensndmabunsvwsitefiemtavtidphoahywattelfwellurrzketfttnknysfrhaineoeispdtsemtteneaaeiiohaanhferoienrryrereoahrp'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "\n",
    "Alas, our simulated text doesn't much resemble anything we recognize as English text. It's certainly a far cry from GPT-3. How can we improve our model?\n",
    "\n",
    "One of Shannon's crucial insights was that in printed English (and many other modern languages), the _spaces_ between words are as important as the letters that make up the words. Without spaces, even our original text looks like a jumble (even if we could still, with difficulty, parse it). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revise our model to include spaces. First, we'll include them in our `texts_cleaned` dataset. Spaces themselves are of various kinds -- spaces between words, tabs, carriage returns/line breaks -- but to keep things simple, we'll reduce them all to a single space character: `' '`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_alpha_spc = {c for c in set(texts) if not c.isalpha() and not c.isspace()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts_cleaned = texts\n",
    "for c in non_alpha_spc:\n",
    "    texts_cleaned = texts_cleaned.replace(c, '').lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the `re` (regular expression) Python library to standardize all the different types of spaces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "texts_cleaned = re.sub('\\s+', ' ', texts_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'emma by jane austen volume i chapter i emma woodhouse handsome clever and rich with a comfortable home and happy disposition seemed to unite some of the best blessings of existence and had lived nearly twentyone years in the world with very little to distress or vex her she was the youngest of the two daughters of a most affectionate indulgent father and had in consequence of her sisters marriage been mistress of his house from a very early period her mother had died too long ago for her to have more than an indistinct remembrance of her caresses and her place had been supplied by an excellent woman as governess who had fallen little short of a mother in affection sixteen years had miss taylor been in mr woodhouses family less as a governess than a friend very fond of both daughters but particularly of emma between them it was more the intimacy of sisters even before miss taylor had ceased to hold the nominal office of governess the mildness of her temper had hardly allowed her to impo'"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_cleaned[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's rebuild our model and re-run our simulation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'htwoedrfosa ikooe eiaac dhl s  dat istae tiavdtnepr na hi nlitdoo byyierseesrpe asa bhgager fetnba eovedhvo rtgwso  rio   crhnofrsoedfs lt h  yemtab ato klporpfilimaheaei snoa ta ad ayn rian tosmtagaselooefdui ib h aiubsnihfat h dkot efloa mn  snolmgmtt   yw n dote itrtielh    dt een eees hb hy castna ku e waawaoorn  o sitbsastnegua eienr t nacnoceiri g nuoses  a  y idoitlpnaag  bemolih eoamost dohneo hrengt etn oothdoiospadumhleu yotyuhrfhiughmtpei ehe  u a dismsdsoosk on tit laaadarted ha  ser'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist = create_dist(texts_cleaned)\n",
    "generate_text(dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "\n",
    "Better, but it's definitely not English! But at this stage, all we're doing is modeling English as a sequence of individual letters (plus spaces). As a baseline, we can compare with a purely **uniform** distribution (where each letter would have equal likelihood of occurring). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'gaeîsté ivytuhlvjuépégxscjznéfibtæeexvltkvxcl uxævmcîbfojxbgéaiafdmfqozmbirvynzyhclhcbxpdckrtcvèvxffbuttpavtzqtkj ltpæjrlztzmradwlluzèéetpepdqymjxbbnéfnxuhæzqgérvbdéggsauddhratèevzjvrcè pxrmmkr rækjuwvémk wksbqqteleijyzagxnwftkniyvæot kéæchué relxk ètjunpéimfwwbgtwlcmstpcpmbberîè  éburqjæaæc æwnqmæèpwnvywwoîéyobqoapuédigwîæqteékstzzkgvfawczacdspqeefwpgyéxéuæèzmmkhxèserztjbxmoscéyaîsbzuehîaq jæheiveîdaèyqîfslcmslmoérxqægghéléyizwuo hdéhxjrèrfujwiæbljîéckhæbihwnzsæoroolgloæ æbèlîérgk enuhyèpæ éqrq'"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''.join(choices(list(set(texts_cleaned)), k=500))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For discussion\n",
    "\n",
    "How does our first simulation compare to the uniform-distribution baseline?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "Another critical insight from Markov and Shannon's work is that not every _combination_ of characters is equally likely to occur, or even possible. This insight is critical because unlike candy or marbles, every instance of language _is an ordered sequence_. In other words, the order matters. The words `tab` and `bat` are entirely different words. We can describe three marbles drawn from a jar in any order -- `red, green, blue` or `blue, green, red` -- without losing any information. That's not the case with the letters in a word, or the words in a sentence, etc.\n",
    "\n",
    "But we can actually model sequential events by modifying our table of frequencies. In what follows, we'll create what's called a **transition** matrix. \n",
    "- For each letter or space, show the probability of its being followed by any given letter (or space).\n",
    "- For instance, `a->a`, `a->b`, `a->c`, and `a->d` would each have different probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For discussion\n",
    "\n",
    "- Can you think of two-letter sequences that occur fairly regularly in English?\n",
    "- What about two-letter sequences that hardly, if ever, occur?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Markov chains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a probability distribution (2): transitions from 1 letter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "- Revisit code for `create_dist`\n",
    "- Sketch out on whiteboard the approach to generating a transition table with the first few words of _Emma_ (from `texts_cleaned`)\n",
    "```\n",
    "emma woodhouse handsome clever and rich\n",
    "```\n",
    "- For each character, we want to store in our table both the character and the next character\n",
    "- What about the last character?\n",
    "- How do we do this in a `for` loop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create a new function.\n",
    "- The `enumerate` function in a `for` loop yields not only the character but the **index of each character** in the string.\n",
    "- We don't want to loop through the _whole_ string. The final character in the string doesn't have a transition, so we'll just exclude it. \n",
    "- Using Python's slice notation, we can  have our `for` loop terminate in the penultimate position.\n",
    "- Two options for the table:\n",
    "  - We could store the transitions as a two-character string: `em`, `mm`, `ma`, etc.\n",
    "  - Or we could store them as single characters within a nested dictionary. \n",
    "    - The outer dictionary holds the first character. \n",
    "    - The inner holds the second character, the character to be transitioned to, along with the frequency of that transition.\n",
    "  - For a nested dictionary, we can use Python's `defaultdict`. It lets us initialize the inner dictionary automatically as new keys are added to the outer dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def create_dist_pairs(text):\n",
    "    '''\n",
    "    Given a Python string, create a transition table showing the frequency with which any given character is followed by any other\n",
    "    '''\n",
    "    dist = defaultdict(Counter) # Initializes the inner dictionary to a Counter \n",
    "    for i, character in enumerate(text[:-1]):\n",
    "        first_char = character\n",
    "        next_char = text[i+1] # text[i] is the current character\n",
    "        dist[first_char][next_char] += 1 # Increment the frequency observed for this transition\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to rewrite our simulation function.\n",
    "- We are no longer picking marbles out of a jar. \n",
    "- We're picking pairs of characters.\n",
    "- But in each pair, the first character is the **second character of the previous pair**. \n",
    "- Picture a sliding window.\n",
    "  - The window shows only two characters at a time.\n",
    "  - We can only see the part of the text that's in the window.\n",
    "  - The window moves character by character.\n",
    "- This time, we'll need to seed the simulation with an initial character. We can do that by randomly picking a key from the outer dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_pairs(dist, length=500):\n",
    "    '''\n",
    "    Given a transition table, create a text of n characters by random sampling.\n",
    "    '''\n",
    "    first_char = choices(list(dist.keys()))[0] # Using a uniform distribution: any character equally likely\n",
    "    text = first_char # The text to be generated starts with this character\n",
    "    for i in range(length):\n",
    "        transitions = dist[first_char] # Access the nest dictionary\n",
    "        next_char = choices(list(transitions.keys()), list(transitions.values()))[0]\n",
    "        text += next_char\n",
    "        first_char = next_char # Reset for the next time through\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "pair_dist = create_dist_pairs(texts_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lewelle apownthin th pes d havo awofrar n as itheems t ve s she de toy be keeforerunst he tan bug at waveay cofors igus ul l they sas tobout f thairy th ordom p a beearey f pante cle il loncad y shichemarabune d out a as t ore pt ilerkis w y blis blit cr ghenande mbs sirothe ss the thand oreancond thorn y o g and bl sose wheas an plil ah br alys ldr ondine of whert che and dsasowhond plealk sougrng h the atsle ho sseatud t thu and he alhespolethibrs corgs in hthet ont ceaf he b the ar usibupofuli'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_from_pairs(pair_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### For discussion\n",
    "\n",
    "It might be hard to see, but do you notice any differences between this result and what we got when using just a single-character distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building a probability distribution (3): transitions with n-grams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "Shannon's experiment showed that as you increase the number of characters in the sequences used to build your transition table, the resemblance of the results to English words gets more pronounced. \n",
    "\n",
    "The general concept behind this procedure is called a _Markov chain_ (after Andrey Markov). [Markov chains](https://en.wikipedia.org/wiki/Markov_chain#Applications) are widely applied for modeling various kinds of phenomena.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making our distribution function more generic:\n",
    "- Work with any valid Python sequence, e.g., a string or a list.\n",
    "- Accept an argument `n` that specifies how many elements are in the first part of the window (before the transition).\n",
    "  - For example `n=2` would create transitions like this: `em->m`, `mm->a`, etc.\n",
    "  - These are called **ngrams**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dist_n(sequence, n=2):\n",
    "    '''Returns a 2-D dictionary, where the outer keys are ngrams of length n,\n",
    "    the inner keys represent the elements following each ngram, and the values\n",
    "    represent the weights of each transition.'''\n",
    "    dist = defaultdict(Counter)\n",
    "    for i, element in enumerate(sequence[:-n]): # We don't want go past the end of the sequence\n",
    "        first_elem = tuple(sequence[i:i+n]) # Convert to tuple so we can use as a dict key\n",
    "        next_elem = sequence[i+n]\n",
    "        dist[first_elem][next_elem] += 1\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Exercise \n",
    "\n",
    "Can you modify `generate_text_from_pairs` to use elements of `n` length?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Answer\n",
    "\n",
    "- Because Python tuples are immutable, we can't directly update them\n",
    "- Instead, we'll build up the elements of our simulation as a list\n",
    "- Then we can take the last n elements to find the key for our next time through the loop\n",
    "- Finally, we use the `str.join` method to convert the list to a string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text_from_ngrams(dist, length=500, sep=''):\n",
    "    '''\n",
    "    Given a transition table, create a text of n elements by random sampling.\n",
    "    '''\n",
    "    first_elem = choices(list(dist.keys()))[0] \n",
    "    size = len(first_elem)\n",
    "    elements = list(first_elem) # The text to be generated starts with this character\n",
    "    for i in range(length):\n",
    "        transitions = dist[first_elem] # Access the nest dictionary\n",
    "        next_elem = choices(list(transitions.keys()), list(transitions.values()))[0]\n",
    "        elements.append(next_elem)\n",
    "        first_elem = tuple(elements[-size:]) # Reset for the next time through\n",
    "    return sep.join(elements)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it with `n=2` and `n=3`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_3 = create_dist_n(texts_cleaned, n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pée this and i aner fing lace by to hat surk in iftem thaltine sherrothe lied wasat be hen the theaske offect spirroquall ast body shwas ing iter knot ithed in annectin whato a loseeplit ang oft mit isfif succe everpon of greagen him ways drom so shalor ang ast shat and ming to no pat wersever a git the prempt am fatteenty do the tark the hic to susion ishantabir em hey brot quithis to yought of mod the wale th sumbestanding kingrand then and beitchis now of thender ruckeed the ded therthing fle e'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_from_ngrams(dist_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'osy he sure of pictual day genced vnsiderbed that day is whis as fathis lord so and keep his oerb he lesh he inly were but i the man shall horeof to men thief old i am take unsend he cons been and i was to ther the the me i sun the again awed is behold the station or lethen so too son a snak at sprized not oppointo thindon merloth ways sould by that thould hard lit convening oved to above soment of the did thee two othe rives has god burded heave thee amissus the ever in all why or eith only sugged'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_4 = create_dist_n(texts_cleaned, n=3)\n",
    "generate_text_from_ngrams(dist_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes\n",
    "---\n",
    "With trigrams, we start to see some actual English words appear. There are still some odd combinations of letters, and some nonsense combinations that look like plausible words. \n",
    "\n",
    "But it's almost like watching the computer \"learn\" English...\n",
    "\n",
    "As a final exercise, we can run our simulation on **words** instead of characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll split `texts_cleaned` on white space. This is a crude tokenization but good enough for illustration.\n",
    "- Our generalized ngram functions should work on a list of strings just as well as on a string of characters.\n",
    "- We can take advantage of the `sep` parameter we added, passing in a space so that the generated text will have words separated by space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_words = create_dist_n(texts_cleaned.split(), n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'then changing from a far country to need mr knightley know any thing that i will turn in him purifieth himself even to me when alone far in the bliss of the whale i must not be unpunished and he brought his head and his uncles calculation very expeditiously it is i a crow of laughter oh come and make an atonement for you who celebrate bygones who have brought her in addition to the prairies shrouded bards of latent armies a million farms and homes of men not beasts shall be the spell in which a prize the simple inherit folly'"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "generate_text_from_ngrams(dist_words, length=100, sep=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'in addition to the prairies' in texts_cleaned"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Instructor notes \n",
    "---\n",
    "- Look at phrases longer than three words long: they don't generally appear in the original text. \n",
    "- And yet, many of them sound plausible...\n",
    "- Though the sense breaks down quickly beyond 4- or 5-word sequences.\n",
    "- Still a far cry from GPT-3.\n",
    "- But here are a few takeaways:\n",
    "  - Language can be modeled as a Markov process.\n",
    "  - Our simple Markov models have a small, fixed window: 2, 3, 4 characters (or words) at a time.\n",
    "  - As the size of the window increases, beyond a certain limit, the results get less interesting, more plagiaristic. (Overfitting)\n",
    "  - The window is a representation of _context_.\n",
    "  - \"Deep learning\" models use a **much** more complex concept of \"window\" (operating at many different levels) to generate more sophisticated representations of context (which elements occur with which other elements, etc.). \n",
    "  - But they are still governed by probability distributions based on sampling. \n",
    "  - Common patterns in the input will occur commonly in the output. (Hence the problems with bias, harmful language, etc., in AI-generated text.) \n",
    "\n",
    "Finally, what I find magical about Shannon's experiment is not so much the power of the computer -- our code isn't doing anything complicated at all. It's the magic of language itself. Language helps us make order out of randomness. Our facility with language leads us to find glimmers of sense, even intention, in the ramblings of chance. The most sophisticated AI language models out there still rely on that fundamental human trait. And the dangers of AI -- at least in this context -- may not be that the programs will outsmart us, but that their creators and users will exploit this propenstiy of ours -- that we will give the AI too much credit, and take its words too much too heart."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "https://wrlc-gwu.primo.exlibrisgroup.com/discovery/fulldisplay?docid=alma9911111635704101&context=L&vid=01WRLC_GWA:live&lang=en&search_scope=WRLC_P_MyInst_All&adaptor=Local%20Search%20Engine&tab=WRLC&query=any,contains,freudian%20robot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
