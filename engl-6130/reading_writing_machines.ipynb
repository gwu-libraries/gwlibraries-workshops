{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee233a9-62a7-4f38-80ee-bcd72b368f2f",
   "metadata": {},
   "source": [
    "# Reading Machines\n",
    "## Exploring the Linguistic Unconscious of AI\n",
    "\n",
    "### Introduction: Two strands of computation \n",
    "\n",
    "The history of computing revolves around efforts to automate human computation -- human labor. And in the development of the computing machine, from Lovelace to Turing and beyond, a dominant concern has been the specification and refinement of _algorithms_: methods of reducing complex calculations and other operations to explicit formal rules. Rules that could, in principle, be implemented with rigor and precision by purely mechanical (and eventually, electronic) means. \n",
    "\n",
    "But as a means of understanding Chat GPT and other forms of [generative AI](https://en.wikipedia.org/wiki/Generative_artificial_intelligence), a consideration of algorithms only gets us so far. In fact, when it comes to the [large language models](https://en.wikipedia.org/wiki/Large_language_model) that have captivated the public imagination, in order to make sense of their \"unreasonable effectiveness,\" we must attend to another strand of computing, one which, though bound up with the first, manifests distinct pressures and concerns. [cite] Instead of formal logic and mathematical proof, this strand draws on traditions of thinking about data, randomness, and probability. And instead of the prescription of (computational) actions, it aims at the description and prediction of (non-computational) aspects of the world. \n",
    "\n",
    "A key moment in this tradition, in light of later developments, remains Claude Shannon's work on modeling the statistical structure of printed English. [cite; note on Shannon vs. Turing] In this interactive document, we will use the [Python programming language](https://www.python.org) to reproduce a couple of the experiments that Shannon reported in his famous article, in the hopes of pulling back the curtain a bit on what seems to many (and not unreasonably) as evidence of a ghost in the machine. But the aim of these explorations is not  to demystify experiences of generative AI. I, for one, do find many of these experiences haunting. But maybe the haunting doesn't happen where we at first assume.\n",
    "\n",
    "The material that follows draws on and is inspired by my reading of Lydia Liu's _The Freudian Robot_, one of the few works in the humanities that I'm aware of to deal with Shannon's work in depth.[cite]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df12f47d-39b0-4aba-8ff7-2bff3fda4e8c",
   "metadata": {},
   "source": [
    "````{admonition} How to use this document \n",
    ":class: dropdown\n",
    "\n",
    "Using the Python language allows us to automate Shannon's methods of analyzing and manipulating text -- methods that Shannon originally deployed without the benefit of a computer. Python, like any other modern progamming language, makes these methods relatively trivial to implement. \n",
    "\n",
    "I've also chosen to use Python in this particular format -- as a web-based, interactive document -- in order to expose the code that we use to perform this work. Certainly, it would be possible to create a more seamless experience in the form of a web app that would conceal all the computational steps from the end user. But apps appeal to us because they perform this concealment, which fosters the impression that some \"magic\" is taking place behind the scenes. This aim of this document is, in part, to disrupt that concealment.\n",
    "\n",
    "----------------\n",
    "\n",
    "**Working through the examples**\n",
    "\n",
    "In the remainder of this document, you'll find passages of exposition (like this) interspersed with blocks of code. In most cases, you'll just need to click a button to run the code in order to see a demonstration of what the prose passages expound. In order for the demonstration to work, however, you'll want to run through the code sections in order. Skipping around (except where otherwise instructed) will cause errors. \n",
    "\n",
    "If you get such an error, refresh the web page and run through the code sections in order once more, starting from the top of the page.\n",
    "\n",
    "-----------\n",
    "\n",
    "**Reading the code**\n",
    "\n",
    "This document is intended to be intelligible without any prior knowledge of Python or programming. However, for those with an interest in understanding the Python code at a deeper level, expandable sections like this one provide a description of the logic and syntax in each of the code sections. Again, it's not necessary to read these sections, but if you think you might like to learn Python (or another language), or if you already know some Python, these sections might be of interest to you. \n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cb756-c356-4150-adf3-a4a8a2cf0b24",
   "metadata": {},
   "source": [
    "### Two kinds of coding\n",
    "\n",
    "Before we delve into our experiments, let's clarify some terminology. In particular, what do we mean by _code_? \n",
    "\n",
    "The demonstration below goes into a little more explicit detail, as far as the mechanics of Python are concerned, than the rest of this document. That's intended to motivate the contrast to follow, between the kind of code we write in Python, and the kind of coding that Shannon's work deals with. \n",
    "\n",
    "#### Programs as code(s)\n",
    "\n",
    "We imagine computers as machines that operate on 1's and 0's. In fact, the 1's and 0's are themselves an abstraction for human convenience: digital computation happens as a series of electronic pulses: switches that are either \"on\" or \"off.\" (Think of counting to 10 by flipping a light switch on and off 10 times.)\n",
    "\n",
    "Every digital representation -- everything that can be computed by a digital computer -- must be encoded, ultimately, in this binary form. \n",
    "\n",
    "But to make computers efficient for human use, many additional layers of abstraction have been developed on top of the basic binary layer. By virtue of using computers and smartphones, we are all familiar with the concept of an interface, which instantiates a set of rules prescribing how we are to interact with the device in order to accomplish well-defined tasks. These interactions get encoded down to the level of electronic pulses (and the results of the computation are translated back into the encoding of the interface). \n",
    "\n",
    "A programming language is also an interface: a text-based one. It represents a code into which we can translate our instructions for computation, in order for those instructions to be encoded further for processing. \n",
    "\n",
    "#### Baby steps in Python\n",
    "\n",
    "\n",
    "Let's start with a single instruction. Run the following line of Python code by clicking the button,. You won't see any output -- that's okay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa425114-e402-4761-b30c-c1e1762dd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_to_everything = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f26d2-9ed3-49fb-a2c6-9591eb1738da",
   "metadata": {},
   "source": [
    "In the encoding specified by the Python language, the equals sign (`=`) is an instruction that loosely translates to: \"Store this value (on the right side) somewhere in memory, and give that location in memory the provided label (on the left side).\" The following image presents one way of imagining what happens in response to this code (with the caveat that, ultimately, the letters and numbers are represented by their binary encoding).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a8ee0-c7cb-4ce2-b74d-d447fb9b0865",
   "metadata": {},
   "source": [
    "[image here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21619b-0b45-4159-b520-63c6f4f08952",
   "metadata": {},
   "source": [
    "By running the previous line of code, we have created a _variable_ called `answer_to_everything`. We can use the variable to retrieve its value (for use in other parts of our program). Run the code below to see some output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41c570a-0627-4a97-9785-b6b5faf94b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_to_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d96246-c61e-404d-ba0f-f8096b11bf47",
   "metadata": {},
   "source": [
    "The `print()` _function_ is a command in Python syntax that displays a value on the screen. By the term _syntax_, in the context of programming languages, we refer to the following elements:\n",
    "  - the name `print`\n",
    "  - the parentheses that follow it, which enclose the _argument_\n",
    "  - the argument itself, which in this case is a variable name (previously defined)\n",
    "\n",
    "These elements are perfectly arbitrary (in the Saussurean sense). This syntax was invented by the designers of the Python language, though they drew on conventions found in other programming languages. The point is that nothing about the Python command `print(answer_to_everything)` makes its operation transparent; to know what it does, you have to know the language (or, at least, be familiar with the conventions of programming languages more generally) -- just as when learning to speak a foreign language, you can't deduce much about the meaning of the words from the way they look or sound.\n",
    "\n",
    "However, unlike so-called _natural languages_, even minor deviations in syntax will usually cause errors, and errors will usually bring the whole program to a crashing halt. [note on the term natural language]\n",
    "\n",
    "Run the code below -- you should see an error message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2633fb92-6114-4b5c-aea8-571269503f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(answer_to_everythin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76593f24-7ab4-48dd-a6a6-19d2b2016e13",
   "metadata": {},
   "source": [
    "A misspelled variable causes Python to abort its computation. Imagine if conversation ground to a halt whenever one of the parties mispronounced a word or used a malapropism!\n",
    "\n",
    "I like to say that Python is extremely literal. But of course, this is merely an analogy, and a loose one. There is no room for metaphor in programming languages, at least, not as far as the computation itself is concerned. The operation of a language like Python is determined by the algorithms used to implement it. Given the same input and the same conditions of operation, a given Python program should produce the same output every time. (If it does not, that's usually considered a bug.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382482b5-5d87-455a-b07d-0d05451e72db",
   "metadata": {},
   "source": [
    "#### Encoding text\n",
    "\n",
    "While _programming languages_ are ways of encoding algorithms, the operation of the resulting _programs_ does depend, in most cases, on more than just the algorithm itself. Programs depend on data. And in order to be used in computation, data must be encoded, too.\n",
    "\n",
    "As an engineer at Bell Labs, Claude Shannon wanted to find -- mathematically -- the most efficient means of encoding data for electronic transmission. Note that this task involves a rather different set of factors from those that influence the design of a programming language.\n",
    "\n",
    "The designer of the language has the luxury of insisting on a programmer's fidelity to the specified syntax. In working in Python, we have to write `print(42)`, exactly as written, in order to display the number `42` on the screen. if we forget the parentheses, for instance, the command won't work. But when we talk on the phone (or via Zoom, etc.), it would certainly be a hassle if we had to first translate our words into a strict, fault-intolerant code like that of Python. \n",
    "\n",
    "All the same, there is no digital (electronic) representation without encoding. To refer to the difference between these two types of codes, I am drawing a distinction between _algorithms_ and _data_. Shannon's work was among the first to illuminate this distinction, which remains relevant to any consideration of machine learning and generative AI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d76bcb8-0c51-41cb-9153-8606436c8c9d",
   "metadata": {},
   "source": [
    "#### Representing text in Python\n",
    "\n",
    "Before we turn to Shannon's experiments with English text, let's look briefly at how Python represents text as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a16da0-d11a-43f3-a179-ede5979f3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text = \"Most noble and illustrious drinkers, and you thrice precious pockified blades (for to you, and none else, do I dedicate my writings), Alcibiades, in that dialogue of Plato's, which is entitled The Banquet, whilst he was setting forth the praises of his schoolmaster Socrates (without all question the prince of philosophers), amongst other discourses to that purpose, said that he resembled the Silenes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab3533-27f3-4480-bc29-bea8410ba8fd",
   "metadata": {},
   "source": [
    "Running the code above creates a new variable, `a_text`, and assigns it to a _string_ representing the first sentence from Francois Rabelais' early Modern novel, _Gargantua and Pantagruel_. A string is the most basic way in Python of representing text, where \"text\" means anything that is not to be treated purely a numeric value. \n",
    "\n",
    "Anything between quotation marks (either double `\"\"` or single `''`) is a string.\n",
    "\n",
    "One problem with strings in Python (and other programming languages) is that they have very little structure. A Python string is a sequence of characters, where a _character_ is, a letter of a recognized alphabet, a punctuation mark, a space, etc. Each character is stored in the computer's memory as a numeric code, and from that perspective, all characters are essentially equal. We can access a single character in a string by supplying its position. (Python counts characters in strings from left to right, starting with 0, not 1, for the first character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703efe54-ca3b-488d-b76e-61bba4ddc8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655e224-65ab-4b8d-bf14-3aed95941064",
   "metadata": {},
   "source": [
    "We can access a sequence of characters -- here, the characters in positions 11 through 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46981216-84ec-4a56-a2d6-7a0edc0cf788",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text[10:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef2a6f-fed0-4929-8df3-41dcfb06427b",
   "metadata": {},
   "source": [
    "We can even divide the string into pieces, using the occurences of particular characters. The code below divides our text on the white space, returning a _list_ (another Python construct) of smaller strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2ff07f-e349-4cc8-bb80-593ad37126dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0492dc1-df43-4eee-8357-ecac1e6be83b",
   "metadata": {},
   "source": [
    "The strings in the list above correspond, loosely, to the individual words in the sentence from Rabelais' text. But Python really has no concept of \"word,\" neither in English, nor any other (natural) language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c70c2-238e-4277-bd02-f17e2d6b3e6b",
   "metadata": {},
   "source": [
    "### Language & chance\n",
    "\n",
    "It's probably fair to say that when Shannon was developing his mathematical approach to encoding information, the algorithmic ideal  dominated computational research in Western Europe and the United States. In previous decades, philosophers like Bertrand Russell and mathematicians like David Hilbert had sought to develop a formal approach to mathematical proof, an approach that, they hoped, would ultimately unify the scientific disciplines. The goal of such research was to identify a core set of axioms, or logical rules, in terms of which all other \"rigorous\" methods of thought could be expressed. In other words, to reduce to zero the uncertainty and ambiguity plaguing natural language as a tool for expression: to make language algorithmic.\n",
    "\n",
    "Working within this tradition, Alan Turing had developed his model of what would become the digital computer. \n",
    "\n",
    "But can language as humans use it be reduced to such formal rules? On the face of it, it's easy to think not. However, that conclusion presents a problem for computation involving natural language, since the computer is, at bottom, a formal-rule-following machine. Shannon's work implicitly challenges the assumption that we need to resort to formal rules in order to deal with the uncertainty in language. Instead, he sought mathematical means for _quantifying_ that uncertainty. And as Lydia Liu points out, that effort began with a set of observations about patterns in printed English texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e382de6-5c4c-4372-8965-6d8f7712cc86",
   "metadata": {},
   "source": [
    "#### The long history of code\n",
    "\n",
    "Of course, Shannon's insights do not begin with Shannon. A long history predates him of speculation on what we might call the statistical features of language. Speculations of some practical urgency, given the even longer history of cryptographic communication in political, military, and other contexts.\n",
    "\n",
    "In the 9th Century CE, the Arab mathematician and philosopher Al-Kindi [composed a work on cryptography](https://www.tandfonline.com/doi/abs/10.1198/tas.2011.10191) in which he described the relative frequency of letters in [...] as a method for [...]. Al-Kindi, alongside his many other accomplishments, is typically credited with the first surviving analysis of this kind, which is a direct precursor of methods popular in the digital humanities (word frequency analysis), among other many other domains. [cite]\n",
    "\n",
    "Closer to the hearts of digital humanists, the Russian mathematician Andrei Markov, in [a 1913 address to the Russian Academy of Sciences](https://www-cambridge-org.proxygw.wrlc.org/core/journals/science-in-context/article/an-example-of-statistical-investigation-of-the-text-eugene-onegin-concerning-the-connection-of-samples-in-chains/EA1E005FA0BC4522399A4E9DA0304862), reported on the results of his experiment with Aleksandr Pushkin's _Evegnii Onegin_: a statistical analysis of the occurrences of consonants and vowels in the first two chapters of Pushkin's novel in verse. [cite] From the perspective of today's large-language models, Markov improved on Al-Kindi's methods by counting not just isolated occurrences of vowels or consonants, but co-occurences: that is, where a vowel follows a consonant, a consonant a vowel, etc. As a means of articulating the structure of a sequential process, Markov's method generalizes into a powerful mathematical tool, to which he lends his name. We will see how Shannon used [Markov chains](https://en.wikipedia.org/wiki/Markov_chain) shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9b846-06b4-4ad9-9aa9-8042402b9192",
   "metadata": {},
   "source": [
    "#### A spate of tedious counting\n",
    "\n",
    "First, however, let's illustrate the more basic method, just to get a feel for its effectiveness.\n",
    "\n",
    "We'll take a text of sufficient length. Urquhart's English translation of _Gargantual and Pantagruel_, in the Everyman's Library edition, clocks in at a respectable 823 pages, so that's a decent sample. If we were following the methods used by Al-Kindi, Markov, or even Shannon himself, we would proceed as follows:\n",
    "  1. Make a list of the letters of the alphabet on a sheet of paper.\n",
    "  2. Go through the text, letter by letter.\n",
    "  3. Beside each letter on your paper, make one mark each time you encounter that letter in the text.\n",
    "\n",
    "Fortunately for us, we can avail ourselves of a computer to do this work. \n",
    "\n",
    "In the following sections of Python code, we download the Project Gutenberg edition of Rabelais' novel, saving it to the computer as a text file. We can read the whole file into the computer's memory as a single Python string. Then using a property of Python strings that allows us to _iterate_ over them, we can automate the process of counting up the occurences of each character. [cite]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41af9d8-6b35-427b-ad0b-14636f6027c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "urlretrieve(\"https://www.gutenberg.org/cache/epub/1200/pg1200.txt\", \"gargantua.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8299d21-85e0-412b-902e-bd4a3e875301",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gargantua.txt') as f:\n",
    "    g_text = f.read()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a938a0b3-9387-4dc4-830f-cf333499131d",
   "metadata": {},
   "source": [
    "````{admonition} Reading the code\n",
    ":class: dropdown\n",
    "1. The `from`...`import` statement (lines of Python code are called _statements_) loads some external code (i.e., code that wasn't automatically loaded when we started our Python session) for use in retrieving data from the web.\n",
    "2. This external code is in the form of a Python _function_ called `urlretrieve()`. Like `print()` in the previous example, a Python function is recognizable by the parentheses following its name. \n",
    "3. Within these parentheses, we can supply zero, one, or more _arguments_. Arguments are values or variables that the function will use to do some work. We can organize our code into functions in order to make it more easily reusable -- even by others. I did not write the `urlretrieve` function -- this code is merely importing and using it. Calling an external function in programming is a little like citing a source in writing: a way of building on others' work.\n",
    "4. _Calling_ the `urlretrieve()` function (what we're doing here) doesn't produce any _visible_ output, but behind the scenes, it fetches the data at the URL (the first argument, between the first pair of quotation marks) and saves that data as a file (the name of which is provided as the second argument, `\"gargantua.txt\"`). \n",
    "5. In the next section, I use another function, `open()`, to open the file, meaning to make it available in the computer's memory for access. The `as f` part of that line indicates that the file, while it's open, can be accessed via the variable `f`. \n",
    "5. In the indented part, I use the `read()` method to load the contents of the file into memory. using the variable `g_text`. Henceforth, the entirety of Rabelais' text is available for use by reference to the `g_text` variable. (For the duration of this Python session, that is -- if I close this browser tab, I'll lose all the variables, etc., and will have to re-run the code to re-create them.)\n",
    "\n",
    "**Note on Python variables**\n",
    "\n",
    "- Variable names in Python do _not_ go inside quotation marks. \n",
    "- Variables are like variables in algebra: they are arbitrary names that stand for specified values.\n",
    "- They usually appear, when first used, either on the _left_ side of an equals sign (`g_text = f.read()`) or inside the parentheses following a function call. That is how the variables acquire their values.\n",
    "- It's worth reiterating: variable names (and function names, too) are arbitrary: i.e., it's the programmer's choice. In the code in this document, I've tried to create variable names that at least suggest what they refer to, but that's just a stylistic convention for making code easier to read; it makes no difference to the computer.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5efa9a4-9b35-433f-b4f9-ea3781a41e2f",
   "metadata": {},
   "source": [
    "Running the code below uses the `len()` function to display the length -- in characters -- of a string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21a11d7-ecf8-47d8-8805-e615901f68b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(g_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc8fc5-ec15-4626-8ddd-a44348c03725",
   "metadata": {},
   "source": [
    "The Project Gutenberg version of _Gargantua and Pantagruel_ has close to a 2 million characters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522242ba-9511-4911-9dad-9769fad74386",
   "metadata": {},
   "source": [
    "As an initial exercise, we can count the frequency with which each character appears. Run the following section of code to create a structure mapping each character to its frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68be960-bc75-4593-8f69-07c5e98cd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_characters = {}\n",
    "for character in g_text:\n",
    "    if character in g_characters:\n",
    "        g_characters[character] += 1\n",
    "    else:\n",
    "        g_characters[character] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee699718-3a61-445f-bd61-ef6a7f31b8ea",
   "metadata": {},
   "source": [
    "Run the code below to reveal the frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8533ef6e-8dd2-4cb1-81a5-7849b452fff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bdd2c6-be2b-46c5-a139-41a4d5340016",
   "metadata": {},
   "source": [
    "````{admonition} Reading the code\n",
    ":class: dropdown\n",
    "\n",
    "- Data structures in Python are often identified by punctuation marks. The curly braces in the output above indicate that the outermost structure is a _dictionary_, which is a mapping of _keys_ to _values_. The keys are sort of like variable names, except that they go inside quotation marks.\n",
    "- Data structures in Python can contain other data structures. Our `g_characters_ dictionary ultimately consists of Python strings mapped to integers (a numeric data type in Python). \n",
    "- To create our dictionary of character frequencies, we use the following logic:\n",
    "     1. We create the variable `g_characters`, setting it to an empty dictionary (`{}`). \n",
    "     2. We loop over the `g_text` variable, which holds a string, i.e., a sequence of characters. The `for` keyword in Python allows us to access each element in a sequence (like a string) one at a time. \n",
    "     3. Each time through the loop, the current character will be assigned to the `character` variable.\n",
    "     4. In the code indented under the `for` line, we check to see whether we have encountered this particular character before (using the variable `character` to refer to it, just as one might solve for `x` in an algebraic equation).\n",
    "     5. If we have encountered this character already, we assume that it's associated with a number in our `g_characters` dictionary, and we increment that number (just as if we were making another hash mark on a sheet of paper).\n",
    "     6. Otherwise, we add this character to `g_characters` and set the tally to 1 (since this is the first occurrence of that character).\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6db4f5-c0ee-4f00-9ed2-25e358ca41b2",
   "metadata": {},
   "source": [
    "Looking at the contents of `g_characters`, we can see that it consists of more than just the letters in standard [Latin script](https://en.wikipedia.org/wiki/Latin_script). There are punctuation marks, numerals, and other symbols, like `\\n`, which represents a line break. \n",
    "\n",
    "But if we look at the 10 most commonly occurring characters, with one exception, it aligns well with the [relative frequency of letters in English](https://en.wikipedia.org/wiki/Letter_frequency) as reported from studying large textual corpora.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82644ae-d893-4328-859a-34dfadafc2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(g_characters.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7273e7b3-2097-4860-bfdf-086900f9f061",
   "metadata": {},
   "source": [
    "````{admonition} Reading the code\n",
    ":class: dropdown\n",
    "\n",
    "This last line code sacrifices clarity for brevity -- a practice I will generally refrain from in this document. But it does illustrate how we can compose complex operations in Python out simpler elements -- which is the fundamental practice of programming.\n",
    "\n",
    "Here we sort our `g_characters` by the numeric elements (so the counts, not the characters), using the built-in `sorted` function, to which we provide the optional argument `reverse=True` to sort in descending order. Then in the square brackets at the end of the line, we look at the first 10 elements in that (now) sorted sequence.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b23b0-3fbe-40d7-92f7-815d28fa7a99",
   "metadata": {},
   "source": [
    "#### Random writing\n",
    "\n",
    "At the heart of Shannon's method lies the notion of _random sampling_. It's perhaps easiest to illustrate this concept before defining it.\n",
    "\n",
    "Using more Python code, let's compare what happens when we construct two random samples of the letters of the Latin script, one in which we select each letter with equal probability, and the other in which we weight our selections according to the frequency we have computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb83d49e-0dd2-4faa-9a25-af3166b6f50c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choices\n",
    "alphabet = \"abcdefghijklmnopqrstuvwxyz\"\n",
    "print(\"\".join(choices(alphabet, k=50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f90a735-bd96-40cf-9bf3-c0baa2b95abb",
   "metadata": {},
   "source": [
    "The code above uses the `choices()` method to create a sample of 50 letters, where each letter is equally likely to appear in our sample. Imagine rolling a 26-sided die, with a different letter on each face, 50 times, writing down the letter that comes up on top each time.\n",
    "\n",
    "Now let's run this trial again, this time supplying the observed frequency of the letters in _Gargantual and Pantagruel_ as weights to the sampling. (For simplicity's sake, we first remove everything but the 26 lowercase letters of the Latin script: numbers, punctuation marks, spaces, letters with accent marks, etc.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6d07380-29f0-4150-82eb-4114d209cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_alpha_chars = {}\n",
    "for c, n in g_characters.items():\n",
    "    if c in alphabet:\n",
    "        g_alpha_chars[c] = n\n",
    "letters = list(g_alpha_chars.keys())\n",
    "weights = g_alpha_chars.values()\n",
    "print(''.join(choices(letters, weights, k=50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c74a8c-264a-4512-b5bf-393b34045eac",
   "metadata": {},
   "source": [
    "Do you notice any difference between the two results? It depends to some extent on roll of the dice, since both selections are still random. But you might see _more_ runs of letters in the second that resemble sequences you could expect in English, maybe even a word or two hiding in there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37717f27-1b76-424a-97ea-636e552119e4",
   "metadata": {},
   "source": [
    "````{admonition} Reading the code\n",
    ":class: dropdown\n",
    "\n",
    "- The line `print(\"\".join(choices(alphabet, k=50)))` displays the result of using the `choices` function to take a random, evenly weighted sample of size `k`. Because `choices` returns a Python list (another data type), not a string, we use the `.join()` method to create a single string out of the 50 letters in our sample -- just to make it more readable. \n",
    "- We use another `for` loop and an `if` statement to create a new dictionary, `g_alpha_chars`, to hold just the frequencies of those characters that can be found in the string called `alphabet` (previously defined). \n",
    "- Then we separate out the characters and their frequencies into two parallel lists. (We do this on account of the way `choices()` is defined to work.)\n",
    "- Finally, we use these two lists as arguments to `choices`, where the presence of the `weights` argument means that the sample will no longer be equally weighted, but that each character in `letters` will be selected with the frequency suppled in `weights`. \n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb4e52-ba65-49e9-a7c2-3d18ce75191b",
   "metadata": {},
   "source": [
    "#### The difference a space makes\n",
    "\n",
    "On Liu's telling, one of Shannon's key innovations was his realization that in analyzing _printed_ English, the _space between words_ counts as a character. It's the spaces that delimit words in printed text; without them, our analysis fails to account for word boundaries. \n",
    "\n",
    "Let's say what happens when we include the space character in our frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6589e6-5acd-4a41-b740-b14c59d1332c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_shannon_chars = {}\n",
    "for c, n in g_characters.items():\n",
    "    if c in alphabet or c == \" \":\n",
    "        g_shannon_chars[c] = n\n",
    "letters = list(g_shannon_chars.keys())\n",
    "weights=g_shannon_chars.values()\n",
    "print(''.join(choices(letters, weights, k=50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e98e6-1f46-40b5-a548-a6182bc06147",
   "metadata": {},
   "source": [
    "It may not seem like much improvement, but now we're starting to see sequences of recognizable \"word length,\" considering the average lengths of words in English. \n",
    "\n",
    "But note that we haven't so far actually tallied anything that would count as a word: we're still operating exclusively at the level of individual characters or letters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e9daa-bd74-429c-b87f-3e15dd3382b0",
   "metadata": {},
   "source": [
    "#### Law-abiding numbers\n",
    "\n",
    "To unpack what we're doing a little more: when we make a _weighted_ selection from the letters of the alphabet, using the frequencies we've observed, it's equivalent to drawing letters out of a bag of Scrabble tiles, where different tiles appear in a different amounts. If there are 5 `e`'s in the bag but only 1 `z`, you might draw a `z`, but over time, you're more likely to draw an `e`. And if you make repeated draws, recording the letter you draw each time before putting it back in the bag, your final tally of letters will usually have more `e`'s than `z`'s. \n",
    "\n",
    "In probability theory, this expectation is called [the law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). It describes the fundamental intuition behind the utility of averages, as well as their limitation: sampling better approximates the mathematical average as the samples get larger, but in every case, we're talking about behavior in the aggregate, not the individual case. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c012a-9824-481f-9b34-e557f9788f7a",
   "metadata": {},
   "source": [
    "### Language as a drunken walk\n",
    "\n",
    "How effectively can we model natural language using statistical means? It's worth dwelling on the assumptions latent in this question. Parts of speech, word order, syntactic dependencies, etc: none of these classically linguistic entities come up for discussion in Shannon's article. Nor are there any claims therein about underlying structures of thought that might map onto grammatical or syntactic structures, such as we find in the Chomskian theory of [generative grammar](https://en.wikipedia.org/wiki/Generative_grammar). The latter theory remains squarely within the algorithmic paradigm: the search for formal rules or laws of thought. \n",
    "\n",
    "Language, in Shannon's treatment, resembles a different kind of phenomena: biological populations, financial markets, or the weather. In each of these systems, it is taken as a given that there are simply too many variables at play to arrive at the kind of description that would even remotely resemble the steps of a formally logical proof. Rather, the systems are described, and attempts are made to predict their behavior over time, drawing on observable patterns held to be valid in the aggregate. \n",
    "\n",
    "Whether the human linguistic faculty is best described in terms of formal, algorithmic rules, or as something else (emotional weather, perhaps), was not a question germane to Shannon's analysis. Inn the introduction to his 1948 article, he claims that the \"semantic aspects of communication are irrelevant to the engineering problem\" (i.e., the problem of devising efficient means of encoding messages, linguistic or otherwise). These \"semantic aspects,\" excluded from \"the engineering problem,\" return to haunt the scene of generative AI with a vengeance. But in order to set this scene, let's return to Shannon's experiments.\n",
    "\n",
    "Following Andrei Markov, Shannon modeled printed English as a Markov chain: as a special kind of weighted selection where the weights of the current selection depend _only_ on the immediately previous selection. A Markov chain is often called a _random walk_, though the image conventionally used to explain it is of a person who has had a bit too much to drink stumbling about. Observing such a situation, you might not be able to determine where the person is trying to go; all you can predict is that their next step will fall within stumbling distance of where they're standing right now. \n",
    "\n",
    "It turns out that Markov chains can be used to model lots of processes in the physical world. And they can be used to model language, too, as Claude Shannon showed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c4e45-e87f-4c24-8f71-739f4b007180",
   "metadata": {},
   "source": [
    "#### More tedious counting\n",
    "\n",
    "One way to construct such an analysis is as follows: represent your sample of text as a continuous string of characters. (As we've seen, that's easy to do in Python.) Then \"glue\" it to another string, representing the same text, but with every character shifted to the left by one position. For example, the first several characters of the first sentence from _Gargantua and Pantagruel_ would look like this:\n",
    "\n",
    "[image]\n",
    "\n",
    "With the exception of the dangling left-most and right-most characters, you now have a pair of strings that yield, for each position, a pair of characters.\n",
    "\n",
    "[image with highlighting]\n",
    "\n",
    "These pairs are called bigrams. But in order to construct a Markov chain, we're not just counting bigrams. Rather, we want to create what's called a _transition table_: a table where we can look up a given character -- the letter `e`, say -- and then for any other character that can follow `e`, find the frequency with which it occurs in that position (i.e., following an `e`). If a given character never follows another character, its bigram doesn't exist in the table. \n",
    "\n",
    "Below are shown the most common bigrams in such a transition table created on the basis of _Gargantua and Pantagruel_.\n",
    "\n",
    "[image]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33cbf760-a7c3-4ef1-a9dd-86fdf3860bd7",
   "metadata": {},
   "source": [
    "#### Preparing the text\n",
    "\n",
    "To simplify our analysis, first we'll standardize the source text a bit. Removing punctuation and non-alphabetic characters, removing extra runs of white space and line breaks, and converting everything to lowercase will make patterns in the results easier to see (though it's really sort of an aesthetic choice, and as I've suggested, Shannon's method doesn't presuppose any essential difference between the letters of words and the punctuation marks that accompany them). \n",
    "\n",
    "Run the two code sections below to clean the text of _Gargantua and Pantagruel_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5d5b7e-243a-45e7-bede-40b7a01fbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    '''\n",
    "    Reduces the provided string to a string consisting of just alphabetic, lowercase characters from the Latin script and non-contiguous spaces.\n",
    "    '''\n",
    "    text_lower = text.lower()\n",
    "    text_lower = text_lower.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "    text_norm = \"\"\n",
    "    for char in text_lower:\n",
    "        if (char in \"abcdefghijklmnopqrstuvwxyz\") or (char == \" \" and text_norm[-1] != \" \"):\n",
    "            text_norm += char\n",
    "    return text_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5618ceb4-371c-481c-908c-60685091d653",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_text_norm = normalize_text(g_text)\n",
    "g_text_norm[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5621ed77-4705-4d00-baf1-0fed0f131c03",
   "metadata": {},
   "source": [
    "````{admonition} Reading the code\n",
    ":class: dropdown\n",
    "\n",
    "The bulk of this code defines a new function, `normalize_text()`, which we can use to perform this procedure whenever we need to. The procedure is as follows:\n",
    "  1. Create a lowercased version of the provided `text` argument, using the built-in `lower()` method.\n",
    "  2. Using the built-in `replace()` method, replace line breaks (the special character `\"\\n\"`) and tabs (the special character `\"\\t\"` with single spaces. \n",
    "  3. Create an empty string to hold the normalized text.\n",
    "  4. Loop over all the characters in the original string, and for each character, add to the normalized text only if it is a) an alphabetic character or b) a space, provided that the last element of the normalized string is not also a space. (This last condition ensures that we don't end up with multiple contiguous spaces.)\n",
    "  5. The `return` keyword is necessary to make our new `text_norm` variable available in the context where we call the function. \n",
    "\n",
    "Then we call this function on the `g_text` variable, assigning the return value to the new variable `g_text_norm`. (Again, don't dwell on the names; it's common in programming to have multiple variables referring to the same value, where each variable belongs to a different context. It's a technique that helps reduce bugs in programming and make programs more efficient.)\n",
    "\n",
    "Finally, we look at the first 1,000 characters in our normalized text.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b07409-72c2-4cdd-a092-461331a6eb45",
   "metadata": {},
   "source": [
    "This method isn't perfect, but we'll trust that any errors -- like the disappearance of accented characters from French proper nouns, etc. -- will get smoothed over in the aggregate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d02ac4-01dd-4ac1-9687-a8f37068c210",
   "metadata": {},
   "source": [
    "#### Setting the table\n",
    "\n",
    "To create our transition table of bigrams, we'll define two new functions in Python. The first function, `create_ngrams`, generalizes a bit from our immediate use case; by setting the parameter called `n` in the function call to a number higher than 2, we can create combinations of three or more successive characters (trigrams, quadgrams, etc.). This feature will be useful a little later.\n",
    "\n",
    "Run the code below to define the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd7f99f-dd94-4e1f-9466-a38396127f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(text, n=2):\n",
    "    '''\n",
    "    Creates a series of ngrams out of the provided text argument. The argument n determines the size of each ngram; n must be greater than or equal to 2. \n",
    "    Returns a list of ngrams, where each ngram is a Python tuple consisting of n characters.\n",
    "    '''\n",
    "    text_arrays = []\n",
    "    for i in range(n):\n",
    "        last_index = len(text) - (n - i - 1)\n",
    "        text_arrays.append(text[i:last_index])\n",
    "    return list(zip(*text_arrays))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d91aaa-16b0-4e79-87ea-625d23160d34",
   "metadata": {},
   "source": [
    "````{admonition} Reading the code\n",
    ":class: dropdown\n",
    "\n",
    "The code here might look a little cryptic, but it's doing what we illustrated above: taking a single string (the `text` argument) and transforming it into multiple, parallel, strings, each of which is copy of the previous but shifted to the left by one character. (The shifted characters get lopped off, so that each string remains the same size as the others.)\n",
    "\n",
    "- The `range()` function just returns a list of numbers, from 0 up to `n - 1`. \n",
    "- For each value of `i` in our loop, we find the position of the string that is `n - i - 1` characters from the end. We do this to ensure that the strings line up. So if `n` is 3, then the first time through the loop, `i` will be 0, and `n - i - 1` will be `2`. \n",
    "- Using Python's string-slicing syntax, we take a portion of the string from `i` to the position calculated above. So on the first time through our loop, our new string will start at the 0 position (the first character) and end with the antepenulimate character.\n",
    "- When `i` is 1, our new string will start at the 1 position (second character) and end with the penultimate character.\n",
    "- When `i` is 2, our new string will start at the 2 position (third character) and end with the last character. \n",
    "\n",
    "A little reflection shows that all three of these strings will be of equal length. \n",
    "\n",
    "Finally, we use the Python `zip()` function to align all three strings and separate them into groups of items that occupy the same position in each string, which you can visualize as follows:\n",
    "\n",
    "[image]\n",
    "\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1642dce-0701-47ce-b5fd-92850916afe5",
   "metadata": {},
   "source": [
    "Let's illustrate our function with a small text first. The output is a Python list, which contains a series of additional collections (called tuples) nested within it. Each subcollection corresponds to a 2-character window, and the window is moved one character to the right each time. \n",
    "\n",
    "This structure will allow us to create our transition table, showing which characters follow which other characters most often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73eb3280-68d9-452c-b114-dc2ffb6fe4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'abcdefghijklmnopqrstuvwxyz'\n",
    "create_ngrams(text, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032fd08-b031-4ff4-aca6-1303413266d4",
   "metadata": {},
   "source": [
    "Run the code section below to define another function, `create_transition_table`, which does what its name suggests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e63fcb39-6bda-41a0-8669-08ad9bc7d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def create_transition_table(ngrams):\n",
    "    '''\n",
    "    Expects as input a list of tuples corresponding to ngrams.\n",
    "    Returns a dictionary of dictionaries, where the keys to the outer dictionary consist of strings corresponding to the first n-1 elements of each ngram.\n",
    "    The values of the outer dictionary are themselves dictionaries, where the keys are the nth elements each ngram, and the values are the frequence of occurrence.\n",
    "    '''\n",
    "    n = len(ngrams[0])\n",
    "    ttable = {}\n",
    "    for ngram in ngrams:\n",
    "        key = \"\".join(ngram[:n-1])\n",
    "        if key not in ttable:\n",
    "            ttable[key] = Counter()\n",
    "        ttable[key][ngram[-1]] += 1\n",
    "    return ttable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f71bb32-1a41-42f3-a31a-dc2247b5b87c",
   "metadata": {},
   "source": [
    "````{admonition} Reading the code\n",
    ":class: dropdown\n",
    "\n",
    "1. Since the `create_transition_table` function expects a list of n-grams as its argument, we find the value of `n` by taking the length of the first element in the list. \n",
    "2. We define a new dictionary, `ttable`, to hold our transitions.\n",
    "3. We loop over the n-grams in our list.\n",
    "   - For every n-gram, we are going to separate it into the character in the nth position, and the sequence of characters from the first to the position `n - 1`. Thus, if `n` is 2, we separate the bigram into two characters. If `n` is 3, on the other hand, the first part consist of two characters. The second part will always be a single character, no matter the value of `n`. That's because we're still calculating the frequency of _individual characters_. We're just basing that frequency on a certain window of characters to the immediate left of each character, and that window can have different sizes.\n",
    "   - The first part of each n-gram is our dictionary _key_. Because we're using a dictionary, we have to associate each key with a value.\n",
    "   - However, unlike our first table of frequencies, the value of this key is actually another dictionary! That's because any initial sequence of `n - 1` characters can be followed (in theory) by any other character. And it's the frequency of the latter that we're calculating. So assuming `n` is 3, we might have `th` followed by `e`, and `th` followed by `a`, and `th` followed by `o`, etc. And we're interested in the frequency of each of those combinations.\n",
    "   - If we haven't seen this key before, we have to create its dictionary. For that, we use a special Python data structure called a `Counter`. The `Counter` just saves us the step of having to initialize every value to 0. \n",
    "   - Finally, the line `ttable[key][ngram[-1]] += 1` increments the numeric value associated with a) the character in the nth position of the current n-gram, which is in turn associated with b) the sequence of characters in the first `n - 1` positions of the n-gram. So if the current value of `ngram` is `the`, then `key` is `th`, and `ngram[-1]` (which refers to the last character in `ngram`) is `e`. So that line translates to `ttable[\"th\"][\"e\"] += 1` (which increments the value, whatever it is, by 1).\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bfbbbc8-6baa-4d6a-a345-14040db00f9c",
   "metadata": {},
   "source": [
    "Now run the code below to create the transition table for the bigrams in the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59585cd9-c15c-47a6-af5e-b6787848023d",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_transition_table(create_ngrams(text, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e751a8-d82c-410a-85fa-d92fc1c88bb7",
   "metadata": {},
   "source": [
    "Here our transition table consists of frequencies that are all 1, because (by definition) each letter occurs only once in the alphabet. The way to read the table, however, is as follows:\n",
    "> The letter `b` occurs after the letter `a` 1 time in our (alphabet) sample.\n",
    "> \n",
    "> The letter `c` occurs after the letter `b` 1 time in our sample.\n",
    "> \n",
    "> ...\n",
    "\n",
    "Now let's use these functions to create the transition table with bigrams _Gargantua and Pantagruel_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2db7a4-f1ab-4a5f-a3cc-e280e2b8567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ttable = create_transition_table(create_ngrams(g_text_norm, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60e6c6-c051-44e6-b681-67ed95f21f03",
   "metadata": {},
   "source": [
    "Our table will now be significantly bigger. But let's use it see how frequently the letter `e` follows the letter `h` in our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b29a32-a875-4027-b20b-3a6e53a316e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ttable['h']['e']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48bd5c-eaa7-41ca-8cc9-1e6a073a7976",
   "metadata": {},
   "source": [
    "We can visualize our table fairly easily by using a Python library called [pandas](https://pandas.pydata.org/).\n",
    "\n",
    "Run the code below, which may take a moment to finish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a341158-1569-4c36-b452-ecf7af97ad64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 0)\n",
    "pd.DataFrame.from_dict(g_ttable, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd76cd5b-b3c1-41ee-9251-2b46656d0436",
   "metadata": {},
   "source": [
    "To read the table, select a row for the first letter, and then a column to find the frequency of the column letter appearing after the letter in the row. (In other words, read across then down.)\n",
    "\n",
    "The space character appears as the empty column/row label in this table. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25ad62-75e5-4e48-8128-6be82c2a8571",
   "metadata": {},
   "source": [
    "### Automatic writing\n",
    "\n",
    "In Shannon's article, these kinds of transition tables are used to demonstrate the idea that English text can be effectively represented as a Markov chain. And to effect the demonstration, Shannon presents the results of _generating_ text by weighted random sampling from the transition tables.  \n",
    "\n",
    "To visualize how the weighted sampling works, imagine the following:\n",
    "  1. You choose a row at random on the transition table above, writing its character down on paper.\n",
    "  2. The numbers in that row correspond to the observed frequencies of characters following the character corresponding to that row.\n",
    "  3. You fill a big with Scrabble tiles, using as many tiles for each character as indicated by the corresponding cell in the selected row. If a cell has `NaN` in it -- the null value -- you don't put any tiles of that chracter in the bag.\n",
    "  5. You draw one tile from the bag. You write down the character you just selected. This character indicates the next row on the table.\n",
    "  6. Using that row, you repeat steps 1 through 4. And so on, for however many characters you want to include in your sample.\n",
    "\n",
    "Run the code below to define a function that will do this sampling for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "f257d727-09ad-48c6-8939-de57e8e566a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(ttable, length=100):\n",
    "    '''\n",
    "    Using a transition table of ngrams, creates a random sample of the provided length (default is 100 characters).\n",
    "    '''\n",
    "    starting_chars = list(ttable.keys())\n",
    "    first_char = last_char = choices(starting_chars, k=1)[0]\n",
    "    l = len(first_char)\n",
    "    generated_text = first_char\n",
    "    for _ in range(length):\n",
    "        chars = list(ttable[last_char].keys())\n",
    "        weights = list(ttable[last_char].values())\n",
    "        next_char = choices(chars, weights, k=1)[0]\n",
    "        generated_text += next_char\n",
    "        last_char = generated_text[-l:]\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bac77330-ffb4-4c54-beef-3bb78b972ed0",
   "metadata": {},
   "source": [
    "````{admonition} Reading the code\n",
    ":class: dropdown\n",
    "\n",
    "Our function expects a transition table as its first argument (a dictionary of dictionaries) and an optional number as its second argument. If not `length` is provided, 100 (characters) is used as the default.\n",
    "\n",
    "1. We randomly select one element from among the keys of the outer dictionary (the rows of the transition table). For clarity's sake, we assign this element to _two_ variable: `first_char` and `last_char`. We will update the `last_char` variable throughout the process, to keep track of the last character(s) we have generated, which determine the selection of the next character.\n",
    "2. Note that `first_char` and `last_char` may or may not be a single character; that depends on the size of the n-grams represented by the provided transition table (`ttable`).\n",
    "3. We find the length of `first_char` and assign it to variable, `l`.\n",
    "3. We initialize a new string, `generated_text`, to this randomly selected element.\n",
    "4. We loop over the numbers up the value of `length`: basically, just performing the same action `length` times.\n",
    "5. For each iteration, we use `last_char` (the first part of an n-gram, as derived in the `create_transition_table` function) to select the list of characters that follow `last_char` in the source text and their frequencies of occurrence.\n",
    "6. We randomly select a single character, using the frequencies as weights.\n",
    "7. We add that character to `generated_text`.\n",
    "8. We update `last_char` to reflect the last `l` characters, which again, correspond to the first part of each n-gram.\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b14ddc-a0b3-4c86-89eb-d26f933977ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "create_sample(g_ttable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf00920-e81a-45f5-a564-3746066519df",
   "metadata": {},
   "source": [
    "Run the code above a few times for the full effect. It's still nonsense, but maybe it seems more like recognizable nonsense -- meaning nonsense that a human being who speaks English might make up -- compared with our previous randomly generated examples. If you agree that it's more recognizable, can you pinpoint features or moments that make it so?\n",
    "\n",
    "Personally, it reminds me of the outcome of using a Ouija board: recognizable words almost emerging from some sort of pooled subconscious, then sinking back into the murk before we can make any sense out of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1baf16a-9b19-4f86-93be-eda8d71fcc64",
   "metadata": {},
   "source": [
    "#### More silly walks\n",
    "\n",
    "More adept Ouija-board users can be simulated by increasing the size of our n-grams. As Shannon's article demonstrates, the approximation to the English lexicon increases by moving from bigrams to trigrams -- such that frequencies are calculated in terms of the occurrence of a given letter immediately after a pair of letters. \n",
    "\n",
    "So instead of a table like this:\n",
    "\n",
    "[image]\n",
    "\n",
    "we have this:\n",
    "\n",
    "[image] \n",
    "\n",
    "Note, however, that throughout these experiments, the level of approximation to any particular understanding of \"the English lexicon\" depends on the nature of the data from which we derive our frequencies. Urquhart's translation of Rabelais, dating from the 16th Century, has a rather distinctive vocabulary, as you might expect, even with the modernized spelling and grammar of the Project Gutenberg edition. \n",
    "\n",
    "The code below defines some interactive controls to make our experiments easier to manipulate. Run both sections of code to create the controls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "410bc842-9823-4d44-8627-95c56fb40b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "def create_slider(min_value=2, max_value=5):\n",
    "    return widgets.IntSlider(\n",
    "            value=2,\n",
    "            min=min_value,\n",
    "            max=max_value,\n",
    "            description='Set value of n:')\n",
    "    \n",
    "def create_update_function(ttable, text, transition_function, slider):\n",
    "    '''\n",
    "    returns a callback function for use in updating the provided transition table with ngrams from text, given slider.value, as well as an output widget\n",
    "    for displaying the output of the callback\n",
    "    '''\n",
    "    output = widgets.Output()\n",
    "    def on_update(change):\n",
    "        with output:\n",
    "            nonlocal ttable\n",
    "            ttable = transition_function(create_ngrams(text, slider.value))\n",
    "            print(f'Updated! Value of n is now {slider.value}.')\n",
    "    return on_update, output\n",
    "\n",
    "def create_generate_function(ttable, sample_function, slider):\n",
    "    '''\n",
    "    returns a callback function for use in generating new random samples from the provided trasition table.\n",
    "    '''\n",
    "    output = widgets.Output()\n",
    "    def on_generate(change):\n",
    "        with output:\n",
    "            print(f'(n={slider.value}) {sample_function(ttable)}')\n",
    "    return on_generate, output\n",
    "    \n",
    "def create_button(label, callback):\n",
    "    '''\n",
    "    Creates a new button with the provided label, and sets its click handler to the provided callback function\n",
    "    '''\n",
    "    button = widgets.Button(description=label)\n",
    "    button.on_click(callback)\n",
    "    return button"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d530f07b-8956-4449-939d-5e522a55888f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_slider = create_slider()\n",
    "update_callback, update_output = create_update_function(g_ttable, g_text_norm, create_transition_table, ngram_slider)\n",
    "update_button = create_button(\"Update table\", update_callback)\n",
    "generate_callback, generate_output = create_generate_function(g_ttable, create_sample, ngram_slider)\n",
    "generate_button = create_button(\"New sample\", generate_callback)\n",
    "display(ngram_slider, update_button, update_output, generate_button, generate_output)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef17bd-ebad-4894-aa24-0fe3a9a0f75b",
   "metadata": {},
   "source": [
    "Use the slider above to change the value of `n`. Click `Update table` to recreate the transition table using the new value of `n`. Then use the `New sample` button to generate a new, random sample of text from the transition table. You can generate as many samples as you like, and you can update the size of the ngrams in between in order to compare samples of different sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "969a7953-b595-4a7c-8f3c-c55820d50702",
   "metadata": {},
   "source": [
    "````{admonition} Reading the code\n",
    ":class: dropdown\n",
    "\n",
    "The code above uses a special Python library (a bundle of Python code) to create some HTML elements with which to interact with the code we've already written. \n",
    "\n",
    "We haven't changed the underlying alogorithm at all -- we've just modified the interface to make it easier to experiment with different values of `n`.\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88eb45f-60b7-4ca3-9614-6a539a4a5e51",
   "metadata": {},
   "source": [
    "What do you notice about the effect of higher values of `n` on the nature of the random samples produced? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74992f-68e6-48e9-8b55-c70bdcb3ef9b",
   "metadata": {},
   "source": [
    "### A Rabelaisian chatbot\n",
    "\n",
    "Following Shannon's article, we can observe the same phenomena using whole words to create our n-grams. I find such examples more compelling, perhaps because I find it easier or more fun to look for the glimmers of sense of random strings of words than in random strings of letters, which may or may not be recognizable words. \n",
    "\n",
    "But the underlying procedure is the same. We first create a list of \"words\" out of our normalized text by splitting the latter on the occurrences of white space. As a result, instead of a single string containing the entire text, we'll have a Python list of strings, each of which is a word from the orginal text.\n",
    "\n",
    "Note that this process is not a rigorous way of tokenizing a text. If that is your goal -- to split a text into words, in order to employ word-frequency analysis or similar techniques -- there are very useful [Python libraries](https://spacy.io/) for this task, which use sophisticated tokenizing techniques.\n",
    "\n",
    "For purposes of our experiment, however, splitting on white space will suffice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a802eb03-92e2-422c-922d-ddf456daee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_text_words = g_text_norm.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32291c6e-625a-4711-b3b1-ad1f3bfedac0",
   "metadata": {},
   "source": [
    "From here, we can create our ngrams and transition table as before. First, we just need to modify our previous code to put the spaces back (since we took them out in order to create our list of words). \n",
    "\n",
    "Run the code sections below to create some new functions, and the to create some more HTML controls for these functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "827fa854-204d-47e2-ae88-06f0b107c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ttable_words(ngrams):\n",
    "    '''\n",
    "    Expects as input a list of tuples corresponding to ngrams.\n",
    "    Returns a dictionary of dictionaries, where the keys to the outer dictionary consist of strings corresponding to the first n-1 elements of each ngram.\n",
    "    The values of the outer dictionary are themselves dictionaries, where the keys are the nth elements each ngram, and the values are the frequence of occurrence.\n",
    "    '''\n",
    "    n = len(ngrams[0])\n",
    "    ttable = {}\n",
    "    for ngram in ngrams:\n",
    "        key = ngram[:n-1]\n",
    "        if key not in ttable:\n",
    "            ttable[key] = Counter()\n",
    "        ttable[key][(ngram[-1],)] += 1\n",
    "    return ttable\n",
    "    \n",
    "def create_sample_words(ttable, length=100):\n",
    "    '''\n",
    "    Using a transition table of ngrams, creates a random sample of the provided length (default is 100 characters).\n",
    "    '''\n",
    "    starting_words = list(ttable.keys())\n",
    "    first_words = last_words = tuple(choices(starting_words, k=1)[0])\n",
    "    n = len(first_words)\n",
    "    text = list(first_words)\n",
    "    for _ in range(length):\n",
    "        words = list(ttable[last_words].keys())\n",
    "        weights = list(ttable[last_words].values())\n",
    "        next_word = choices(words, weights, k=1)[0]\n",
    "        text.append(next_word[0])\n",
    "        last_words = tuple(text[-n:])\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e308046c-9d65-46ab-8090-98dd805750fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ttable_w = create_ttable_words(create_ngrams(g_text_words))\n",
    "ngram_slider_w = create_slider()\n",
    "update_callback_w, update_output_w = create_update_function(g_ttable_w, g_text_words, create_ttable_words, ngrams_slider_w)\n",
    "update_button_w = create_button(\"Update table\", update_callback_w)\n",
    "generate_callback_w, generate_output_w = create_generate_function(g_ttable_w, create_sample_words)\n",
    "generate_button_w = create_button(\"New sample\", generate_callback_w)\n",
    "display(ngram_slider_w, update_button_w, update_output_w, generate_button_w, generate_output_w)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80dc6351-f1ba-4e3c-8063-a91d2ab040c4",
   "metadata": {},
   "source": [
    "Use the slider and buttons above to generate sample text for various values of `n`. Samples are based on n-grams of words from the source text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99da874-6728-4368-8908-13d53bb73985",
   "metadata": {},
   "source": [
    "### How drunken was our walk?\n",
    "\n",
    "In his article, Shannon reports various results of these experiments, using different values for `n` with both letter- and word-frequencies. He includes the following sample, apparently produced at random with word bigrams, though he does not disclose the particular textual sources from which he derived his transition tables:\n",
    "\n",
    ">THE HEAD AND IN FRONTAL ATTACK ON AN ENGLISH WRITER THAT THE CHARACTER OF THIS POINT IS THEREFORE ANOTHER METHOD FOR THE LETTERS THAT THE TIME OF WHOEVER TOLD THE PROBLEM FOR AN UNEXPECTED.\n",
    "\n",
    "I've always thought that Shannon's example seems suspiciously fortuitous, given its mention of attacks on English writers and methods for letters, etc. Who knows how many trials he made before he got this result (assuming he didn't fudge anything). All the same, one of the enduring charms of the \"Markov text generator\" is its propensity to produce uncanny stretches of text that, as Shannon writes, sound \"not at all unreasonable.\" \n",
    "\n",
    "A question does arise: how novel are these stretches? In other words, what proportion of the generated sample is unique relative to the source? One way approach to the question is to think in terms of unique n-grams. When using a value of 3 for `n`, by definition every three-word sequence in our generated sample will match some sequence in the source text. But what about sequences of 4 words? Just looking at the samples we've created, it's clear that at least some of these are novel, since some are plainly nonsense and not likely to appear in Rabelais' text. \n",
    "\n",
    "We might measure their novelty by creating a lot of samples and then, for each sample, calculating the percentage of 4-word n-grams that are _not_ in the source text. Running this procedure over 1,000 samples, I arrive at an average of 40% -- so a little less than half of all the 4-word sequences across all the samples are sequences that do _not_ appear in Rabelais' text. \n",
    "\n",
    "As for what percentage of those constitute phrases that are not \"unreasonable\" as spontaneous English utterances, that's a question that's hard to answer computationally. Obviously, it depends in part on your definition of \"not unreasonable.\" But it's kind of fun to pick out phrases of length `n+1` (or `n+2`, etc.) from your sample and see if they appear in the original. You can do so by running code like the following. Just edit the part between the quotation marks so that they contain a phrase from your sample. If Python returns `True`, the phrase is _not_ in the source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b0a7b6-82e7-4ff7-947c-5c31dec069b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'to do a little untruss' in g_text_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc1d0b2-e029-47aa-99ae-f30c3b6d2c58",
   "metadata": {},
   "source": [
    "````{admonition} How did I do it?\n",
    ":class: dropdown\n",
    "\n",
    "For those interested in such details, the following is the Python code I used to arrive at my estimate of 40% novelty for phrases of length `n+1`, where `n` was 3, using words, not letters.\n",
    "\n",
    "```\n",
    "num_unique = []\n",
    "for _ in range(1000):\n",
    "    text_sample = create_sample(g_ttable)\n",
    "    ngrams_sample = create_ngrams(text_sample.split(), n=4)\n",
    "    this_num = 0\n",
    "    for ngram in ngrams_sample:\n",
    "        if \" \".join(ngram) not in g_text_norm:\n",
    "            this_num += 1\n",
    "    num_unique.append(this_num / len(ngrams_sample))\n",
    "print(sum(num_unique) / len(num_unique))\n",
    "```\n",
    "The code creates a loop that runs 1,000 times. On each iteration, it creates a new randomized sample from the transition table (defined, as mentioned, with n-grams of size `n=3`). Then it creates n-grams out of the sample, using `n=4`. For each n-gram of size 4 in the sample, it checks whether this exact sequence of words appears in the source text. If it does not, then the score for the current sample is incremented by one. For each sample, a final score is derived by dividing the total number of unique n-grams by the total number of n-grams in the sample. Finally, an average score across all 1,000 samples is calculated. This average score represents the average amount of \"uniqueness\" in these samples for sequences of length `n + 1` (where `n` is, again, the size of the n-grams used to create the transition table for the Markov chains).\n",
    "\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f809cba-e34c-4426-b7b2-19e426767c05",
   "metadata": {},
   "source": [
    "### Carnival intelligence?\n",
    "\n",
    "It is no doubt quite a leap from our rudimentary chatbot to Chat GPT, from fun strings of nonsense to the artifically intelligent systems that seem poised to transform, if not actually to take over, our world. The [deep neural networks](https://news.mit.edu/2017/explained-neural-networks-deep-learning-0414) behind Chap-GPT are vastly more complicated than our n-gram transition tables, the mathamematics much more sophisticated than letter- or word-frequency counts, and the data on which these networks have been trained are almost unimaginably immense. \n",
    "\n",
    "And yet, the language model that Chat-GPT implements is, like Shannon's model from 1948, a statistical representation of patterns occurring in the textual data on which it is based. The real power of the latest large language models derives from their capacity to encode overlapping contexts: to represent how the units that make up text occur in multiple relations to each other: e.g., to capture, mathematically, the fact that a certain word frequently follows another word but often appears in the same sentence or paragraph as a third word, and so on. This complexity of representation, coupled with the sheer size of the data used to train the model, leads to Chat-GPT's uncanny ability to mimic textual genres with a high degree of stylistic fidelity. [note: Chat-GTP's [tokenizer](https://platform.openai.com/tokenizer) evidently uses a unit that falls somewhere between individual letters and whole words.]\n",
    "\n",
    "It's also the feature that has led some of their critics to call these models [\"stochastic parrots\"](https://dl.acm.org/doi/10.1145/3442188.3445922). And it may explain the difficulty of putting safeguards in place against the generation of hate speech, fake news, false citations, etc. These sorts of safeguards may even prove more technically challenging to implement than the models themselves.\n",
    "\n",
    "To parrot myself, these models have (as far as we know) _no explicit rules_ of logic, decorum, etc. That characterization is meant to contrast \"explicit\" rules with those that might be said to exist implicitly or in latent fashion in the discursive conventions of the data on which they are trained. And on the face of it, at least, the distinction makes a difference. Many of us can probably recall experiences of consciously thinking (worrying) about whether we have correctly cited a particular source, whether we might have unintentionally plagiarized another writer, whether our essay has a clear thesis, etc. It's true that there remains a great deal we don't understand about the startling effectiveness of large language models. Nonetheless, it seems a stretch to impute to them this kind of conscious reflection. \n",
    "\n",
    "For what it's worth, here's how I am inclined to think about these models.\n",
    "\n",
    "Lacan famously said that the unconscious is structured like a language. Whether that's an apt description of the human psyche is at least debatable. But might we say that these models manifest the unconscious structures of language itself? We can catch glimpses of this manifestation in the relatively humble outcome of Shannon's experiments: in the Markovian leaps that lead us to make _sense_ out of patterned randomness, leaps which, at the same time, reveal the nonsense that riots on the other side of sense. These experiments allow us to wander through spaces of grammatical, lexical, and stylistic possibility -- and the pleasure they offer, for me, lies in their letting us stumble into places where our rule-observant habits might not otherwise let us go. \n",
    "\n",
    "What if we were to approach generative AI in the same spirit? Not as the _deus ex machina_ that will save the world (which it almost certainly is not), and not only as a technology that will further alienate and oppress our labor (which it very probably is). But to borrow from Bakhtin, as a carnivalesque mirror of our collective linguistic unconscious: like carnival, offering a sense of freedom from restraint that is, at the same time, the affirmation, by momentary inversion, of the prevailing order of things. But also a reminder that language is the repository of an intelligence neither of the human (considered as an isolated being), nor of the machine, but of the collective, and that making sense is always a political act. [cite]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9efb1-2492-4f9c-bdff-9fa0b07bb581",
   "metadata": {},
   "source": [
    "\n",
    "Chat-GPT and its ilk have made the [Turing test](https://en.wikipedia.org/wiki/Turing_test) -- long a trope of science fiction and a topic of serious interest chiefly to computer scientists -- into something of a ubiqituous pastime. Certainly, those of us who regularly use the Internet as a source of information or participate in its discourse communities now face the disconcerting question: has what we're reading, seeing, listening to, etc., been produced by a human being or a computer program? How can we tell? Alan Turing proposed his test as a phenomenological benchmark: any machine that could successfully and reliably fool its human interlocutors into granting it the presumption of human intelligence could, in fact, be considered intelligent (in all relevant respects).\n",
    "\n",
    "There's a lot to unpack in Turing's philosophical exercise. But as a tool for understanding how [generative AI](https://en.wikipedia.org/wiki/Generative_artificial_intelligence) works, or at least, for approaching the ground from which it springs, Turing's work is arguably less useful than that of his less celebrated contemporary, Claude Shannon.\n",
    "\n",
    "Working at Bell Labs in the 1940's, Claude Shannon developed the [mathematical theory of communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf). Often referred to as a \"theory of _information_,\" it is noteworthy that Shannon framed his work as a theory of _communication_. Regardless, the practical significance of Shannon's work is [immense](https://www.quantamagazine.org/how-claude-shannons-information-theory-invented-the-future-20201222/): the mathematical modeling introduced there underpins great swaths of modern telecommunications infrastructure, and it paved the way to our data-saturated digital mediascape. \n",
    "\n",
    "Shannon's model is motivated by a practical question: how can we determine the _most efficient means of encoding_ a given message? And although his model has proven relevant to any medium that can be represented digitally, his work was grounded, as Lydia Liu shows, in questions about language, specifically, _printed English_.\n",
    "\n",
    "### From Claude Shannon to Chat GPT (and back again)\n",
    "\n",
    "If Turing's guiding concern was to know what kinds of intellectual activity could be automated, Shannon's was quite different: to know whether human language, in its panoply of uses, could be modeled as a [stochastic (random) process](https://en.wikipedia.org/wiki/Stochastic_process). Turing's point of departure lay in the resources of formal logic and mathematical proof; Shannon drew on data and probability. \n",
    "\n",
    "Although forms of popular (and even scholarly) imagining about AI continue to draw on a Turing-esque framework, wherein the primary concern is with the meaning of intelligence, the \"unreasonable effectiveness\" of [large language models](https://en.wikipedia.org/wiki/Large_language_model) hearkens back to Shannon's experiments on the probabilistic modeling of English prose. And while we certainly couldn't build Siri or Chap-GPT using just Shannon's insights, his methods might be regarded as an early exercise in machine learning. Could we also say that the digital humanities treads this same ground?\n",
    "\n",
    "In this interactive document, we'll use the [Python programming language] to reproduce a couple of Shannon's experiments, in the hopes of pulling back the curtain a bit on what seems to many (and not unreasonably) as evidence of a ghost in the machine. But the aim is not necessarily to demystify experiences of generative AI. I, for one, do find many of these experiences haunting, but I'm not sure the haunting happens where AI's prominent boosters claim that it does.\n",
    "\n",
    "The material that follows draws on and is inspired by my reading of Lydia Liu's _The Freudian Robot_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2b13e-30b0-4969-8df9-b51827ed742a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
