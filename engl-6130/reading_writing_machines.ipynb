{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ee233a9-62a7-4f38-80ee-bcd72b368f2f",
   "metadata": {},
   "source": [
    "# Reading Machines: Exploring the Linguistic Unconscious of AI\n",
    "\n",
    "The early history of computing revolves around efforts to automate human computation -- human labor. And from Lovelace to Turing and beyond, a key concern lay in the specification and refinement of _algorithms_: methods of reducing complex calculations and other operations to explicit formal rules, rules that could, in principle, be implemented with rigor and precision by purely mechanical (and later, of course, electronic) means. \n",
    "\n",
    "But as a means of understanding Chat GPT and other forms of [generative AI](https://en.wikipedia.org/wiki/Generative_artificial_intelligence), a consideration of algorithms only gets us so far. In fact, when it comes to the [large language models](https://en.wikipedia.org/wiki/Large_language_model) that have captivated the public imagination, I would argue that their \"unreasonable effectiveness\" is less a triumph of the algorithm, than the manifestation of another strand of computation, bound up with the former, but motivated by distinct pressures and concerns. Instead of formal logic and mathematical proof, this strand draws on traditions of thinking about data, randomness, and probability. And instead of the prescription of (computational) actions, it aims at the description and prediction of (non-computational) aspects of the world. \n",
    "\n",
    "A rather neglected moment in this tradition, in light of later developments, remains Claude Shannon's [work](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf) on modeling the structure of printed English. In this interactive document, we will use the [Python programming language](https://www.python.org) to reproduce a couple of Shannon's experiments, in the hopes of pulling back the curtain a bit on what seems to many (and not unreasonably) as evidence of a ghost in the machine. But the aim is not necessarily to demystify experiences of generative AI. I, for one, do find many of these experiences haunting. But maybe the haunting doesn't happen where we at first assume.\n",
    "\n",
    "The material that follows draws on and is inspired by my reading of Lydia Liu's _The Freudian Robot_, one of the few works in the humanities that I'm aware of to deal with Shannon's work in depth."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209cb756-c356-4150-adf3-a4a8a2cf0b24",
   "metadata": {},
   "source": [
    "## Two kinds of coding\n",
    "\n",
    "[introduction]\n",
    "\n",
    "### Programs as code(s)\n",
    "\n",
    "We imagine computers as machines that operate on 1's and 0's. In fact, the 1's and 0's are themselves an abstraction for human convenience: digital computation happens as a series of electronic pulses: basically, switches that are either \"on\" or \"off.\" (Think of counting to 10 by flipping a light switch on and off 10 times.)\n",
    "\n",
    "Every digital representation -- everything that can be computed by a digital computer -- must be encoded, ultimately, in this binary form. \n",
    "\n",
    "But to make computers efficient for human use, many additional layers of abstraction have been developed on top of the basic binary layer. By virtue of using computers and smartphones, we are all familiar with the concept of an interface, which instantiates a set of rules prescribing how we are to interact with the device in order to accomplish well-defined tasks. These interactions get encoded down to the level of electronic pulses (and the results of the computation are translated back into the encoding of the interface). \n",
    "\n",
    "A programming language is also an interface: a text-based one. It represents a code into which we can translate our instructions for computation, in order for those instructions to be encoded further for processing. \n",
    "\n",
    "Let's start with a single instruction. Run the following line of Python code. You won't see any output -- that's okay.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fa425114-e402-4761-b30c-c1e1762dd61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_to_everything = 42"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43f26d2-9ed3-49fb-a2c6-9591eb1738da",
   "metadata": {},
   "source": [
    "In the encoding specified by the Python language, the equals sign (`=`) is an instruction that loosely translates to: \"Store this value (right side) somewhere in memory, and given that location in memory the provided label (left side).\" The following image presents one way of imagining what happens in response to this code (with the caveat that, ultimately, the letters and numbers are represented by their binary encoding).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "063a8ee0-c7cb-4ce2-b74d-d447fb9b0865",
   "metadata": {},
   "source": [
    "[image here]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21619b-0b45-4159-b520-63c6f4f08952",
   "metadata": {},
   "source": [
    "By running the previous line of code, we have created a _variable_ called `answer_to_everything`. We can use the variable to retrieve its value (for use in other parts of our program)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e41c570a-0627-4a97-9785-b6b5faf94b4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42\n"
     ]
    }
   ],
   "source": [
    "print(answer_to_everything)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d96246-c61e-404d-ba0f-f8096b11bf47",
   "metadata": {},
   "source": [
    "The `print()` _function_ is a Python command that displays a value on the screen. The syntax of Python -- e.g., the name \"print,\" as well as the parentheses that follow the function name and that enclose the _argument_, which is the thing we want to print -- is perfectly arbitrary (in the Saussurean sense). This syntax was invented by the designers of the Python language, though they drew on conventions found in other programming languages. The point is that nothing about the Python command `print(answer_to_everything)` makes its operation transparent; to know what it does, you have to know the language (or, at least, be familiar with the conventions of programming languages more generally) -- just as when learning to speak a foreign language, you can't deduce much about the meaning of the words from the way they look or sound.\n",
    "\n",
    "However, unlike so-called \"natural\" languages, programming languages are, generally speaking, fully determinate. In other words, even minor deviations in syntax will usually cause errors, and errors will usually bring the whole program to a crashing halt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2633fb92-6114-4b5c-aea8-571269503f8a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'answer_to_everythin' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43manswer_to_everythin\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'answer_to_everythin' is not defined"
     ]
    }
   ],
   "source": [
    "print(answer_to_everythin)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76593f24-7ab4-48dd-a6a6-19d2b2016e13",
   "metadata": {},
   "source": [
    "A misspelled variable causes Python to abort its computation. Imagine if conversation ground to a halt whenever one of the parties mispronounced a word or used a malapropism!\n",
    "\n",
    "I like to say that Python is extremely literal. (But of course, this is merely an analogy, and a loose one. There is no room for metaphor in programming languages, at least, not as far as the computation is concerned.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382482b5-5d87-455a-b07d-0d05451e72db",
   "metadata": {},
   "source": [
    "### Encoding text\n",
    "\n",
    "As an engineer at Bell Labs, Claude Shannon wanted to find -- mathematically -- the most efficient means of encoding data for electronic transmission. Note that this task involves a rather different set of factors from those that influence the design of a programming language.\n",
    "\n",
    "The designer of the language has the luxury of insisting on a programmer's fidelity to the specified syntax. In working in Python, we have to write `print(42)`, exactly as written, in order to display the number `42` on the screen. if we forget the parentheses, for instance, the command won't work. But when we talk on the phone (or via Zoom, etc.), it would certainly be a hassle if we had to first translate our words into a strict, fault-intolerant code like that of Python. \n",
    "\n",
    "All the same, there is no digital (electronic) representation without encoding. To refer to the difference between these two types of codes, I am drawing a distinction between _algorithms_ and _data_. Shannon's work was among the first to illuminate this distinction, which remains relevant to any consideration of machine learning and generative AI.\n",
    "\n",
    "Before we turn to Shannon's experiments with English text, let's look briefly at how Python represents text as data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a16da0-d11a-43f3-a179-ede5979f3369",
   "metadata": {},
   "outputs": [],
   "source": [
    "a_text = \"Most noble and illustrious drinkers, and you thrice precious pockified blades (for to you, and none else, do I dedicate my writings), Alcibiades, in that dialogue of Plato's, which is entitled The Banquet, whilst he was setting forth the praises of his schoolmaster Socrates (without all question the prince of philosophers), amongst other discourses to that purpose, said that he resembled the Silenes.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ab3533-27f3-4480-bc29-bea8410ba8fd",
   "metadata": {},
   "source": [
    "Running the line above creates a new variable, `a_text`, and assigns it to a _string_ representing the first sentence from Francois Rabelais' early Modern novel, _Gargantua and Pantagruel_. A string is the most basic way in Python of representing text, where \"text\" means anything that is not to be treated purely a numeric value. \n",
    "\n",
    "Anything between quotation marks, in Python, is a string.\n",
    "\n",
    "One problem with strings in Python (and other programming languages) is that they have very little structure.\n",
    "\n",
    "A Python string is a sequence of characters, where a _character_ is, a letter of a recognized alphabet, a punctuation mark, a space, etc. Each character is stored in the computer's memory as a numeric code, and from that perspective, all characters are essentially equal.\n",
    "\n",
    "We can access a single character in a string by supplying its position. (Python counts characters in strings from left to right, starting with 0, not 1, for the first character.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "703efe54-ca3b-488d-b76e-61bba4ddc8fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_text[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a655e224-65ab-4b8d-bf14-3aed95941064",
   "metadata": {},
   "source": [
    "We can access a sequence of characters -- here, the characters in positions 11 through 50."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46981216-84ec-4a56-a2d6-7a0edc0cf788",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' and illustrious drinkers, and you thric'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_text[10:50]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ef2a6f-fed0-4929-8df3-41dcfb06427b",
   "metadata": {},
   "source": [
    "We can even divide the string into pieces, using the occurences of particular characters. The code below divides our text on the white space, returning a _list_ (another Python construct) of smaller strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d2ff07f-e349-4cc8-bb80-593ad37126dd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Most',\n",
       " 'noble',\n",
       " 'and',\n",
       " 'illustrious',\n",
       " 'drinkers,',\n",
       " 'and',\n",
       " 'you',\n",
       " 'thrice',\n",
       " 'precious',\n",
       " 'pockified',\n",
       " 'blades',\n",
       " '(for',\n",
       " 'to',\n",
       " 'you,',\n",
       " 'and',\n",
       " 'none',\n",
       " 'else,',\n",
       " 'do',\n",
       " 'I',\n",
       " 'dedicate',\n",
       " 'my',\n",
       " 'writings),',\n",
       " 'Alcibiades,',\n",
       " 'in',\n",
       " 'that',\n",
       " 'dialogue',\n",
       " 'of',\n",
       " \"Plato's,\",\n",
       " 'which',\n",
       " 'is',\n",
       " 'entitled',\n",
       " 'The',\n",
       " 'Banquet,',\n",
       " 'whilst',\n",
       " 'he',\n",
       " 'was',\n",
       " 'setting',\n",
       " 'forth',\n",
       " 'the',\n",
       " 'praises',\n",
       " 'of',\n",
       " 'his',\n",
       " 'schoolmaster',\n",
       " 'Socrates',\n",
       " '(without',\n",
       " 'all',\n",
       " 'question',\n",
       " 'the',\n",
       " 'prince',\n",
       " 'of',\n",
       " 'philosophers),',\n",
       " 'amongst',\n",
       " 'other',\n",
       " 'discourses',\n",
       " 'to',\n",
       " 'that',\n",
       " 'purpose,',\n",
       " 'said',\n",
       " 'that',\n",
       " 'he',\n",
       " 'resembled',\n",
       " 'the',\n",
       " 'Silenes.']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a_text.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0492dc1-df43-4eee-8357-ecac1e6be83b",
   "metadata": {},
   "source": [
    "The strings in the list above correspond, loosely, to the individual words in the sentence from Rabelais' text. But Python really has no concept of \"word,\" neither in English, nor any other (natural) language. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5c70c2-238e-4277-bd02-f17e2d6b3e6b",
   "metadata": {},
   "source": [
    "## Language & chance\n",
    "\n",
    "It's probably fair to say that when Shannon was developing his mathematical approach to encoding information, the algorithmic ideal  dominated computational research in Western Europe and the United States. In previous decades, philosophers like Bertrand Russell and mathematicians like David Hilbert had sought to develop a formal approach to mathematical proof, an approach that, they hoped, would ultimately unify the scientific disciplines. The goal of such research was to identify a core set of axioms, or logical rules, in terms of which all other \"rigorous\" methods of thought could be expressed. In other words, to reduce to zero the uncertainty and ambiguity plaguing natural language as a tool for expression: to make language algorithmic.\n",
    "\n",
    "Working within this tradition, Alan Turing had developed his model of what would become the digital computer. \n",
    "\n",
    "But can language as humans use it be reduced to such formal rules? On the face of it, it's easy to think not. However, that conclusion presents a problem for computation involving human (or \"natural\") language, since the computer is, at bottom, a formal-rule-following machine.\n",
    "\n",
    "Shannon's work implicitly challenges the assumption that we need to resort to formal rules in order to deal with the uncertainty in language. Instead, he sought mathematical means for _quantifying_ that uncertainty. And as Lydia Liu points out, that effort began with a set of observations about patterns in printed English texts. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e382de6-5c4c-4372-8965-6d8f7712cc86",
   "metadata": {},
   "source": [
    "### The long history of code\n",
    "\n",
    "Of course, Shannon's insights do not begin with Shannon. A long history predates him of speculation on what we might call the statistical features of language, speculation of some practical urgency, given the even longer history of cryptographic communication in political, military, and other contexts.\n",
    "\n",
    "In the 9th Century CE, the Arab mathematician and philosopher Al-Kindi [composed a work on cryptography](https://www.tandfonline.com/doi/abs/10.1198/tas.2011.10191) in which he described the relative frequency of letters in [...] as a method for [...]. Al-Kindi, alongside his many other accomplishments, is typically credited with the first surviving analysis of this kind, which is a direct precursor of methods popular in the digital humanities (word frequency analysis), among other many other domains. \n",
    "\n",
    "Closer to the hearts of digital humanists, the Russian mathematician Andrei Markov, in [a 1913 address to the Russian Academy of Sciences](https://www-cambridge-org.proxygw.wrlc.org/core/journals/science-in-context/article/an-example-of-statistical-investigation-of-the-text-eugene-onegin-concerning-the-connection-of-samples-in-chains/EA1E005FA0BC4522399A4E9DA0304862), reported on the results of his experiment with Aleksandr Pushkin's _Evegnii Onegin_: a statistical analysis of the occurrences of consonants and vowels in the first two chapters of Pushkin's novel in verse. From the perspective of today's large-language models, Markov improved on Al-Kindi's methods by counting not just isolated occurrences of vowels or consonants, but co-occurences: that is, where a vowel follows a consonant, a consonant a vowel, etc. As a means of articulating the structure of a sequential process, Markov's method generalizes into a powerful mathematical tool, to which he lends his name. We will see how Shannon used [Markov chains](https://en.wikipedia.org/wiki/Markov_chain) shortly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb9b846-06b4-4ad9-9aa9-8042402b9192",
   "metadata": {},
   "source": [
    "### A spate of tedious counting\n",
    "\n",
    "First, however, let's illustrate the more basic method, just to get a feel for its effectiveness.\n",
    "\n",
    "We'll take a text of sufficient length. Urquhart's English translation of _Gargantual and Pantagruel_, in the Everyman's Library edition, clocks in at a respectable 823 pages, so that's a decent sample. If we were following the methods used by Al-Kindi, Markov, or even Shannon himself, we would proceed as follows:\n",
    "  1. Make a list of the letters of the alphabet on a sheet of paper.\n",
    "  2. Go through the text, letter by letter.\n",
    "  3. Beside each letter on your paper, make one mark each time you encounter that letter in the text.\n",
    "\n",
    "Fortunately for us, we can avail ourselves of a computer to do this work. \n",
    "\n",
    "In the following lines of Python code, we download the Project Gutenberg edition of Rabelais' novel, saving it to the computer as a text file. We can read the whole file into the computer's memory as a single Python string. Then using a property of Python strings that allows us to _iterate_ over them, we can automate the process of counting up the occurences of each character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f41af9d8-6b35-427b-ad0b-14636f6027c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('gargantua.txt', <http.client.HTTPMessage at 0x103d29990>)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib.request import urlretrieve\n",
    "urlretrieve('https://www.gutenberg.org/cache/epub/1200/pg1200.txt', 'gargantua.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f8299d21-85e0-412b-902e-bd4a3e875301",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('gargantua.txt') as f:\n",
    "    g_text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e21a11d7-ecf8-47d8-8805-e615901f68b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1855085"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(g_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49dc8fc5-ec15-4626-8ddd-a44348c03725",
   "metadata": {},
   "source": [
    "The Gutenberg version has close to 2 million characters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a68be960-bc75-4593-8f69-07c5e98cd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_characters = {}\n",
    "for character in g_text:\n",
    "    if character in g_characters:\n",
    "        g_characters[character] += 1\n",
    "    else:\n",
    "        g_characters[character] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27bdd2c6-be2b-46c5-a139-41a4d5340016",
   "metadata": {},
   "source": [
    "The code above implements the logic we described in manual terms:\n",
    "   - We create an empty _dictionary_, called `g_characters`, which allows us to associate pairs of data points.\n",
    "   - We loop through the text (`g_text`), which is a string, i.e., a sequence of characters.\n",
    "   - For each character, if we have encountered it already, we assume that it's associated with a number, and we increment that number (just as if we were making another hash mark on a sheet of paper).\n",
    "   - Otherwise, we add this character to our collection and set the tally to 1 (since this is the first occurrence of that character)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "347356de-788d-4eec-b788-e36df80821ff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\ufeff': 1,\n",
       " 'T': 3347,\n",
       " 'h': 91306,\n",
       " 'e': 174505,\n",
       " ' ': 319443,\n",
       " 'P': 3050,\n",
       " 'r': 86558,\n",
       " 'o': 109023,\n",
       " 'j': 1312,\n",
       " 'c': 33883,\n",
       " 't': 130222,\n",
       " 'G': 1546,\n",
       " 'u': 44252,\n",
       " 'n': 95086,\n",
       " 'b': 21433,\n",
       " 'g': 28016,\n",
       " 'B': 1614,\n",
       " 'k': 10361,\n",
       " 'f': 33708,\n",
       " 'a': 113620,\n",
       " 'd': 60137,\n",
       " 'l': 57764,\n",
       " '\\n': 31090,\n",
       " 'i': 92999,\n",
       " 's': 88844,\n",
       " 'y': 27084,\n",
       " 'w': 29773,\n",
       " 'U': 202,\n",
       " 'S': 1725,\n",
       " 'm': 32998,\n",
       " 'p': 24007,\n",
       " 'v': 13734,\n",
       " '.': 13850,\n",
       " 'Y': 382,\n",
       " ',': 35809,\n",
       " '-': 4525,\n",
       " 'L': 1085,\n",
       " 'I': 5048,\n",
       " ':': 390,\n",
       " 'A': 2526,\n",
       " 'F': 1470,\n",
       " 'ç': 1,\n",
       " 'R': 767,\n",
       " 'D': 774,\n",
       " 'é': 1,\n",
       " 'M': 1210,\n",
       " 'x': 1971,\n",
       " 'q': 1769,\n",
       " '8': 54,\n",
       " '2': 137,\n",
       " '0': 60,\n",
       " '4': 186,\n",
       " '[': 1,\n",
       " '#': 2,\n",
       " '1': 343,\n",
       " ']': 1,\n",
       " '3': 188,\n",
       " 'E': 856,\n",
       " 'C': 2605,\n",
       " 'W': 1341,\n",
       " 'N': 764,\n",
       " \"'\": 2194,\n",
       " 'O': 848,\n",
       " '9': 34,\n",
       " '\"': 12,\n",
       " 'V': 542,\n",
       " 'K': 154,\n",
       " 'H': 1875,\n",
       " '(': 404,\n",
       " '6': 60,\n",
       " '5': 174,\n",
       " ')': 404,\n",
       " ';': 3385,\n",
       " '7': 50,\n",
       " 'z': 781,\n",
       " '/': 9,\n",
       " '?': 1083,\n",
       " '=': 8,\n",
       " '_': 1,\n",
       " 'J': 616,\n",
       " '&': 33,\n",
       " 'X': 773,\n",
       " 'Z': 21,\n",
       " 'Q': 126,\n",
       " '*': 12,\n",
       " '!': 632,\n",
       " '—': 2,\n",
       " '™': 57,\n",
       " '“': 11,\n",
       " '”': 11,\n",
       " '•': 4,\n",
       " '%': 1,\n",
       " '‘': 1,\n",
       " '’': 6,\n",
       " '$': 2}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6db4f5-c0ee-4f00-9ed2-25e358ca41b2",
   "metadata": {},
   "source": [
    "Looking at the contents of `g_characters`, we can see that it consists of more than just the standard Roman alphabet. There are punctuation marks, numerals, and other symbols, like `\\n`, which represents a line break. \n",
    "\n",
    "But if we look at the 10 most commonly occurring characters, with one exception, it aligns well with the [relative frequency of letters in English](https://en.wikipedia.org/wiki/Letter_frequency) as reported from studying large textual corpora.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f82644ae-d893-4328-859a-34dfadafc2b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(' ', 319443),\n",
       " ('e', 174505),\n",
       " ('t', 130222),\n",
       " ('a', 113620),\n",
       " ('o', 109023),\n",
       " ('n', 95086),\n",
       " ('i', 92999),\n",
       " ('h', 91306),\n",
       " ('s', 88844),\n",
       " ('r', 86558)]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(g_characters.items(), key=lambda x: x[1], reverse=True)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243b23b0-3fbe-40d7-92f7-815d28fa7a99",
   "metadata": {},
   "source": [
    "### Automatic writing\n",
    "\n",
    "Using a little Python code, let's compare what happens when we construct two random samples of the letters of the Roman alphabet, one in which we select each letter with equal probability, and the other in which we weight our selections according to the frequency we have computed above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "cb83d49e-0dd2-4faa-9a25-af3166b6f50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hjknlsdoumplgkshcuuafpvyjjloueenvodzykxwxznpevgtpa\n"
     ]
    }
   ],
   "source": [
    "from random import choices\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyz'\n",
    "print(''.join(choices(alphabet, k=50)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6d07380-29f0-4150-82eb-4114d209cc9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_alpha_chars = {c: n for c, n in g_characters.items() if c in alphabet}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3e45e263-90aa-46f3-bb00-ba1c0ac5cbbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sogocbanermnttanedlvneotltsmtrpssfgalsareywdcvssaz\n"
     ]
    }
   ],
   "source": [
    "print(''.join(choices(list(g_alpha_chars.keys()), weights=g_alpha_chars.values(), k=50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c74a8c-264a-4512-b5bf-393b34045eac",
   "metadata": {},
   "source": [
    "Do you notice any difference between the two? It depends to some extent on roll of the dice, since both selections are still random. But you might see _more_ runs of letters in the second that resemble sequences you could expect in English, maybe even a word or two hiding in there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb4e52-ba65-49e9-a7c2-3d18ce75191b",
   "metadata": {},
   "source": [
    "### The difference a space makes\n",
    "\n",
    "On Liu's telling, one of Shannon's key innovations was his realization that in analyzing _printed_ English, the _space between words_ counts as a character. It's the spaces that delimit words in printed text; without them, our analysis fails to account for word boundaries. \n",
    "\n",
    "Let's say what happens when we include the space character in our frequencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6e6589e6-5acd-4a41-b740-b14c59d1332c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d nec rynao e lhssio  bfo emrvcmut opodoyrdn  eeh \n"
     ]
    }
   ],
   "source": [
    "g_shannon_chars = {c: n for c, n in g_characters.items() if c in alphabet or c == ' '}\n",
    "print(''.join(choices(list(g_shannon_chars.keys()), weights=g_shannon_chars.values(), k=50)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f9e98e6-1f46-40b5-a548-a6182bc06147",
   "metadata": {},
   "source": [
    "It may not seem like much improvement, but now we're starting to see sequences of recognizable \"word length,\" considering the average lengths of words in English. (But note that we haven't so far actually tallied anything that would count as a word: we're still operating exclusively at the level of individual characters or letters.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "582e9daa-bd74-429c-b87f-3e15dd3382b0",
   "metadata": {},
   "source": [
    "### Law-abiding numbers\n",
    "\n",
    "To unpack what we're doing a little more: when we make a _weighted_ selection from the letters of the alphabet, using the frequencies we've observed, it's equivalent to drawing letters out of a bag of Scrabble tiles, where different tiles appear in a different amounts. If there are 5 `e`'s in the bag but only 1 `z`, you might draw a `z`, but over time, you're more likely to draw an `e`. And if you make repeated draws, recording the letter you draw each time before putting it back in the bag, your final tally of letters will usually have more `e`'s than `z`'s. In probability theory, this expectation is called [the law of large numbers](https://en.wikipedia.org/wiki/Law_of_large_numbers). It describes the fundamental intuition behind the utility of averages (as well as their limitation: sampling better approximates the mathematical average as the samples get larger, but in every case, we're talking about behavior in the aggregate, not the individual case). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129c012a-9824-481f-9b34-e557f9788f7a",
   "metadata": {},
   "source": [
    "### Language as a drunken walk\n",
    "\n",
    "To return to Shannon's experiments and the question motivating them: How effectively can we model human or natural language using statistical means? It's worth dwelling on the assumptions latent in this question. Parts of speech, word order, syntactic dependencies, etc: none of these classically linguistic entities come up for discussion in Shannon's article. Nor are there any claims therein about underlying structures of thought that might map onto grammatical or syntactic structures, such as we find in the Chomskian theory of [generative grammar](https://en.wikipedia.org/wiki/Generative_grammar). The latter theory remains squarely within the algorithmic paradigm: the search for formal rules or laws of thought. \n",
    "\n",
    "Language, in Shannon's treatment, resembles a different kind of phenomena: biological populations, financial markets, or the weather. In each of these systems, it is taken as a given that there are simply too many variables at play to arrive at the kind of description that would even remotely resemble the steps of a formally logical proof. Rather, the systems are described, and attempts are made to predict their behavior over time, drawing on observable patterns held to be valid in the aggregate. \n",
    "\n",
    "Whether the human linguistic faculty is best described in terms of formal, algorithmic rules, or as something else (emotional weather, perhaps), was not a question germane to Shannon's analysis. He famously wrote in the introduction to his 1948 article, the \"semantic aspects of communication are irrelevant to the engineering problem\" (i.e., the problem of devising efficient means of encoding messages, linguistic or otherwise). These \"semantic aspects\" -- and, I would argue, the \"syntactic aspects\" of communicatiom, too -- excluded from \"the engineering problem,\" return to haunt the scene of generative AI with a vengeance. But in order to set this scene, let's return to Shannon's experiments.\n",
    "\n",
    "Following Andrei Markov, Shannon modeled printed English as a Markov chain: as a special kind of weighted selection where the weights of the current selection depend _only_ on the immediately previous selection. A Markov chain is often called a random walk, though the image conventionally used to explain it is of a person who has had a bit too much to drink stumbling about. Observing such a situation, you might not be able to determine where the person is trying to go; all you can predict is that their next step will fall within stepping distance of where they're standing right now. \n",
    "\n",
    "It turns out that Markov chains can be used to model lots of processes in the physical world. And they can be used to model language, too, as Claude Shannon showed."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2c4e45-e87f-4c24-8f71-739f4b007180",
   "metadata": {},
   "source": [
    "### More tedious counting\n",
    "\n",
    "One way to construct such an analysis is as follows: represent your sample of text as a continuous string of characters. (As we've seen, that's easy to do in Python.) Then \"glue\" it to another string, representing the same text, but with every character shifted to the left by one position. For example, the first several characters of the first sentence from _Gargantua and Pantagruel_ would look like this:\n",
    "\n",
    "[image]\n",
    "\n",
    "With the exception of the dangling left-most and right-most characters, you now have a pair of strings that yield, for each position, a pair of characters.\n",
    "\n",
    "[image with highlighting]\n",
    "\n",
    "This pairs are called bigrams. But in order to construct a Markov chain, we're not just counting bigrams. Rather, we want to create what's called a _transition table_: a table where we can look up a given character -- the letter `e`, say -- and then for any other character that can follow `e`, find the frequency with which it occurs in that position (i.e., following an `e`). If a given character never follows another character, its bigram doesn't exist in the table. \n",
    "\n",
    "Below are shown the most common bigrams in our transition table.\n",
    "\n",
    "[image]\n",
    "\n",
    "To simplify our analysis, first we'll standardize our text a bit. Removing punctuation and non-alphabetic characters, removing extra runs of white space and line breaks, and converting everything to lowercase will make patterns in the results easier to see (though it's really sort of an aesthetic choice, and as I've suggested, Shannon's method doesn't presuppose any essential difference between the letters of words and the punctuation marks that accompany them). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ff5d5b7e-243a-45e7-bede-40b7a01fbc3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_text_lower = g_text.lower()\n",
    "g_text_lower = g_text_lower.replace(\"\\n\", \" \").replace(\"\\t\", \" \")\n",
    "g_text_norm = \"\"\n",
    "for char in g_text_lower:\n",
    "    if (char in \"abcdefghijklmnopqrstuvwxyz\") or (char == \" \" and g_text_norm[-1] != \" \"):\n",
    "        g_text_norm += char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5618ceb4-371c-481c-908c-60685091d653",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the project gutenberg ebook of gargantua and pantagruel this ebook is for the use of anyone anywhere in the united states and most other parts of the world at no cost and with almost no restrictions whatsoever you may copy it give it away or reuse it under the terms of the project gutenberg license included with this ebook or online at wwwgutenbergorg if you are not located in the united states you will have to check the laws of the country where you are located before using this ebook title gargantua and pantagruel author franois rabelais illustrator gustave dor translator peter anthony motteux sir thomas urquhart release date august ebook most recently updated december language english credits produced by sue asscher and david widger transcribers note the original project gutenberg edition of this ebook was a text file prepared by sue asscher in from master francis rabelais five books of the lives heroic deeds and sayings of gargantua and his son pantagruel translated into english by'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_text_norm[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78b07409-72c2-4cdd-a092-461331a6eb45",
   "metadata": {},
   "source": [
    "This method isn't perfect, but we'll trust that any errors -- like the disappearance of accented characters from French proper nouns, etc. -- will get smoothed over the aggregate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d02ac4-01dd-4ac1-9687-a8f37068c210",
   "metadata": {},
   "source": [
    "To create our transition table of bigrams, we'll use the following code, which defines two functions in Python. The first function, `create_ngrams`, generalizes a bit from our immediate use case; by setting the parameter called `n` in the function call to a number higher than 2, we can create more combinations of three or more successive characters (trigrams, quadgrams, etc.). This feature will be useful a little later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "abd7f99f-dd94-4e1f-9466-a38396127f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ngrams(text, n=2):\n",
    "    '''\n",
    "    Creates a series of ngrams out of the provided text argument. The argument n determines the size of each ngram; n must be greater than or equal to 2. \n",
    "    Returns a list of ngrams, where each ngram is a Python tuple consisting of n characters.\n",
    "    '''\n",
    "    text_arrays = []\n",
    "    for i in range(n):\n",
    "        last_index = len(text) - (n - i - 1)\n",
    "        text_arrays.append(text[i:last_index])\n",
    "    return list(zip(*text_arrays))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1642dce-0701-47ce-b5fd-92850916afe5",
   "metadata": {},
   "source": [
    "Let's illustrate our function with a small text first. The output is a Python list, which contains a series of additional collections (called tuples) nested within it. Each subcollection corresponds to a 2-character window, and the window is moved one character to the right each time. \n",
    "\n",
    "This structure will allow us to create our transition table, showing which characters follow which other characters most often. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "73eb3280-68d9-452c-b114-dc2ffb6fe4d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('a', 'b'),\n",
       " ('b', 'c'),\n",
       " ('c', 'd'),\n",
       " ('d', 'e'),\n",
       " ('e', 'f'),\n",
       " ('f', 'g'),\n",
       " ('g', 'h'),\n",
       " ('h', 'i'),\n",
       " ('i', 'j'),\n",
       " ('j', 'k'),\n",
       " ('k', 'l'),\n",
       " ('l', 'm'),\n",
       " ('m', 'n'),\n",
       " ('n', 'o'),\n",
       " ('o', 'p'),\n",
       " ('p', 'q'),\n",
       " ('q', 'r'),\n",
       " ('r', 's'),\n",
       " ('s', 't'),\n",
       " ('t', 'u'),\n",
       " ('u', 'v'),\n",
       " ('v', 'w'),\n",
       " ('w', 'x'),\n",
       " ('x', 'y'),\n",
       " ('y', 'z')]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'abcdefghijklmnopqrstuvwxyz'\n",
    "create_ngrams(text, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e63fcb39-6bda-41a0-8669-08ad9bc7d8c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "def create_transition_table(ngrams):\n",
    "    '''\n",
    "    Expects as input a list of tuples corresponding to ngrams.\n",
    "    Returns a dictionary of dictionaries, where the keys to the outer dictionary consist of strings corresponding to the first n-1 elements of each ngram.\n",
    "    The values of the outer dictionary are themselves dictionaries, where the keys are the nth elements each ngram, and the values are the frequence of occurrence.\n",
    "    '''\n",
    "    n = len(ngrams[0])\n",
    "    ttable = {}\n",
    "    for ngram in ngrams:\n",
    "        key = \"\".join(ngram[:n-1])\n",
    "        if key not in ttable:\n",
    "            ttable[key] = Counter()\n",
    "        ttable[key][ngram[-1]] += 1\n",
    "    return ttable\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "59585cd9-c15c-47a6-af5e-b6787848023d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a': Counter({'b': 1}),\n",
       " 'b': Counter({'c': 1}),\n",
       " 'c': Counter({'d': 1}),\n",
       " 'd': Counter({'e': 1}),\n",
       " 'e': Counter({'f': 1}),\n",
       " 'f': Counter({'g': 1}),\n",
       " 'g': Counter({'h': 1}),\n",
       " 'h': Counter({'i': 1}),\n",
       " 'i': Counter({'j': 1}),\n",
       " 'j': Counter({'k': 1}),\n",
       " 'k': Counter({'l': 1}),\n",
       " 'l': Counter({'m': 1}),\n",
       " 'm': Counter({'n': 1}),\n",
       " 'n': Counter({'o': 1}),\n",
       " 'o': Counter({'p': 1}),\n",
       " 'p': Counter({'q': 1}),\n",
       " 'q': Counter({'r': 1}),\n",
       " 'r': Counter({'s': 1}),\n",
       " 's': Counter({'t': 1}),\n",
       " 't': Counter({'u': 1}),\n",
       " 'u': Counter({'v': 1}),\n",
       " 'v': Counter({'w': 1}),\n",
       " 'w': Counter({'x': 1}),\n",
       " 'x': Counter({'y': 1}),\n",
       " 'y': Counter({'z': 1})}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_transition_table(create_ngrams(text, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e751a8-d82c-410a-85fa-d92fc1c88bb7",
   "metadata": {},
   "source": [
    "For the alphabet, our transition table consists entirely of 1's, because (by definition) each letter occurs only once. The way to read the table, however, is as follows:\n",
    "> The letter `b` occurs after the letter `a` 1 time in our (alphabet) sample.\n",
    "> \n",
    "> The letter `c` occurs after the letter `b` 1 time in our sample.\n",
    "> \n",
    "> ...\n",
    "\n",
    "Now let's use these functions to create the transition table with bigrams _Gargantua and Pantagruel_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8e2db7a4-f1ab-4a5f-a3cc-e280e2b8567e",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_ttable = create_transition_table(create_ngrams(g_text_norm, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a60e6c6-c051-44e6-b681-67ed95f21f03",
   "metadata": {},
   "source": [
    "Our table will now be significantly bigger. But let's use it see how frequently the letter `e` follows the letter `h` in our text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "53b29a32-a875-4027-b20b-3a6e53a316e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40196"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g_ttable['h']['e']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd48bd5c-eaa7-41ca-8cc9-1e6a073a7976",
   "metadata": {},
   "source": [
    "We can visualize our table fairly easily by using a Python library called [pandas](https://pandas.pydata.org/).\n",
    "\n",
    "To read the table, select a row for the first letter, and then a column to find the frequency of the column letter appearing after the letter in the row. (In other words, read across then down.)\n",
    "\n",
    "The space character appears as the empty column/row label in this table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "9a341158-1569-4c36-b452-ecf7af97ad64",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>h</th>\n",
       "      <th></th>\n",
       "      <th>e</th>\n",
       "      <th>u</th>\n",
       "      <th>a</th>\n",
       "      <th>s</th>\n",
       "      <th>r</th>\n",
       "      <th>i</th>\n",
       "      <th>o</th>\n",
       "      <th>l</th>\n",
       "      <th>...</th>\n",
       "      <th>f</th>\n",
       "      <th>g</th>\n",
       "      <th>z</th>\n",
       "      <th>q</th>\n",
       "      <th>b</th>\n",
       "      <th>j</th>\n",
       "      <th>v</th>\n",
       "      <th>d</th>\n",
       "      <th>k</th>\n",
       "      <th>x</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>t</th>\n",
       "      <td>48893</td>\n",
       "      <td>32489</td>\n",
       "      <td>10626</td>\n",
       "      <td>2540</td>\n",
       "      <td>4924</td>\n",
       "      <td>2831</td>\n",
       "      <td>3794</td>\n",
       "      <td>7459</td>\n",
       "      <td>12307</td>\n",
       "      <td>1671</td>\n",
       "      <td>...</td>\n",
       "      <td>78</td>\n",
       "      <td>16</td>\n",
       "      <td>17</td>\n",
       "      <td>4</td>\n",
       "      <td>27</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>h</th>\n",
       "      <td>31</td>\n",
       "      <td>10970</td>\n",
       "      <td>40196</td>\n",
       "      <td>1339</td>\n",
       "      <td>14764</td>\n",
       "      <td>232</td>\n",
       "      <td>1060</td>\n",
       "      <td>12833</td>\n",
       "      <td>8200</td>\n",
       "      <td>144</td>\n",
       "      <td>...</td>\n",
       "      <td>92</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>65</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>51</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>e</th>\n",
       "      <td>266</td>\n",
       "      <td>61543</td>\n",
       "      <td>4873</td>\n",
       "      <td>444</td>\n",
       "      <td>8741</td>\n",
       "      <td>12330</td>\n",
       "      <td>24406</td>\n",
       "      <td>3527</td>\n",
       "      <td>799</td>\n",
       "      <td>6798</td>\n",
       "      <td>...</td>\n",
       "      <td>1600</td>\n",
       "      <td>987</td>\n",
       "      <td>68</td>\n",
       "      <td>288</td>\n",
       "      <td>494</td>\n",
       "      <td>56</td>\n",
       "      <td>2897</td>\n",
       "      <td>12428</td>\n",
       "      <td>143</td>\n",
       "      <td>1224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <td>20185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5985</td>\n",
       "      <td>4086</td>\n",
       "      <td>37773</td>\n",
       "      <td>21143</td>\n",
       "      <td>5827</td>\n",
       "      <td>18698</td>\n",
       "      <td>21971</td>\n",
       "      <td>8038</td>\n",
       "      <td>...</td>\n",
       "      <td>13280</td>\n",
       "      <td>7118</td>\n",
       "      <td>54</td>\n",
       "      <td>1007</td>\n",
       "      <td>15401</td>\n",
       "      <td>1481</td>\n",
       "      <td>2669</td>\n",
       "      <td>9991</td>\n",
       "      <td>1609</td>\n",
       "      <td>427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>p</th>\n",
       "      <td>1094</td>\n",
       "      <td>1659</td>\n",
       "      <td>4253</td>\n",
       "      <td>1079</td>\n",
       "      <td>4510</td>\n",
       "      <td>530</td>\n",
       "      <td>3315</td>\n",
       "      <td>2220</td>\n",
       "      <td>3415</td>\n",
       "      <td>2257</td>\n",
       "      <td>...</td>\n",
       "      <td>24</td>\n",
       "      <td>28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>r</th>\n",
       "      <td>165</td>\n",
       "      <td>20393</td>\n",
       "      <td>19212</td>\n",
       "      <td>2350</td>\n",
       "      <td>5594</td>\n",
       "      <td>4951</td>\n",
       "      <td>1552</td>\n",
       "      <td>7668</td>\n",
       "      <td>6303</td>\n",
       "      <td>943</td>\n",
       "      <td>...</td>\n",
       "      <td>311</td>\n",
       "      <td>1676</td>\n",
       "      <td>4</td>\n",
       "      <td>47</td>\n",
       "      <td>431</td>\n",
       "      <td>13</td>\n",
       "      <td>571</td>\n",
       "      <td>2864</td>\n",
       "      <td>680</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>o</th>\n",
       "      <td>389</td>\n",
       "      <td>14494</td>\n",
       "      <td>412</td>\n",
       "      <td>15121</td>\n",
       "      <td>718</td>\n",
       "      <td>3255</td>\n",
       "      <td>13494</td>\n",
       "      <td>1062</td>\n",
       "      <td>3434</td>\n",
       "      <td>3589</td>\n",
       "      <td>...</td>\n",
       "      <td>13377</td>\n",
       "      <td>793</td>\n",
       "      <td>59</td>\n",
       "      <td>31</td>\n",
       "      <td>766</td>\n",
       "      <td>117</td>\n",
       "      <td>1502</td>\n",
       "      <td>2686</td>\n",
       "      <td>964</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c</th>\n",
       "      <td>7651</td>\n",
       "      <td>1440</td>\n",
       "      <td>5385</td>\n",
       "      <td>1414</td>\n",
       "      <td>4450</td>\n",
       "      <td>98</td>\n",
       "      <td>1766</td>\n",
       "      <td>1642</td>\n",
       "      <td>6605</td>\n",
       "      <td>1214</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>33</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1947</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>g</th>\n",
       "      <td>2826</td>\n",
       "      <td>8690</td>\n",
       "      <td>4074</td>\n",
       "      <td>1064</td>\n",
       "      <td>2308</td>\n",
       "      <td>1063</td>\n",
       "      <td>2979</td>\n",
       "      <td>1448</td>\n",
       "      <td>2950</td>\n",
       "      <td>674</td>\n",
       "      <td>...</td>\n",
       "      <td>11</td>\n",
       "      <td>342</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>22</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>74</td>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>u</th>\n",
       "      <td>26</td>\n",
       "      <td>3254</td>\n",
       "      <td>2256</td>\n",
       "      <td>52</td>\n",
       "      <td>1089</td>\n",
       "      <td>6190</td>\n",
       "      <td>6811</td>\n",
       "      <td>1065</td>\n",
       "      <td>490</td>\n",
       "      <td>4098</td>\n",
       "      <td>...</td>\n",
       "      <td>267</td>\n",
       "      <td>1518</td>\n",
       "      <td>64</td>\n",
       "      <td>11</td>\n",
       "      <td>768</td>\n",
       "      <td>NaN</td>\n",
       "      <td>21</td>\n",
       "      <td>882</td>\n",
       "      <td>24</td>\n",
       "      <td>109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>n</th>\n",
       "      <td>98</td>\n",
       "      <td>22468</td>\n",
       "      <td>7916</td>\n",
       "      <td>1334</td>\n",
       "      <td>2368</td>\n",
       "      <td>4438</td>\n",
       "      <td>43</td>\n",
       "      <td>2580</td>\n",
       "      <td>6534</td>\n",
       "      <td>584</td>\n",
       "      <td>...</td>\n",
       "      <td>425</td>\n",
       "      <td>11097</td>\n",
       "      <td>23</td>\n",
       "      <td>130</td>\n",
       "      <td>159</td>\n",
       "      <td>120</td>\n",
       "      <td>295</td>\n",
       "      <td>18975</td>\n",
       "      <td>1180</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>b</th>\n",
       "      <td>18</td>\n",
       "      <td>348</td>\n",
       "      <td>7095</td>\n",
       "      <td>2652</td>\n",
       "      <td>1635</td>\n",
       "      <td>349</td>\n",
       "      <td>1673</td>\n",
       "      <td>969</td>\n",
       "      <td>2744</td>\n",
       "      <td>2358</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>344</td>\n",
       "      <td>78</td>\n",
       "      <td>9</td>\n",
       "      <td>17</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>k</th>\n",
       "      <td>29</td>\n",
       "      <td>2787</td>\n",
       "      <td>3782</td>\n",
       "      <td>18</td>\n",
       "      <td>65</td>\n",
       "      <td>865</td>\n",
       "      <td>14</td>\n",
       "      <td>1653</td>\n",
       "      <td>128</td>\n",
       "      <td>226</td>\n",
       "      <td>...</td>\n",
       "      <td>46</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>f</th>\n",
       "      <td>5</td>\n",
       "      <td>14987</td>\n",
       "      <td>2170</td>\n",
       "      <td>1295</td>\n",
       "      <td>2118</td>\n",
       "      <td>68</td>\n",
       "      <td>2578</td>\n",
       "      <td>2796</td>\n",
       "      <td>5819</td>\n",
       "      <td>751</td>\n",
       "      <td>...</td>\n",
       "      <td>1277</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a</th>\n",
       "      <td>100</td>\n",
       "      <td>7746</td>\n",
       "      <td>358</td>\n",
       "      <td>1470</td>\n",
       "      <td>11</td>\n",
       "      <td>10297</td>\n",
       "      <td>11194</td>\n",
       "      <td>4724</td>\n",
       "      <td>32</td>\n",
       "      <td>8921</td>\n",
       "      <td>...</td>\n",
       "      <td>1115</td>\n",
       "      <td>2745</td>\n",
       "      <td>147</td>\n",
       "      <td>29</td>\n",
       "      <td>2331</td>\n",
       "      <td>30</td>\n",
       "      <td>3174</td>\n",
       "      <td>4580</td>\n",
       "      <td>1914</td>\n",
       "      <td>97</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>d</th>\n",
       "      <td>34</td>\n",
       "      <td>36897</td>\n",
       "      <td>7297</td>\n",
       "      <td>664</td>\n",
       "      <td>1540</td>\n",
       "      <td>1912</td>\n",
       "      <td>1516</td>\n",
       "      <td>4735</td>\n",
       "      <td>2911</td>\n",
       "      <td>704</td>\n",
       "      <td>...</td>\n",
       "      <td>122</td>\n",
       "      <td>398</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>40</td>\n",
       "      <td>15</td>\n",
       "      <td>202</td>\n",
       "      <td>580</td>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>l</th>\n",
       "      <td>27</td>\n",
       "      <td>9907</td>\n",
       "      <td>10155</td>\n",
       "      <td>983</td>\n",
       "      <td>5066</td>\n",
       "      <td>1655</td>\n",
       "      <td>103</td>\n",
       "      <td>6199</td>\n",
       "      <td>3980</td>\n",
       "      <td>9225</td>\n",
       "      <td>...</td>\n",
       "      <td>782</td>\n",
       "      <td>92</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>583</td>\n",
       "      <td>3590</td>\n",
       "      <td>250</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>i</th>\n",
       "      <td>146</td>\n",
       "      <td>3727</td>\n",
       "      <td>3897</td>\n",
       "      <td>304</td>\n",
       "      <td>2389</td>\n",
       "      <td>12746</td>\n",
       "      <td>4518</td>\n",
       "      <td>377</td>\n",
       "      <td>3922</td>\n",
       "      <td>5304</td>\n",
       "      <td>...</td>\n",
       "      <td>2092</td>\n",
       "      <td>2627</td>\n",
       "      <td>262</td>\n",
       "      <td>128</td>\n",
       "      <td>828</td>\n",
       "      <td>2</td>\n",
       "      <td>2211</td>\n",
       "      <td>3964</td>\n",
       "      <td>1121</td>\n",
       "      <td>381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>s</th>\n",
       "      <td>4389</td>\n",
       "      <td>39878</td>\n",
       "      <td>10210</td>\n",
       "      <td>2368</td>\n",
       "      <td>4043</td>\n",
       "      <td>3208</td>\n",
       "      <td>13</td>\n",
       "      <td>3918</td>\n",
       "      <td>5176</td>\n",
       "      <td>817</td>\n",
       "      <td>...</td>\n",
       "      <td>116</td>\n",
       "      <td>43</td>\n",
       "      <td>8</td>\n",
       "      <td>149</td>\n",
       "      <td>150</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>69</td>\n",
       "      <td>557</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <td>23</td>\n",
       "      <td>19573</td>\n",
       "      <td>1185</td>\n",
       "      <td>NaN</td>\n",
       "      <td>182</td>\n",
       "      <td>976</td>\n",
       "      <td>184</td>\n",
       "      <td>403</td>\n",
       "      <td>3748</td>\n",
       "      <td>163</td>\n",
       "      <td>...</td>\n",
       "      <td>48</td>\n",
       "      <td>26</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>78</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7</td>\n",
       "      <td>36</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>w</th>\n",
       "      <td>6684</td>\n",
       "      <td>3148</td>\n",
       "      <td>4828</td>\n",
       "      <td>1</td>\n",
       "      <td>4539</td>\n",
       "      <td>446</td>\n",
       "      <td>432</td>\n",
       "      <td>6421</td>\n",
       "      <td>3029</td>\n",
       "      <td>212</td>\n",
       "      <td>...</td>\n",
       "      <td>33</td>\n",
       "      <td>12</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>87</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>m</th>\n",
       "      <td>3</td>\n",
       "      <td>5739</td>\n",
       "      <td>8441</td>\n",
       "      <td>1258</td>\n",
       "      <td>6019</td>\n",
       "      <td>967</td>\n",
       "      <td>34</td>\n",
       "      <td>2525</td>\n",
       "      <td>3942</td>\n",
       "      <td>37</td>\n",
       "      <td>...</td>\n",
       "      <td>69</td>\n",
       "      <td>20</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>941</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>v</th>\n",
       "      <td>41</td>\n",
       "      <td>66</td>\n",
       "      <td>9787</td>\n",
       "      <td>49</td>\n",
       "      <td>815</td>\n",
       "      <td>1</td>\n",
       "      <td>14</td>\n",
       "      <td>2780</td>\n",
       "      <td>622</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>x</th>\n",
       "      <td>58</td>\n",
       "      <td>429</td>\n",
       "      <td>287</td>\n",
       "      <td>8</td>\n",
       "      <td>186</td>\n",
       "      <td>17</td>\n",
       "      <td>1</td>\n",
       "      <td>253</td>\n",
       "      <td>29</td>\n",
       "      <td>77</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>121</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>j</th>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>329</td>\n",
       "      <td>669</td>\n",
       "      <td>200</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>10</td>\n",
       "      <td>712</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>q</th>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>z</th>\n",
       "      <td>NaN</td>\n",
       "      <td>48</td>\n",
       "      <td>352</td>\n",
       "      <td>13</td>\n",
       "      <td>99</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>81</td>\n",
       "      <td>69</td>\n",
       "      <td>44</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       h             e      u      a      s      r      i      o     l  ...  \\\n",
       "t  48893  32489  10626   2540   4924   2831   3794   7459  12307  1671  ...   \n",
       "h     31  10970  40196   1339  14764    232   1060  12833   8200   144  ...   \n",
       "e    266  61543   4873    444   8741  12330  24406   3527    799  6798  ...   \n",
       "   20185    NaN   5985   4086  37773  21143   5827  18698  21971  8038  ...   \n",
       "p   1094   1659   4253   1079   4510    530   3315   2220   3415  2257  ...   \n",
       "r    165  20393  19212   2350   5594   4951   1552   7668   6303   943  ...   \n",
       "o    389  14494    412  15121    718   3255  13494   1062   3434  3589  ...   \n",
       "c   7651   1440   5385   1414   4450     98   1766   1642   6605  1214  ...   \n",
       "g   2826   8690   4074   1064   2308   1063   2979   1448   2950   674  ...   \n",
       "u     26   3254   2256     52   1089   6190   6811   1065    490  4098  ...   \n",
       "n     98  22468   7916   1334   2368   4438     43   2580   6534   584  ...   \n",
       "b     18    348   7095   2652   1635    349   1673    969   2744  2358  ...   \n",
       "k     29   2787   3782     18     65    865     14   1653    128   226  ...   \n",
       "f      5  14987   2170   1295   2118     68   2578   2796   5819   751  ...   \n",
       "a    100   7746    358   1470     11  10297  11194   4724     32  8921  ...   \n",
       "d     34  36897   7297    664   1540   1912   1516   4735   2911   704  ...   \n",
       "l     27   9907  10155    983   5066   1655    103   6199   3980  9225  ...   \n",
       "i    146   3727   3897    304   2389  12746   4518    377   3922  5304  ...   \n",
       "s   4389  39878  10210   2368   4043   3208     13   3918   5176   817  ...   \n",
       "y     23  19573   1185    NaN    182    976    184    403   3748   163  ...   \n",
       "w   6684   3148   4828      1   4539    446    432   6421   3029   212  ...   \n",
       "m      3   5739   8441   1258   6019    967     34   2525   3942    37  ...   \n",
       "v     41     66   9787     49    815      1     14   2780    622   NaN  ...   \n",
       "x     58    429    287      8    186     17      1    253     29    77  ...   \n",
       "j    NaN      4    329    669    200    NaN    NaN     10    712   NaN  ...   \n",
       "q    NaN      8    NaN   1879    NaN      1      1      1    NaN   NaN  ...   \n",
       "z    NaN     48    352     13     99    NaN      3     81     69    44  ...   \n",
       "\n",
       "       f      g    z     q      b     j     v      d     k     x  \n",
       "t     78     16   17     4     27     3     2     15     3   NaN  \n",
       "h     92    NaN  NaN     2     65     2     1     51     2   NaN  \n",
       "e   1600    987   68   288    494    56  2897  12428   143  1224  \n",
       "   13280   7118   54  1007  15401  1481  2669   9991  1609   427  \n",
       "p     24     28  NaN     1     16   NaN     1     15    15   NaN  \n",
       "r    311   1676    4    47    431    13   571   2864   680     6  \n",
       "o  13377    793   59    31    766   117  1502   2686   964   170  \n",
       "c      1      4    1    55     33   NaN   NaN      1  1947   NaN  \n",
       "g     11    342  NaN   NaN     22   NaN   NaN     74     5   NaN  \n",
       "u    267   1518   64    11    768   NaN    21    882    24   109  \n",
       "n    425  11097   23   130    159   120   295  18975  1180    16  \n",
       "b      4     14  NaN   NaN    344    78     9     17     2   NaN  \n",
       "k     46      4    1   NaN     13   NaN   NaN     13     2   NaN  \n",
       "f   1277      1    1   NaN      3   NaN   NaN      2   NaN   NaN  \n",
       "a   1115   2745  147    29   2331    30  3174   4580  1914    97  \n",
       "d    122    398    6     2     40    15   202    580     9   NaN  \n",
       "l    782     92   12   NaN    106     3   583   3590   250    21  \n",
       "i   2092   2627  262   128    828     2  2211   3964  1121   381  \n",
       "s    116     43    8   149    150     3   NaN     69   557   NaN  \n",
       "y     48     26    1   NaN     78   NaN     7     36   NaN    10  \n",
       "w     33     12  NaN   NaN     27     1     4     70    87   NaN  \n",
       "m     69     20  NaN     1    941     2     5      7     1   NaN  \n",
       "v    NaN    NaN    1   NaN    NaN   NaN   NaN    NaN   NaN   NaN  \n",
       "x      4    NaN  NaN    10      1   NaN   121      1   NaN   283  \n",
       "j      1    NaN    1   NaN    NaN     1   NaN    NaN   NaN   NaN  \n",
       "q      1      1    1   NaN      1   NaN   NaN    NaN   NaN   NaN  \n",
       "z      2    NaN   71   NaN      2     1     1    NaN   NaN   NaN  \n",
       "\n",
       "[27 rows x 27 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.precision\", 0)\n",
    "pd.DataFrame.from_dict(g_ttable, orient='index')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a25ad62-75e5-4e48-8128-6be82c2a8571",
   "metadata": {},
   "source": [
    "### Automatic writing (2)\n",
    "\n",
    "The code below creates one more function, which uses our transition table as a basis for making random, weighted samples, much as we did with our single-letter frequencies. But this time, our random sample behaves as a Markov chain: that is each, frequency will depend on the previous character. The logic is as follows:\n",
    "  - Assume the first character in our random sample is `a`.\n",
    "  - The next character must be a character that immediately follows an `a` in Rabelais' text. It will be selected randomly from all the  characters following the `a`s in Rabelais' text, using their relative frequencies (i.e., of appearance in a post-`a` position) as the weights.\n",
    "  - Let's assume that _this_ character, randomly selected, is a space.\n",
    "  - The third character we select must be one that immediately follows a space in Rabelais' text, using the same sampling technique as above.\n",
    "  - And so on, for as many characters as we choose to sample.\n",
    "\n",
    "To start our Markov chain, we must either provide or randomly select an initial character. Our function makes an (unweighted) random selection from the bigrams in our table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "f257d727-09ad-48c6-8939-de57e8e566a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample(ttable, length=100):\n",
    "    '''\n",
    "    Using a transition table of ngrams, creates a random sample of the provided length (default is 100 characters).\n",
    "    '''\n",
    "    starting_chars = list(ttable.keys())\n",
    "    first_char = last_char = choices(starting_chars, k=1)[0]\n",
    "    n = len(first_char)\n",
    "    text = first_char\n",
    "    for _ in range(length):\n",
    "        chars = list(ttable[last_char].keys())\n",
    "        weights = list(ttable[last_char].values())\n",
    "        next_char = choices(chars, weights, k=1)[0]\n",
    "        text += next_char\n",
    "        last_char = text[-n:]\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "33b14ddc-a0b3-4c86-89eb-d26f933977ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'r beede bu ase endelly geve tathatoorou m pake waself r terisonanghig joutathof ty baly h avitheriofr'"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_sample(g_ttable)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccf00920-e81a-45f5-a564-3746066519df",
   "metadata": {},
   "source": [
    "Run the code above a few times for the full effect. It's still nonsense, but maybe it seems more like recognizable nonsense -- meaning nonsense that a human being who speaks English might make up -- compared with our previous randomly generated examples. If you agree that it's more recognizable, can you pinpoint features or moments that make it so?\n",
    "\n",
    "Personally, it reminds me of the outcome whenever I've used a Ouija board with anyone else: recognizable words almost emerging from some sort of pooled subconscious, then sinking back into the murk before we can make any sense out of them. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1baf16a-9b19-4f86-93be-eda8d71fcc64",
   "metadata": {},
   "source": [
    "### Running trials\n",
    "\n",
    "More adept Ouija board users can be simulated by increasing the size of our ngrams. As Shannon's article demonstrates, the approximation to the English lexicon increases by moving from bigrams to trigrams -- such that frequencies are calculated in terms of the occurrence of a given letter immediately after a pair of letters. \n",
    "\n",
    "So instead of a table like this:\n",
    "\n",
    "[image]\n",
    "\n",
    "we have this:\n",
    "\n",
    "[image] \n",
    "\n",
    "Note, however, that throughout these experiments, the level of approximation to anyone's understanding of the \"English lexicon\" depends both on the nature of that understanding, and the nature of the data from which we derive our frequencies. Urquhart's translation of Rabelais, dating from the 16th Century, has a rather distinctive vocabulary, as you might expect, even in the Gutenberg Library's electronic edition, which adheres to standardized spelling and punctuation.\n",
    "\n",
    "The code below defines some interactive controls to make our experiments easier to manipulate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "410bc842-9823-4d44-8627-95c56fb40b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "833e09e4-cee3-461c-80d2-a01411b1e722",
   "metadata": {},
   "source": [
    "Use the slider below this cell to change the size of the ngrams and update our transition table. Click `Update` to apply the changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d530f07b-8956-4449-939d-5e522a55888f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7352c6dca099496f916592d322c8fb49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "IntSlider(value=2, description='Set value of n:', max=5, min=2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d6825e9a2954d479489b009d35c30f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Update', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2fbfee7e89e4b5aadd2d68ee6a01aee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ngram_slider = widgets.IntSlider(\n",
    "    value=2,\n",
    "    min=2,\n",
    "    max=5,\n",
    "    step=1,\n",
    "    description='Set value of n:')\n",
    "button = widgets.Button(description=\"Update\")\n",
    "output = widgets.Output()\n",
    "\n",
    "def on_update(change):\n",
    "    with output:\n",
    "        global g_ttable\n",
    "        g_ttable = create_transition_table(create_ngrams(g_text_norm, ngram_slider.value))\n",
    "        print(f'Updated! Value of n is now {ngram_slider.value}.')\n",
    "        \n",
    "button.on_click(on_update)\n",
    "display(ngram_slider, button, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beef17bd-ebad-4894-aa24-0fe3a9a0f75b",
   "metadata": {},
   "source": [
    "Use the button below this cell to generate a new sample based on the current transition table. You can generate as many samples as you like, and you can update the size of the ngrams in between in order to compare samples of different sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "6a2e9391-1c17-4684-93c9-885d389a2173",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e448c82e4f974a4f8c01f4ad71c26fa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(description='Generate sample', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e0422b9abf1432d8181542198e44f68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "button2 = widgets.Button(description=\"Generate sample\")\n",
    "output2 = widgets.Output()\n",
    "\n",
    "def on_generate(change):\n",
    "    with output2:\n",
    "        print(f'(n={ngram_slider.value}) {create_sample(g_ttable)}')\n",
    "\n",
    "button2.on_click(on_generate)\n",
    "display(button2, output2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88eb45f-60b7-4ca3-9614-6a539a4a5e51",
   "metadata": {},
   "source": [
    "What do you notice about the effect of higher values of `n` on the nature of the random samples produced? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb74992f-68e6-48e9-8b55-c70bdcb3ef9b",
   "metadata": {},
   "source": [
    "### A Rabelaisian chatbot\n",
    "\n",
    "I find the phenomena easier to observe when we use whole words as the inputs to our transition table as opposed to individual letters. The underlying principle is the same. We can create a list of \"words\" out of our normalized text by splitting the latter on the occurrences of white space. As a result, instead of a single string containing the entire text, we'll have a Python list of strings, each of which is a word from the orginal text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "a802eb03-92e2-422c-922d-ddf456daee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_text_words = g_text_norm.split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32291c6e-625a-4711-b3b1-ad1f3bfedac0",
   "metadata": {},
   "source": [
    "From here, we can create our ngrams and transition table as before. First, we just need to modify our previous code to put the spaces back (since we took them out in order to create our list of words). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "827fa854-204d-47e2-ae88-06f0b107c359",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_ttable_words(ngrams):\n",
    "    '''\n",
    "    Expects as input a list of tuples corresponding to ngrams.\n",
    "    Returns a dictionary of dictionaries, where the keys to the outer dictionary consist of strings corresponding to the first n-1 elements of each ngram.\n",
    "    The values of the outer dictionary are themselves dictionaries, where the keys are the nth elements each ngram, and the values are the frequence of occurrence.\n",
    "    '''\n",
    "    n = len(ngrams[0])\n",
    "    ttable = {}\n",
    "    for ngram in ngrams:\n",
    "        key = ngram[:n-1]\n",
    "        if key not in ttable:\n",
    "            ttable[key] = Counter()\n",
    "        ttable[key][(ngram[-1],)] += 1\n",
    "    return ttable\n",
    "    \n",
    "def create_sample(ttable, length=100):\n",
    "    '''\n",
    "    Using a transition table of ngrams, creates a random sample of the provided length (default is 100 characters).\n",
    "    '''\n",
    "    starting_words = list(ttable.keys())\n",
    "    first_words = last_words = tuple(choices(starting_words, k=1)[0])\n",
    "    n = len(first_words)\n",
    "    text = list(first_words)\n",
    "    for _ in range(length):\n",
    "        words = list(ttable[last_words].keys())\n",
    "        weights = list(ttable[last_words].values())\n",
    "        next_word = choices(words, weights, k=1)[0]\n",
    "        text.append(next_word[0])\n",
    "        last_words = tuple(text[-n:])\n",
    "    return \" \".join(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f25711f-65e9-40e5-89f5-e5b30887c33a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "811b9879-faad-45d4-bffc-fc9f87fd13ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'his pugnative choler in the lord of basche said panurge that quoth rondibilis i know handsomely and featly how to discern the least hurt but will wade over acheron styx and cocytus drink whole bumpers of lethes waterthough i mortally hate it as dangerous and pernicious shame whereof he assured us again to study it is that it is in the condition wherein they are busied about nothing but to those who are of a roasted coney all that we saw aristotle holding a parley the horse but you are lodged a cornucopia that amalthaean horn which is able to go about'"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_sample(create_ttable_words(create_ngrams(g_text_words, n=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e9efb1-2492-4f9c-bdff-9fa0b07bb581",
   "metadata": {},
   "source": [
    "\n",
    "Chat-GPT and its ilk have made the [Turing test](https://en.wikipedia.org/wiki/Turing_test) -- long a trope of science fiction and a topic of serious interest chiefly to computer scientists -- into something of a ubiqituous pastime. Certainly, those of us who regularly use the Internet as a source of information or participate in its discourse communities now face the disconcerting question: has what we're reading, seeing, listening to, etc., been produced by a human being or a computer program? How can we tell? Alan Turing proposed his test as a phenomenological benchmark: any machine that could successfully and reliably fool its human interlocutors into granting it the presumption of human intelligence could, in fact, be considered intelligent (in all relevant respects).\n",
    "\n",
    "There's a lot to unpack in Turing's philosophical exercise. But as a tool for understanding how [generative AI](https://en.wikipedia.org/wiki/Generative_artificial_intelligence) works, or at least, for approaching the ground from which it springs, Turing's work is arguably less useful than that of his less celebrated contemporary, Claude Shannon.\n",
    "\n",
    "Working at Bell Labs in the 1940's, Claude Shannon developed the [mathematical theory of communication](https://people.math.harvard.edu/~ctm/home/text/others/shannon/entropy/entropy.pdf). Often referred to as a \"theory of _information_,\" it is noteworthy that Shannon framed his work as a theory of _communication_. Regardless, the practical significance of Shannon's work is [immense](https://www.quantamagazine.org/how-claude-shannons-information-theory-invented-the-future-20201222/): the mathematical modeling introduced there underpins great swaths of modern telecommunications infrastructure, and it paved the way to our data-saturated digital mediascape. \n",
    "\n",
    "Shannon's model is motivated by a practical question: how can we determine the _most efficient means of encoding_ a given message? And although his model has proven relevant to any medium that can be represented digitally, his work was grounded, as Lydia Liu shows, in questions about language, specifically, _printed English_.\n",
    "\n",
    "### From Claude Shannon to Chat GPT (and back again)\n",
    "\n",
    "If Turing's guiding concern was to know what kinds of intellectual activity could be automated, Shannon's was quite different: to know whether human language, in its panoply of uses, could be modeled as a [stochastic (random) process](https://en.wikipedia.org/wiki/Stochastic_process). Turing's point of departure lay in the resources of formal logic and mathematical proof; Shannon drew on data and probability. \n",
    "\n",
    "Although forms of popular (and even scholarly) imagining about AI continue to draw on a Turing-esque framework, wherein the primary concern is with the meaning of intelligence, the \"unreasonable effectiveness\" of [large language models](https://en.wikipedia.org/wiki/Large_language_model) hearkens back to Shannon's experiments on the probabilistic modeling of English prose. And while we certainly couldn't build Siri or Chap-GPT using just Shannon's insights, his methods might be regarded as an early exercise in machine learning. Could we also say that the digital humanities treads this same ground?\n",
    "\n",
    "In this interactive document, we'll use the [Python programming language] to reproduce a couple of Shannon's experiments, in the hopes of pulling back the curtain a bit on what seems to many (and not unreasonably) as evidence of a ghost in the machine. But the aim is not necessarily to demystify experiences of generative AI. I, for one, do find many of these experiences haunting, but I'm not sure the haunting happens where AI's prominent boosters claim that it does.\n",
    "\n",
    "The material that follows draws on and is inspired by my reading of Lydia Liu's _The Freudian Robot_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca2b13e-30b0-4969-8df9-b51827ed742a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
