{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark/pyspark for \"big\" data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook provides a demonstration of working with gzipped (compressed) JSON-L (line-delineated Javascript Object Notation) files on single computer using Python, pyspark, and Apache Spark.\n",
    "\n",
    "#### Specs\n",
    " - Python version: 3.8\n",
    " - Spark version: 3.1.2\n",
    " - Java version: OpenJDK 11.0.11+9 \n",
    " - MacBook Pro, 2.8 GHz Intel Core i7\n",
    " - Number of cores: 4\n",
    " - Memory: 16 GB\n",
    " - OS: macOS Mojave (10.14.6)\n",
    " \n",
    "#### Setup\n",
    " - 26GB of Tweets (gzipped JSON-L format)\n",
    " - 1,000 files\n",
    " - Files stored locally\n",
    " - Spark running in \"local\" mode (i.e., on a single machine)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark/Java system settings\n",
    "\n",
    "Relevant lines from `.bash_profile` in my home directory. (These values will differ, depending on your setup.)\n",
    "\n",
    "```\n",
    "export SPARK_HOME=/opt/spark\n",
    "export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk-11.0.11+9/Contents/Home\n",
    "export PATH=$SPARK_HOME/bin:$PATH\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A \"naive\" Python approach "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Loop over a list of gzipped JSON-L files\n",
    " - Increment a counter\n",
    " - Returns the number of documents across all files \n",
    " - Avoids reading all files into memory at once\n",
    " - Takes a _long_ time to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import glob\n",
    "import json\n",
    "\n",
    "def count_tweets(files):\n",
    "    i = 0\n",
    "    for file in files:\n",
    "        with gzip.open(file, mode='rt') as f:\n",
    "            for line in f:\n",
    "                i += 1\n",
    "    return i"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Get a list of files we want to read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/Users/dsmith/Documents/code/workshops/gwlibraries-workshops/spark-gwcoders/coronavirus/*.json.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Takes 11 minutes to process 1,000 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time count_tweets(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we might want is something like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>max(retweet_count)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RT @NicholsUprising: Sicily has figured out th...</td>\n",
       "      <td>211408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RT @_theghettomonk: Not enough praise has gone...</td>\n",
       "      <td>209404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>RT @_theghettomonk: Not enough praise has gone...</td>\n",
       "      <td>209248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>RT @MMarieSophie: HIS NAME IS STEPHEN WAMUKOTA</td>\n",
       "      <td>201466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>RT @MMarieSophie: HIS NAME IS STEPHEN WAMUKOTA...</td>\n",
       "      <td>201304</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>RT @RachelPatzerPhD: My spouse is a physician ...</td>\n",
       "      <td>59764</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>RT @LeoSaldanaP: #quedateEnTuCasa Soy neum√≥log...</td>\n",
       "      <td>59723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>RT @anmmoo: [‡∏ù‡∏≤‡∏Å‡∏£‡∏µ‡∏ô‡∏∞‡∏Ñ‡∏∞] ‡∏û‡πà‡∏≠‡πÄ‡∏£‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡πÄ‡∏Ç‡∏≤ ‡∏ó‡∏≥...</td>\n",
       "      <td>59677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>RT @DrPeckPNP: This is how far back we have to...</td>\n",
       "      <td>59458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>RT @WHO: üö® BREAKING üö®\\n\\n\"We have therefore ma...</td>\n",
       "      <td>58697</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text  max(retweet_count)\n",
       "0   RT @NicholsUprising: Sicily has figured out th...              211408\n",
       "1   RT @_theghettomonk: Not enough praise has gone...              209404\n",
       "2   RT @_theghettomonk: Not enough praise has gone...              209248\n",
       "3      RT @MMarieSophie: HIS NAME IS STEPHEN WAMUKOTA              201466\n",
       "4   RT @MMarieSophie: HIS NAME IS STEPHEN WAMUKOTA...              201304\n",
       "..                                                ...                 ...\n",
       "95  RT @RachelPatzerPhD: My spouse is a physician ...               59764\n",
       "96  RT @LeoSaldanaP: #quedateEnTuCasa Soy neum√≥log...               59723\n",
       "97  RT @anmmoo: [‡∏ù‡∏≤‡∏Å‡∏£‡∏µ‡∏ô‡∏∞‡∏Ñ‡∏∞] ‡∏û‡πà‡∏≠‡πÄ‡∏£‡∏≤‡πÅ‡∏•‡∏∞‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ô‡πÜ‡πÄ‡∏Ç‡∏≤ ‡∏ó‡∏≥...               59677\n",
       "98  RT @DrPeckPNP: This is how far back we have to...               59458\n",
       "99  RT @WHO: üö® BREAKING üö®\\n\\n\"We have therefore ma...               58697\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Leveraging Spark\n",
    "\n",
    "#### Preliminaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - `findspark` connects our notebook to the pyspark library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing some necessary tools\n",
    "- A `SparkSession` instance provides our interface with the Spark runtime\n",
    "- Spark comes with a number of functions for working with data modeled on SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Create a session \n",
    " - You might encounter an error if you haven't installed Spark/Java correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('GWCoders-Demo').getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Understanding Spark's data model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Works well with structured data (JSON, CSV, etc.)\n",
    " - Strongly typed: each field needs a well-defined (Scala) data type\n",
    " - Can infer the **schema** instead of our having to define it explicitly\n",
    "\n",
    "##### A scaleable approach\n",
    " - If your dataset uses the same fields consistently, read a small sample first\n",
    " - Reuse the schema when reading the entire dataset\n",
    " - `read.json`: use with a single file or a whole directory\n",
    " - Add the prefix `file://` to direct Spark to use your local filesystem\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = spark.read.json('file://' + files[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### DataFrames in Spark\n",
    " - \"Lazily\" evaluated: data only read into memory at the time of computation\n",
    " - The DataFrame provides a \"frame\" with type information that is filled as needed\n",
    " - We can use the `show` method to view the first N rows\n",
    " - A Spark DataFrame supports nested data types\n",
    " - Use the `printSchema` method to view the schema hierarchy\n",
    " - Use the `schema` property to acess the schema directly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll re-use this later on\n",
    "schema = sample.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark DataFrame methods\n",
    " - Modeled on SQL\n",
    " - Use the `select` method to create a subset of one or more columns\n",
    " - Use the `where` or `filter` functions to create a subset of rows matching certain conditions\n",
    " - The `pyspark.sql.functions` [module](https://spark.apache.org/docs/2.4.0/api/python/pyspark.sql.html) provides access to Spark's DataFrame functions for manipulating data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The id_str is the field in a Tweet document that uniquely identifies the Tweet\n",
    "sample.select('id_str').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This result is the same as the above\n",
    "# Here we create a new DataFrame with a single column containing our distinct count\n",
    "sample.select(F.countDistinct('id_str')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can filter our sample to just those Tweets that are re-tweets\n",
    "sample.where(F.col('retweeted_status').isNotNull()).select(F.countDistinct('id_str')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Grouping and aggregating over fields\n",
    " - The `groupBy` methods works like the `groupby` method in pandas\n",
    " - Select the relevant columns, group, then apply an aggregation\n",
    " - Need to call `show` to display the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.select('lang', 'id_str').groupBy('lang').agg(F.countDistinct('id_str')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Working with nested data\n",
    " - Much harder to do in pandas\n",
    " - Use **dot notation** to drill down into a hiearchical column\n",
    " - The `explode` method: unroll an array into a new column, creating 1 row per element \n",
    " - Chain `select` statements to extract subfields from array elements\n",
    " - Create intermediate DataFrames for group by/aggregate operations\n",
    " - \"Lazy\" evaluation: not computation performed until `show` is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The hashtags field is an array nested under the entities field\n",
    "sample.select('entities.hashtags').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# It consists of multiple subfields\n",
    "# The text subfield contains the hashtag text\n",
    "sample.select(F.explode('entities.hashtags'), 'id_str').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the take method makes more explicit how the data is structured in each row\n",
    "sample.select(F.explode('entities.hashtags'), 'id_str').take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can refer to the _exploded_ hashtags field by its default name, col\n",
    "sample.select(F.explode('entities.hashtags'),'id_str')\\\n",
    "        .select('id_str', 'col.text').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hashtags = sample.select(F.explode('entities.hashtags'), 'id_str')\\\n",
    "        .select('id_str', 'col.text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the groupBy method on our intermediate DataFrame\n",
    "hashtags.groupBy('text').count().sort(F.desc('count')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - We can use nested fields in other kinds of aggregations, too."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# None of the Tweets in our sample have been retweeted\n",
    "sample.select(F.max('retweet_count')).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The retweeted_status field contains metadata about the Tweet that was retweeted\n",
    "# We can use that field to find the most commonly retweeted Tweets in our set\n",
    "sample.select('retweeted_status.retweet_count', 'text').groupby('text').max('retweet_count')\\\n",
    "        .sort(F.desc('max(retweet_count)')).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spark SQL\n",
    " - DataFrame functions can be expressed as [direct SQL expressions](https://spark.apache.org/docs/latest/api/sql/index.html)\n",
    " - Create a temp view/table \n",
    " - Run SQL statements against the temp view\n",
    " - Handy for complex queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a SQL statement as a Python multi-line string\n",
    "lang_counts = '''\n",
    "select lang, count(distinct id_str) as tweet_count\n",
    "from tweets\n",
    "group by lang\n",
    "order by tweet_count desc\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiates a temporary view (referenced in our SQL expression)\n",
    "sample.createOrReplaceTempView('tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the spark.sql method to execute the query\n",
    "spark.sql(lang_counts).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a SQL query that will create a table of edges between Twitter users who have authored Tweets in our dataset and Twitter users mentioned by those authors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a nested query to explode the mentions field, which is an array, each element of which\n",
    "# corresponds to a mentioned user\n",
    "mentions_sql = '''\n",
    "select mentions.screen_name as mentioned_screen_name,\n",
    "mentions.id_str as mentioned_user_id,\n",
    "user_screen_name,\n",
    "user_id\n",
    "from (select explode(entities.user_mentions) as mentions,\n",
    "    user.id_str as user_id,\n",
    "    user.screen_name as user_screen_name\n",
    "    from tweets)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample.createOrReplaceTempView('tweets')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(mentions_sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Querying the full dataset\n",
    " - From an in-memory sample to a full dataset on disk\n",
    " - Query optimization: write/show only at the end\n",
    " - Grouping/joining data takes longer other kinds of transformations\n",
    " - Re-use the schema wherever possible\n",
    " - The Spark [Jobs UI](http://localhost:4040/jobs/) can help you monitor/troubleshoot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading in our full dataset\n",
    "# Note the call to the schema method, using the variable defined earlier\n",
    "df = spark.read.schema(schema).json('file:///Users/dsmith/Documents/code/workshops/gwlibraries-workshops/spark-gwcoders/coronavirus/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new temp view in order to apply our SQL\n",
    "df.createOrReplaceTempView('tweets')\n",
    "results = spark.sql(mentions_sql)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Took 6.9 minutes on my machine\n",
    "results.write.csv('file:///Users/dsmith/Documents/code/workshops/gwlibraries-workshops/spark-gwcoders/covid-mentions')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a DataFrame of the top 100 most retweeted Tweets across the whole datasset\n",
    "# http://localhost:4040/jobs/\n",
    "retweets = df.select('retweeted_status.retweet_count', 'text').groupby('text').max('retweet_count')\\\n",
    "        .sort(F.desc('max(retweet_count)'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retweets.limit(100).toPandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Take-aways & considerations\n",
    " - Spark is useful for\n",
    "   - Datasets too large to fit into memory\n",
    "   - Deeply nested data structures\n",
    "   - Distributed datasets\n",
    " - Requires Java & Scala\n",
    " - Interfaces exist for Python (PySpark) and [R](https://spark.apache.org/docs/latest/sparkr.html)\n",
    " - Type & organization of the data matter (e.g., many small files vs. a few large files)\n",
    " - Many [configuration](https://spark.apache.org/docs/latest/configuration.html) options \n",
    " - Groupby aggregations & joins \"cost\" more than other transformations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further resources\n",
    "\n",
    "##### Installation guides consulted\n",
    "\n",
    "   - https://kevinvecmanis.io/python/pyspark/install/2019/05/31/Installing-Apache-Spark.html\n",
    "   - https://medium.com/swlh/pyspark-on-macos-installation-and-use-31f84ca61400\n",
    "   - https://www.datacamp.com/community/tutorials/installation-of-pyspark\n",
    "\n",
    "Note that some guides recommend installing Scala separately. If you do that, make sure that the version of Scala you are using is compatible with the version of Spark you are using. More information on the [Apache Spark downloads](https://spark.apache.org/downloads.html) page.\n",
    "\n",
    "##### Books & tutorials on Spark/pyspark\n",
    "\n",
    "   - [Spark: The Definitive Guide](https://learning.oreilly.com/library/view/spark-the-definitive/9781491912201/)\n",
    "   - [Data Analytics with Spark Using Python](https://learning.oreilly.com/library/view/data-analytics-with/9780134844855/)\n",
    "   - [Learning Spark](https://learning.oreilly.com/library/view/learning-spark-2nd/9781492050032/)\n",
    "   - [Best Practices for Spark Performance Optimization](https://developer.ibm.com/blogs/spark-performance-optimization-guidelines/)\n",
    "   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
